{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarize Texts\n",
    "In this notebook we summarize the paper using abstractive summarization (BART) so that it can be fed into OpenAI gpt3.5 API.\n",
    "\n",
    "Papers can be long, so it is easy to exceed the 1024 token limit of BART.\n",
    "\n",
    "For this reason we approach ti summarization by splitting the paper in multiple parts before summarization, and then returning the complete document summarized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartForConditionalGeneration, AutoTokenizer\n",
    "\n",
    "model_ckpt = \"sshleifer/distilbart-cnn-6-6\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_ckpt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Papers to summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r similar_docs\n",
    "print(similar_docs[1].payload['body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 26.43it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "input_ids = [tokenizer(doc.payload['body'], padding='max_length', return_tensors='pt').to(device) for doc in tqdm(similar_docs)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3678"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_ids[0]['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_size = 1024\n",
    "\n",
    "documents_tokenized = []\n",
    "\n",
    "for input_id in input_ids:\n",
    "    n_splits = math.ceil(len(input_id[0])/max_size)\n",
    "    token_splits = []\n",
    "    for index in list(range(n_splits)):\n",
    "        if(index != n_splits-1):\n",
    "            print(str(index*max_size) + \" - \" + str((index+1)*max_size))\n",
    "            token_splits.append({ \"input_ids\": torch.tensor(input_id['input_ids'][0][index*max_size:(index+1)*max_size]).unsqueeze(0), \n",
    "                                \"attention_mask\": torch.tensor(input_id['attention_mask'][0][index*max_size:(index+1)*max_size]).unsqueeze(0)})\n",
    "        else:\n",
    "            print(str(index*max_size) + \" - \" + str(len(input_id[0])%max_size + index*max_size))\n",
    "            token_splits.append({ \"input_ids\": torch.tensor(input_id['input_ids'][0][index*max_size:len(input_id[0])%max_size + index*max_size]).unsqueeze(0), \n",
    "                            \"attention_mask\": torch.tensor(input_id['attention_mask'][0][index*max_size:len(input_id[0])%max_size + index*max_size]).unsqueeze(0)})\n",
    "    \n",
    "    documents_tokenized.append(token_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:40<00:00, 10.18s/it]\n",
      "100%|██████████| 6/6 [01:03<00:00, 10.60s/it]\n",
      "100%|██████████| 4/4 [00:41<00:00, 10.47s/it]\n",
      "100%|██████████| 2/2 [00:18<00:00,  9.47s/it]\n",
      "100%|██████████| 7/7 [01:08<00:00,  9.86s/it]\n",
      "100%|██████████| 7/7 [01:10<00:00, 10.04s/it]\n",
      "100%|██████████| 6/6 [05:04<00:00, 50.74s/it]\n"
     ]
    }
   ],
   "source": [
    "summaries = []\n",
    "\n",
    "for doc_tokenized in tqdm(documents_tokenized):\n",
    "    doc_summary = []\n",
    "    for index in tqdm(list(range(len(doc_tokenized)))):\n",
    "        doc_summary.append(model.generate(input_ids=doc_tokenized[index]['input_ids'], \n",
    "                            attention_mask=doc_tokenized[index]['attention_mask'],\n",
    "                            min_length=16, \n",
    "                            max_length=64))\n",
    "    summaries.append(doc_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_summaries = []\n",
    "\n",
    "for summary in summaries:\n",
    "    text_summary = \"\"\n",
    "\n",
    "    for split in summary:\n",
    "        extracted_summary = tokenizer.decode(split[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "    \n",
    "        if (\".\" in extracted_summary):\n",
    "            text_summary += (\".\".join(extracted_summary.split(\".\")[0:-1])) + \"\\n\"\n",
    "        else:\n",
    "            text_summary += extracted_summary + \"\\n\"\n",
    "    \n",
    "    text_summaries.append(text_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'text_summaries' (list)\n"
     ]
    }
   ],
   "source": [
    "%store text_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
