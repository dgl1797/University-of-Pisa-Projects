[
    {
        "link": "https://www.sciencedirect.com/science/article/pii/S0720048X23001419",
        "body_split": "1. Introduction\nWith the emergence of the coronavirus disease 2019 (COVID 19), pandemic health care facilities face the challenge to timely identify patients who are infected with the severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2). Rapid rule out of COVID-19 before inpatient admission is still crucial to prevent spread within high-risk transmission settings such as hospitals. Shortly after the pandemic onset, the reverse transcription-polymerase chain reaction (RT-PCR) test was available to identify SARS-CoV-2 from respiratory specimen. However, initially, diagnostic turnaround time amounted several days. Although, meanwhile, response time could be reduced to less than two hours with rapid diagnostic tests and to about 24 h with laboratory-based tests, sensitivity still depends on the viral load and thus, does not reliably rule out SARS-CoV-2 [[1], [2]].\nTherefore, the COVID 19 pandemic provoked joint interinstitutional research efforts to develop a diagnostic models as part of an infection prevention strategy that can rapidly and reliably rule out COVID-19 using data from routine clinical examination, laboratory tests, and chest CT. Models should accelerate and secure identification of patients with high probability of COVID-19, independent of RT-PCR test results, and thus, support decision making of physicians at the emergency department and other points of triage in favor of or against isolation of patients. Classification by such models could be both incidental and upon suspicion. An artificial intelligence approach using supervised machine learning for large datasets may become an efficient instrument to improve prospective pandemic preparedness [[3], [4], [5], [6], [7]].\nThe purpose of this study was to develop a predictive model using supervised machine learning, based on a university network database to identify COVID-19 in patients before inpatient admission. It should be assessed whether addition of chest CT features to clinical examination- and laboratory test features improves the performance of the diagnostic model. This approach could serve as a template to prepare for future health pandemics.\n2.1. Study design\nThis study aimed to develop a machine learning-driven predictive model based on a large university network database to rule out COVID-19 among patients at emergency departments before admission to hospitals\u2019 regular wards to prevent in-hospital spread of SARS-CoV-2. The retrospective study was designed to show whether inclusion of chest-CT features to findings from clinical examination and laboratory tests improves the diagnostic performance",
        "summary": "P pandemic health care facilities face challenge to timely identify patients infected with severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) Rapid rule out of COVID-19 before inpatient admission is still crucial to prevent spread within high-risk transmission settings such as hospitals"
    },
    {
        "link": "https://www.sciencedirect.com/science/article/pii/S0720048X23001419",
        "body_split": " of the model.\n\n2.2. Study population and sites\nA pooled database was retrospectively constructed from patient data, acquired at 12 German university hospitals between January 2017 and October 2020 (Supplemental Fig. 1). Participating sites included 4437 consecutive patients of at least 18 years of age (60.5 \u00b1 23.2 years) who underwent chest-CT for any reason. A total of 692 (15.6 %) of the participants were SARS-CoV-2 positive according to the RT-PCR test (median proportion of COVID-19 positives in the centers: 13.2 % [IQR: 30.2\u20136.3 %]). Participants who were included before March 2020 were considered COVID-19 negative without RT-PCR test. Data were anonymized for analysis.\nAll 12 participating sites were part of the radiological cooperative network of the COVID-19 pandemic (RACOON) consortium. The RACOON consortium had been founded by 36 German university hospitals to establish an infrastructure to collect, transfer, and pool radiological data on COVID-19 for strengthening preparedness and responsiveness for pandemics. Data were acquired according to sites\u2019 routine standard of care from clinical examination, laboratory tests, and chest CT evaluation and collected using standardized RACOON electronic data capture templates. Investigators reported radiological findings using the mint Lesion\u2122 software (Mint Medical, Heidelberg, Germany).\n\n2.3. Model establishment\nWe constructed three prediction models to calculate individual participants\u2019 likelihood of being diseased with COVID-19 and to classify participants as COVID-19 positive or negative using a supervised machine learning algorithm. The first model included only features from clinical examination and laboratory tests (model CL), the second model included only chest CT findings (model R), and the third model included variables from clinical evaluation, laboratory tests, and chest CT evaluation (model RCL). RT-PCR test was considered as reference standard.\nFor model construction, experienced specialists for internal medicine and radiologists of the participating sites manually selected 126 relevant candidate variables from the RACOON template as potential predictors for COVID-19 (19, 19, and 88 input features from clinical examination, laboratory tests, and chest CT, respectively [Supplemental Table 1]). To prevent overfitting, we conducted variance thresholding using a cut-off threshold of zero (Scikit-learn machine learning library, version 1.1.2,",
        "summary": "Study population and sitesA pooled database was retrospectively constructed from patient data, acquired at 12 German university hospitals between January 2017 and October 2020. Participating sites included 4437 consecutive patients of at least 18 years of age (60.5 \u00b1 23.2 years) who underwent chest-CT for any reason"
    },
    {
        "link": "https://www.sciencedirect.com/science/article/pii/S0720048X23001419",
        "body_split": " https://scikit-learn.org/stable/) [8]. Subsequently, we used recursive feature elimination [9] with cross validation to select variables to be included into the following multivariable logistic regression analysis. During this process, the model was trained repeatedly while iteratively reducing the number of included features by removing the least essential features during each iteration. For evaluation of the model performance within an iteration, a stratified k-fold cross-validator with 5 folds was used, i.e., the model was split into 5 equally sized subsets and trained on 4 subsets while testing on the remaining subset. This process was repeated 5 times with a different subset being used as test set. The Scikit-learn machine learning library [8] was then used to run iterative logistic regression a hundred times using randomly generated training- and test subsets. With each run, 70 % of the data were randomly assigned as training dataset and 30 % as test dataset. The machine learning algorithm analyzed the training datasets to learn which variables are predictive of COVID-19 using the truncated conjugate gradient newton method to solve the optimization problem [10]. Weights were adjusted inversely proportional to class frequencies (balanced class weights).\nThirteen clinical examination-, 10 laboratory test-, and 7 chest CT covariables were identified as relevant features of the model RCL. The model CL identified 13 clinical examination and 10 laboratory test features, and the model R identified 8 chest CT features as relevant for classification Fig. 1 shows the association of every feature with COVID-19 determined with each of the models. The algorithm classified the event of COVID-19 as occurring if the probability according to logistic regression was \u2265 0.5. Classification was run with both the training datasets and the test datasets. The test datasets were analyzed to assess how accurately the algorithm predicted COVID-19 in the remaining 30 % of participants.\n\n2.4. Statistical analysis\nDiagnostic performance of the three models (R, CL, and RCL) was characterized by sensitivity, specificity, accuracy, negative predictive value (NPV), and positive predictive value (PVV). Receiver operating characteristic (ROC) analysis was performed and areas under ROC curves (AUC) were compared. Performance of the models was compared with z-test for paired samples. A difference of p < 0.05 was considered significant. Association of selected variables with COVID-19 was measured in odds ratios (OR) with 95 % confidence intervals. Statistical",
        "summary": " A stratified k-fold cross-validator with 5 folds was used for evaluation of the model performance within an iteration. The model was split into 5 equally sized subsets and trained on 4 subsets while testing on the remaining subset. This process was repeated 5 times with a different subset being used as test set"
    },
    {
        "link": "https://www.sciencedirect.com/science/article/pii/S0720048X23001419",
        "body_split": " analysis was performed with Python software (Phyton Software Foundation, Beaverton, USA, version 3.10.7).\n3. Results\nOverall accuracy in classification was 0.77, 0.84, and 0.89, respectively with model R, CL, and RCL. Sensitivity was 0.87, 0.82, and 0.89, and specificity 0.75, 0.84, and 0.89, respectively with model R, CL, and RCL (results from the test datasets). The performance of model RCL that added chest CT features to the analysis, was superior regarding accuracy, sensitivity, specificity, NPV, and PPV compared to model CL that included only clinical examination- and laboratory test features (p < 0.001 for each of the outcomes referred) (Table 1). Chest CT features of ground glass opacity (RCL model: OR 2.69 [95 %CI: 2.65\u20132.74]) bronchus wall thickening (RCL model: OR 3.08 [95 %CI: 3.04\u201323.12]), and bronchiectasis (RCL model: OR 3.20 [95 %CI: 3.16\u20133.24]) contributed significantly to the prediction of COVID 19, whereas lung parenchyma mass > 30 mm and nodule as well as arterial occlusions reduced the odds of COVID 19. Chest CT features introduced the highest and the lowest odds ratios, i.e., the strongest associations with COVID-19, to the RCL model (Fig. 1).\nThe AUC of model RCL was significantly larger compared to model CL (difference in the test datasets: 0.030 [95 %CI: 0.029\u20130.030], p < 0.0001) and to model R (difference in the test datasets: 0.073 [95 %CI: 0.070\u20130.075], p < 0.0001) (Fig. 2). This means that the discriminative performance of model RCL to predict COVID-19 was superior to the models CL and R.\nConfusion matrices show that proportion of false negatives (participants with COVID-19 who were wrongly classified as negative by the model) is significantly smaller with model RCL compared to the models CL and R (test datasets: RCL 1.7 % vs CL 2.6 % [p < 0.0001]; vs R 2.1 % [p <",
        "summary": "Overall accuracy in classification was 0.77, 0.84, and 0.89, respectively with model R, CL, and RCL (results from the test datasets) The performance of model RCL that added chest CT features to the analysis, was superior to model CL that included only clinical examination- and laboratory test features (p < 0"
    },
    {
        "link": "https://www.sciencedirect.com/science/article/pii/S0720048X23001419",
        "body_split": " 0.0001]). The same applied to the share of false positives (test datasets: RCL 8.8 % vs CL 13.0 % [p < 0.0001]; vs R 20.9 % [p < 0.0001]) (Fig. 3).\n4. Discussion\nSupervised machine learning was applied to construct diagnostic models from a large, pooled university network database to predict COVID-19 upon suspicion or incidentally and thus, assist in clinicians\u2019 diagnosis before inpatient admission. A model that included clinical examination- and laboratory test features identified COVID-19 with satisfactory accuracy. Addition of chest CT features improved the model performance significantly.\nAlthough RT-PCR tests provide an almost one hundred percent specificity, sensitivity, particularly in case of upper respiratory specimen, is not sufficient to rule out the disease [2]. Moreover, molecular laboratory-based RT-PCR test results are unlikely to be available before 24 h. Turnaround time of rapid point-of-care antigen tests is much shorter. Results should be available within 2 h of sample collection. However, sensitivity decreases with absence of symptoms, during the second week after symptom onset, and with no suspected epidemiological exposure. In addition, sensitivity varies with brands and probably varies with mutations that affect the virus nucleoprotein. Overall, sensitivity ranges from 34 % to 91 % in symptomatic and from 29 % and 78 % in asymptomatic individuals. Depending on prevalence, one in two to five true positives will be missed with rapid antigen tests [11]. Moreover, a recent Cochrane review revealed that absence or presence of individual signs or symptoms have only poor diagnostic accuracy to rule out COVID-19 [12].\nTherefore, the syndromic presentation of COVID-19 as combination of signs and symptoms is better captured in a model based on a large dataset with a wide range of clinical, laboratory, and radiologic features as constructed in this study. Artificial intelligence can handle and analyze large datasets and shortens the procedure of model construction considerably. Machine learning algorithms develop real time prediction models that adapt to growing databases [[3], [5], [6], [7]]. Thus, diagnostic models that are based on machine learning may serve as important prerequisite to achieve readiness for surges of COVID 19 and other emerging or re-emerging pathogens.\nThe diagnostic model constructed in this study, is intended to rule out COVID-19 in a high-risk SARS-CoV-2 setting of hospitals. To protect vulnerable individuals at risk of severe disease",
        "summary": " A model that included clinical examination- and laboratory test features identified COVID-19 with satisfactory accuracy. Addition of chest CT features improved the model performance significantly. Overall, sensitivity ranges from 34 % to 91 % in symptomatic and from 29 % and 78 % in asymptomatic individuals"
    },
    {
        "link": "https://www.sciencedirect.com/science/article/pii/S0720048X23001419",
        "body_split": ", true positives must not be missed. Moreover, as care facilities can become amplifiers of infectious disease outbreaks, efficacious infection prevention and control is paramount (https://www.who.int/publications/i/item/WHO-2019-nCoV-Policy_Brief-IPC-2022.1). This gives reason to establish a highly sensitive test to be applied before inpatient admission [13]. On the other hand, false positives are not that critical because they can be identified with subsequent laboratory-based RT-PCR tests. Nevertheless, to avoid infection of false positives within the quarantine, patients who initially were diagnosed as positive by the model should be isolated separately until diagnosis is confirmed, so that false positives can be released from quarantine.\nThe most accurate model created in this study included features of all three categories (clinical, laboratory, and chest CT) and achieved a considerably higher sensitivity as established as minimum performance requirement (\u22650.80) by the World Health Organization (WHO) for rapid diagnostic tests. Specificity of the model was sufficient for the desired purpose of \u201crule out\u201d, however, fell below the WHO requirement for rapid diagnostic tests (\u22650.97) (https://www.who.int/publications/i/item/WHO2019-nCoVAntigen_Detection2021.1).\nA recent Cochrane test-accuracy review that included 69 chest CT studies revealed a pooled sensitivity of 0.87 (range: 0.45\u20131.0) and a pooled specificity of 0.78 (range: 0.1\u20131.0).\nThese findings led authors conclude that chest CT is appropriate to rule out COVID-19 but not to differentiate SARS-CoV-2 infection from other respiratory diseases [14]. Results concern only suspected cases. A French university study that reported on chest CT for rapid triage in multiple emergency departments also found favorable sensitivity (0.90) and specificity (0.88) [15]. However, due to radiation exposure even with a low-dose mode, chest CT should be justified by clinical indication. It should only serve as diagnostic approach in patients who require chest CT due to suspicion of COVID-19 or for whatever other clinical reasons. Of note, even in patients without characteristic symptoms, COVID-19 can manifest as pneumonia and signs can incidentally be recognized with chest CT [[13], [16], [17]]. Whether ultrasonography of the lungs may serve as alternative that",
        "summary": " The most accurate model created in this study included features of all three categories (clinical, laboratory, and chest CT) This gives reason to establish a highly sensitive test to be applied before inpatient admission. On the other hand, false positives are not that critical because they can be identified with subsequent laboratory-based RT-PCR tests"
    },
    {
        "link": "https://www.sciencedirect.com/science/article/pii/S0720048X23001419",
        "body_split": " might be applied more widely, even as screening tool without exposure to radiation, remains to be proven. The review mentioned above already found a similar sensitivity and a somewhat lower specificity for suspected cases (0.89 and 0.72, respectively, pooled from 15 ultrasonography studies) [14]. Thus, ultrasonography features may be considered in future predictive models. Another diagnostic approach could be radiomics methods. Based on artificial intelligence, imaging features can be converted into data for analysis and subsequently integrated into predictive models. Although initial findings are promising, to date, the approach of radiomics is not ready for clinical implementation [18].\nMajor strength of this study is the large database pooled from multiple nation-wide university centers that included consecutive patients who were admitted due to various diseases. COVID-19 positive participants could have been asymptomatic or symptomatic and there were also no restrictions regarding symptom onset. Therefore, we can claim a high degree of generalizability. Nevertheless, this study also has limitations. First, this study presents a static snapshot of the learning algorithm constructed. However, the algorithm may be continuously updated as additional data is acquired. Diagnostic performance is expected to improve with growing data volume over time. Moreover, in the course of time, the model can adapt to new virus variants. Second, we used RT-PCR test as reference standard for SARS-CoV-2 infection. However, positive RT-PCR test results do not constitute infectiousness. Third, we only included patients who underwent chest CT. Although chest CT was conducted not necessarily due to suspicion of COVID 19 but rather due to various diseases, patient selection gives rise to sample selection bias. Finally, the test datasets originate from subdivisions of the pooled dataset. No external validation was conducted.\n5. Conclusions\nChest CT features improve the performance of diagnostic models to predict COVID-19 before inpatient admission. An artificial intelligence approach of COVID-19 prediction can inform medical decisions right at the beginning of patients\u2019 diagnostic pathways in a timely manner. The approach might serve as an example of how to make use of large, pooled data bases to address future pandemics right from the beginning.\n6. Declarations\nEthics approval and consent to participate\nThe study was approved by the Friedrich-Schiller-University ethics commission (Reg. No. 2021-2128). Data were anonymized for retrospective analysis and thus, ethics commission waived the requirement for informed consent.\nConsent for publication\nNot applicable.\nFund",
        "summary": " The study was approved by the Friedrich-Schiller-University ethics commission (Reg. No. 2021-2128) The approach of radiomics is not ready for clinical implementation. The approach might serve as an example of how to make use of large, pooled data bases to address future pandemics right from the beginning"
    },
    {
        "link": "https://www.sciencedirect.com/science/article/pii/S0720048X23001419",
        "body_split": "ing\nThis work was supported by the German Federal Ministry of Education and Research (BMBF) as part of the University Medicine Network (Project RACOON, 01KX2021).\nAuthors' contributions\nFG and UT conceptualized and supervised the study. MK constructed the models and analyzed the data. MI and MK contributed to writing the initial draft with input from all authors. FG, UT, MK, and MI discussed and interpreted outcomes. The participating RACOON consortium provided data infrastructure and data acquisition. All authors reviewed and approved the final manuscript.\n",
        "summary": "This work was supported by the German Federal Ministry of Education and Research. All authors reviewed and approved the final manuscript"
    },
    {
        "link": "https://www.sciencedirect.com/science/article/pii/S2001037023001551",
        "body_split": "1. Introduction\nAs the most lethal gastrointestinal malignancy, pancreatic cancer is the fourth leading cause of cancer-related death worldwide, with a 5-year survival rate of approximately 9%. The majority of patients with pancreatic cancer are diagnosed at an advanced stage, and only 20% of patients are eligible for curative surgery. In addition, the current therapies for pancreatic cancer do not produce satisfactory results. Abundant stroma in the tumor microenvironment (TME) is a crucial feature of pancreatic cancer, and it contributes to tumor growth, progression and chemoresistance,. Therefore, further research on the TME of pancreatic cancer will help to explore new therapeutic strategies.\nAt present, two important clinical examinations, [18F]-fluoro-2-deoxy-D-glucose (18F-FDG) positron emission tomography-computed tomography (PET-CT) and endoscopic ultrasound (EUS), are used to diagnose pancreatic cancer. The standardized uptake value (SUV) is a well-known measurement for 18F-FDG uptake in PET-CT imaging. Increasing evidence shows that the SUVmax parameter, the largest SUV in the selected region, is an important prognostic factor,,. EUS elastography (EUS-EG) has been used to detect the stiffness of the pancreas, and EUS-guided fine needle aspiration (EUS-FNA) has been employed to obtain tumor tissue. The calculated strain ratio (SR) is a parameter derived from EUS-EG images and provides a quantitative measure of pancreatic tumor stiffness,. A previous study by our team showed that the SR value identified on EUS correlated with the stroma proportion and predicted the survival of pancreatic cancer patients treated with a nab-paclitaxel and gemcitabine regimen.\nMetabolic reprogramming in tumors is a hot research topic, but few relevant studies have focused on the TME in pancreatic cancer. The cellular components of the TME include cancer-associated fibroblasts (CAFs), endothelial cells, immune inflammatory cells, and marrow-derived mesenchymal stem cells (MSCs). Recent studies have reported that FDG uptake might be increased in stromal cells in many tumors,,,. Hence, we aimed to explore the metabolic reprogramming of CAFs in pancreatic cancer.\n2.1. Study population\nThe 126 enrolled patients were histologically or cytologically",
        "summary": " The majority of patients with pancreatic cancer are diagnosed at an advanced stage, and only 20% of patients are eligible for curative surgery. Abundant stroma in the tumor microenvironment (TME) is a crucial feature of the cancer, and it contributes to tumor growth, progression and chemoresistance"
    },
    {
        "link": "https://www.sciencedirect.com/science/article/pii/S2001037023001551",
        "body_split": " diagnosed with either locally advanced pancreatic cancer (stage III, LAPC) or metastatic pancreatic cancer (stage IV, MPC) at the Fudan University Shanghai Cancer Center (FUSCC) from January 2018 to December 2020. All patients underwent EUS and 18F-FDG PET-CT and only received gemcitabine (GEM)-based first-line chemotherapy. Clinical information on age, sex, tumor location, vascular involvement, lymph node or distant metastasis, and tumor markers (CA19\u20139, CA125, CA153, AFP, CEA) was extracted from our database. This study was approved by the Ethics Board of FUSCC, and due to its retrospective nature, the requirement for informed patient consent was waived.\n\n2.2. EUS-EG\nEUS-EG was conducted using a linear EUS scope (EG3870UTK; Pentax, Japan) combined with a HI VISION Preirus EUS system (Hitachi, Japan). The procedure was performed as previously described.\n\n2.3. 18F-FDG PET-CT\nPET-CT was performed on a PET-CT scanner (Biograph Duo or Biograph TruePoint; Siemens Medical, Erlangen, Germany) at our institution. The SUVmax was calculated by identifying the area of most intense uptake using the Xeleris workstation. The PET-CT images were evaluated by two nuclear medicine physicians who were blinded to the clinical data.\n\n2.4. Single-cell RNA analysis\nThe pancreatic cancer scRNA-seq datasets were downloaded from the Gene Expression Omnibus (GSE155698) and Genome Sequence Archive (AAD_CRA001160). The combination and analysis of the two scRNA-seq datasets were performed within the Seurat framework (4.2.0). Fibroblasts were selected based on their specific markers. The AUCell package was used to score the glycolytic activity of each fibroblast. The expression matrixes of two scRNA-seq datasets were integrated using the algorithm \u201cHarmony\u201d with the R package \u201charmony (0.1.1).\n\n2.5. Immunohistochemistry (IHC)\nTumor tissues were obtained from pancreatic cancer patients who underwent PET-CT before treatment and underwent radical surgery at the FUSCC (n = 10). All procedures were performed after obtaining approval from the Clinical",
        "summary": " All patients underwent EUS and 18F-FDG PET-CT and only received gemcitabine (GEM)-based first-line chemotherapy. This study was approved by the Ethics Board of FUSCC, and due to its retrospective nature, the requirement for informed patient consent was waived"
    },
    {
        "link": "https://www.sciencedirect.com/science/article/pii/S2001037023001551",
        "body_split": " Research Ethics Committee of FUSCC, and informed consent was obtained from each patient prior to the analyses. The protocol was performed as previously described. IHC staining with antibodies against CAV1 (#3238, CST), GLUT1 (66290\u20131-lg, Proteintech), HK2 (22029\u20131-AP, Proteintech), LDHA (19987\u20131-AP, Proteintech), and PKM2 (15822\u20131-AP, Proteintech) was performed to detect protein expression levels.\n\n2.6. Cell lines and reagents\nThe human pancreatic cancer cell lines PANC-1 and SW1990 were obtained from American Type Culture Collection (ATCC). The culture conditions for all cells have been described previously,. Human CAFs were isolated from pancreatic cancer tissue from FUSCC. After excision, the tumor sample was immediately transported to the laboratory on ice. Next, the tissue sample was digested with 0.1% type I collagenase and trypsin. After filtering with a 400-mesh sieve, the cell suspension was centrifuged at 1,000g for 10 min. The isolated CAFs were tested for mycoplasma presence, verified by morphology and \u03b1-SMA expression analyses, and cultured in DMEM supplemented with 10% FBS. All cells were cultured in a humidified atmosphere in 5% CO2 at 37 \u00b0C. 2-Deoxy-D-glucose (2-DG), a glycolysis inhibitor (S4701), was obtained from Selleck (Houston, TX, USA).\n\n2.7. Wound-healing assay\nThe cells were seeded into 6-well plates and grown to 90% confluence. The cell monolayer was scratched using a micropipette tip, and the cells were cultured in serum-free medium. Microphotographs were obtained at the indicated times.\n\n2.8. Transwell migration assay\nCell migration assays were performed as previously described. Cells were seeded in the upper chamber (approximately 6 \u00d7104 cells) and cultured in 200 \u03bcl serum-free medium. The lower chamber was filled with 800 \u03bcl of medium containing 10% FBS. After the plate was cultured for 24 h, the lower surface of the plate containing cells was washed, and the cells were fixed, stained and imaged. The migrating cell number was determined in randomly selected fields.",
        "summary": " The protocol was performed as previously described. IHC staining with antibodies against CAV1 (#3238, CST), GLUT1 (66290\u20131-lg, Proteintech), HK2 (22029-1-AP) and PKM2 (15822-2-AP, ProTeintech) were performed to detect protein expression levels"
    },
    {
        "link": "https://www.sciencedirect.com/science/article/pii/S2001037023001551",
        "body_split": "\n\n2.9. Statistical methods\nThe data are presented as the mean\u00b1 SD. The relationships between clinicopathological characteristics were assessed using Spearman tests. The OS rates were assessed using KaplanMeier curves, and the differences between groups were compared using the log-rank test. Univariate and multivariate Cox proportional hazards models were applied using SPSS for Windows version 24.0 software (SPSS, Inc., Chicago, IL, USA). Differences with P values (two-sided)< 0.05 were considered statistically significant.\n3.1. Characteristics of patients\nA total of 77 males and 49 females were enrolled in our study, with a median age of 60.8 years. The patients consisted of 59 LAPC and 67 MPC patients. The median tumor size, SUVmax and EUS-SR of the primary lesions were 35.9, 7.1 and 41.9, respectively, with ranges of 11\u201399.3, 0\u201327.1 and 7.9\u2013124.7. Overall, 111 patients (88%) were found to have lymph node metastasis, and 108 patients (86%) were found to have vascular involvement. All patients underwent PET-CT and EUS and received a GEM-based first-line regimen. The clinical characteristics of the patients are described in Table 1. The median overall survival (OS) was 8.9 (range 4\u201324) months.\n\n3.2. Correlation of SUVmax with duration of survival\nThe SUV is a well-known measurement for 18F-FDG uptake and the SUVmax is the largest value in the selected region of interest the trans-axial PET image. Next, we explored the correlation between SUVmax and other characteristics of patients with pancreatic cancer using Spearman tests. A high SUVmax was associated with tumor size, vascular involvement, EUS-SR, and lymph node and distant metastasis but not with sex, age or tumor location (Table 2). There was a positive correlation between SUVmax and SR (Fig. 1A). A scatter plot of the relationship of SUVmax with SR is shown in Fig. 1B (R= 0.298, P < 0.001). All factors associated with survival in the univariate analysis are shown in Table 3. There was a significant association of tumor size (>3 cm), SUVmax (>6.4), SR (>40.33), CA19\u20139 (>246 U/ml), CA125 (>27.9",
        "summary": "A total of 77 males and 49 females were enrolled in our study, with a median age of 60.8 years. The median overall survival (OS) was 8.9 (range 4\u201324) months. 111 patients (88%) were found to have lymph node metastasis, and 108 patients (86%) had vascular involvement"
    },
    {
        "link": "https://www.sciencedirect.com/science/article/pii/S2001037023001551",
        "body_split": " U/ml), CA153 (>12 U/ml), vascular involvement, lymph node metastasis and distant metastasis with poor OS. In addition, cumulative survival was significantly better in patients in the low SUVmax group (9.8 months for SUVmax<6 versus 8.2 months for SUVmax \u2265 6, p < 0.01; Fig. 1C). The relationship of cumulative survival with EUS-SR is shown in Fig. 1D (10.1 months for SR<40 versus 7.8 months for SR \u2265 40, p < 0.01). The characteristics significantly correlated with OS in the univariate analysis were entered into the multivariate analysis. The multivariate Cox regression analysis showed that tumor size, vascular involvement, distant metastasis, CA19\u20139 and SR were significant prognostic factors for OS.\n\n3.3. Metabolic reprogramming of CAFs in pancreatic cancer affects the heterogeneity of PET-CT\nBoth neoplastic and non-neoplastic components can affect the total 18F-FDG uptake of PET-CT in malignant tumors. Similar research has showed that 18F-FDG preferentially accumulates in \u03b1-SMA-positive myofibroblasts in tumor model. Recent study also showed that CAFs may influence 18F-FDG uptake in PET-CT imaging. Our previous study showed that the glucose metabolism in pancreatic cancer cells was associated with the SUVmax in PET-CT scan. In this study, we explored the role of CAFs on 18F-FDG uptake of PET-CT imaging in pancreatic cancer. The pancreatic cancer scRNA-seq datasets were downloaded from the Gene Expression Omnibus (GSE155698) and Genome Sequence Archive (AAD_CRA001160). The combination and analysis of the two scRNA-seq datasets were performed within the Seurat framework (4.2.0). Fibroblasts were selected based on their specific markers. We first annotated all cells according to their specific markers as shown in supplementary Fig. 1\u2009A. Then, we isolated fibroblast as a new Seurat object and visualize it as shown in supplementary Fig. 1B according to the fibroblast marker (DCN, TAGLN, COL3A1, COL1A1, FAP and CAV1). A dot plot showing the distribution of common fibroblast markers in selected pancreatic cancer-infiltrating fibroblasts was generated",
        "summary": "In this study, we explored the role of CAFs on 18F-FDG uptake of PET-CT imaging in pancreatic cancer. Recent study also showed that CAFs may influence 18f-FDg uptake in PET- CT imaging. We first annotated all cells according to their specific markers as shown in supplementary Fig"
    },
    {
        "link": "https://www.sciencedirect.com/science/article/pii/S2001037023001551",
        "body_split": " (Fig. 2). A UMAP plot showing six subclusters for the selected fibroblasts was created (Fig. 3A). Furthermore, the AUCell package was used to score the glycolytic activity of each fibroblast, and the fibroblast cells of subcluster 3 showed higher glycolytic activity (Fig. 3B). Caveolin-1 (CAV1), as one of the main indicators of CAF activation, was expressed at low levels in cells of subcluster 3 with a high glycolysis score (Fig. 3C). Next, we analyzed the correlation between the expression of CAV1 and glycolytic enzymes in CAFs. As shown in Fig. 4A, glycolytic enzymes, including PKM2, HK2, PGK1 and HOMER1, were highly expressed in CAFs with low CAV1 expression, suggesting that CAV1 is involved in the metabolic alteration of CAFs to favor the Warburg effect. To further verify the conclusions, we detected the expression of the above proteins in the SUVmax-high and SUVmax-low groups of pancreatic cancer patients and observed a negative correlation between CAV1 and glycolytic enzyme expression in the tumor stroma (Fig. 4B). The above results further confirmed that glucose metabolic reprogramming of CAFs in pancreatic cancer affects the heterogeneity of PET-CT.\n\n3.4. Blocking CAF glycolysis affects the progression of pancreatic cancer\nTo investigate the role of CAF glycolysis in pancreatic cancer progression, we next pretreated CAFs with 2-DG, a glycolysis inhibitor, and cocultured them with pancreatic cancer cells. Wound-healing assays revealed that the increase in cancer cell migration induced by CAFs was reversed in by 2-DG (Fig. 5A-B). Moreover, in vitro transwell assays indicated that the number of migrating cells was significantly increased in the CAF group, and CAF glycolysis inhibition decreased the progression of pancreatic cancer (Fig. 5C-D).\n4. Discussion\n18F-FDG PET-CT has been routinely used for cancer diagnosis, staging and therapeutic response monitoring. Clinical studies have demonstrated that 18F-FDG uptake reflects metabolic activity and is closely correlated with glucose metabolism in many malignant tumors. SUVmax has been shown to be a surrogate marker for prognosis",
        "summary": " AUCell package was used to score the glycolytic activity of each fibroblast. Caveolin-1 (CAV1), as one of the main indicators of CAF activation, was expressed at low levels in cells of subcluster 3 with a high Glycolysis score (Fig"
    },
    {
        "link": "https://www.sciencedirect.com/science/article/pii/S2001037023001551",
        "body_split": ". However, the major factors affecting 18F-FDG accumulation in pancreatic cancer have not been fully elucidated. The thick desmoplastic stroma is one of the defining features of pancreatic cancer, and the highly heterogeneous stroma consists of CAFs, immune cells, blood vessels and extracellular matrix. Many studies have proposed that CAFs are involved in energy metabolism in tumor cells,. Hence, this study aimed to explore the potential effect of CAF glycolysis on tumors.\nIntratumor heterogeneity of PET-CT is reflected by variation in 18F-FDG uptake. Consistent with our conclusion, the SUV derived from PET-CT has been suggested to be related to the distribution of different tissue components in the tumor and clinical parameters, including tumor size, angiogenesis, histological grade and tumor growth pattern,,. The SR obtained by EUS-EG provides a quantitative measure of tumor stiffness related to stromal proportion in pancreatic cancer. Our previous study demonstrated that the SR was correlated with the stroma proportion and predicted the survival of pancreatic cancer patients. In this study, we also clearly demonstrated that both high PET-CT-derived SUVmax and SR predicted poor OS. Interestingly, this is the first study to propose a significant and positive correlation between SUVmax and SR. We wondered whether the rich stroma of the pancreas affects the metabolic changes in tumors. Many controversial roles of the stroma in tumors have been proposed. The stroma in tumors was found to be closely correlated with 18F-FDG uptake, likely because it influences angiogenesis, microvessel density, and nutrient and oxygen supply,,. Similarly, the nonneoplastic components in tumors have been shown to contribute to total 18F-FDG uptake. 18F-FDG accumulates predominantly in the nonneoplastic stroma, especially in \u03b1-SMA-positive myofibroblasts, in various malignant tumor models,,. As CAFs are the major component of the tumor stroma, these findings suggest that glycolytic CAFs increase the total 18F-FDG uptake in tumors, and high PET-CT-derived SUVmax may be a marker for therapy targeting the neoplastic stroma. Moreover, there are several possible mechanisms accounting for the impact of CAFs with high glycolysis on killing function of cytotoxic T lymphocytes (CTLs) in dense ECM. On the one hand, an increased",
        "summary": " The thick desmoplastic stroma is one of the defining features of pancreatic cancer, and the highly heterogeneous stroma consists of CAFs, immune cells, blood vessels and extracellular matrix. Many studies have proposed that CAFs are involved in energy metabolism in tumor cells. This study aimed to explore the potential effect of CAF glycolysis on tumors"
    },
    {
        "link": "https://www.sciencedirect.com/science/article/pii/S2001037023001551",
        "body_split": " rate of glycolytic metabolism within the TME induced by CAFs can lead to glucose deficiency, which contributes to a competitive struggling between cancer cells with CTLs. In such conditions, CTLs tend to decrease their number. On the other hand, some CAF-derived secretory proteins, such as \u03b2ig-h3, can decrease the transduction of T cell receptor (TCR) signaling via binding to the surface marker of CTLs. Moreover, CAFs can also contribute to CTL killing in an antigen-dependent mechanism by activating PD-L2 an FasL.\nPrevious studies have reported that CAV1 increases tumor cell glucose uptake, lactate output and proliferation,. Recently, CAV1 was shown to be involved in metabolic reprogramming in fibroblasts,,. In our study, single-cell RNA analysis and IHC assays showed that CAV1 affected glycolytic activity, and CAV1 expression was correlated with glycolytic enzyme expression in fibroblasts in pancreatic cancer. Similarly, several glycolytic enzymes, including PGK1, PKM2, aldolase A, enolase 1, and LDHA, were upregulated in bone marrow-derived stromal cells from Cav1-knockout mice. In addition, it has been shown that CAV1-deficient stromal fibroblasts surrounding malignant cells promote aerobic glycolysis with a simultaneous increase in mitochondrial activity and enhance angiogenesis by recruiting CAV1-positive microvascular cells,,.\nThe metabolic crosstalk between CAFs and tumor cells depends on the amount of oxygen, availability of extracellular metabolites and regulation of signaling molecules. Metabolic reprogramming of CAFs promotes the growth of tumor cells, which is considered reversed metabolic symbiosis. Moreover, CAV1 expression in CAFs was found to promote CAF contractility and the migration and invasiveness of carcinoma cells. Similarly, in the present study, CAFs with high glycolytic activity contributed to pancreatic cancer cell migration, and blocking CAF glycolysis reversed this process. However, the potential regulatory mechanism is still unclear. Interestingly, CAV1 is believed to regulate constitutive activation of CAFs in fibrotic diseases by preventing collagen deposition, fibroblast proliferation and TGF-\u03b2 signaling via the PI3K/AKT, MAPK, Rho-like GTPase",
        "summary": " CAF-derived secretory proteins, such as \u03b2ig-h3, can decrease the transduction of T cell receptor (TCR) signaling via binding to the surface marker of CTLs. CAFs can also contribute to CTL killing in an antigen-dependent mechanism by activating PD-L2 an FasL"
    },
    {
        "link": "https://www.sciencedirect.com/science/article/pii/S2001037023001551",
        "body_split": ", or JNK pathways,. Furthermore, CAF subpopulations existing in the tumor stroma play different roles according to their CAV1 expression.\nThe present study had several potential limitations. First, the number of samples used in this study was small, and further large-scale prospective studies are needed. Second, patients with pancreatic cancer were not included in the surgical resection group, and thus, the results may be biased. Third, the mechanism by which CAV1 affects CAF glycolysis remains unclear, and additional clinical and basic research studies are needed.\n5. Conclusion\nIn conclusion, our research demonstrated that SUVmax was positively associated with EUS-SR in pancreatic cancer. Increased numbers of glycolytic CAFs with decreased CAV1 expression promoted tumor progression, and high SUVmax may be a marker for therapy targeting the neoplastic stroma. Further studies should be helpful to clarify the underlying mechanisms.\nEthical approval\nThe Institutional Research Ethics Committee of FUSCC approved this study, and written informed consent was obtained from all patients prior to the investigation. We confirmed that the experiments on human tissue samples were performed in accordance with relevant guidelines and regulations.\nFunding\nThis study was jointly supported by the National Natural Science Foundation of China (No. 82002541), the Shanghai Sailing Program (No. 20YF1409000), Scientific Innovation Project of Shanghai Education Committee (2019\u201301\u201307\u201300\u201307-E00057), Clinical Research Plan of Shanghai Hospital Development Center (SHDC2020CR1006A), and Xuhui District Artificial Intelligence Medical Hospital Cooperation Project (2021\u2013011).\nCRediT authorship contribution statement\nQCM and RT performed the bioinformatic analysis. ZLF and XQM cultured the cancer cells and performed the functional experiments. CL and JH carried out the IHC assays. QCM and WW performed the statistical analyses. QCM, SS and JX designed the study. JX and XJY revised the manuscript. All authors read and approved the final manuscript.\n",
        "summary": "The present study had several potential limitations. The number of samples used in this study was small. Patients with pancreatic cancer were not included in the surgical resection group. The mechanism by which CAV1 affects CAF glycolysis remains unclear"
    },
    {
        "link": "https://www.sciencedirect.com/science/article/pii/S2405844023023447",
        "body_split": "1. Introduction\nThe COVID-19 outbreak has been the most significant pandemic of the 21st century [1], with hundreds of millions of reported cases and over five million deaths worldwide as of 2021 [2]. Though reverse transcription-polymerase chain reaction (RT-PCR) is the reference standard method to identify patients with a COVID-19 infection, Chest X-ray (CXR) and Computed Tomography (CT) have been extensively used in diagnosis, monitoring, and treatment decisions regarding COVID-19 cases [3], [4], [5]. Pneumonia is the most common radiological manifestation of COVID-19, which can be detected using CXR images [6], [7]. Many thoracic imaging societies like the Radiological Society of North America state that routine CT for the identification of COVID-19 pneumonia is currently not recommended in the diagnosis of COVID-19 unless the patient is seriously ill [8], [9], [10]. Moreover, X-ray images are preferable for COVID-19 case detection because they are captured faster at low cost and are more readily available than CT images [10], [11], [12]. Manual diagnosing pneumonia in X-ray images is a challenging, time-consuming process and has poor diagnostic performance [13], [14]. Recently, a variety of Machine Learning (ML) based COVID-19 detection methods using X-rays have been developed and implemented [15]. Pneumonia with X-rays could be detected as the first stage of COVID-19 disease [16]. Some models use AI and computer vision associated with the CXR imagery of patients to identify if the patients are diagnosed as COVID-19 positive [17]. The second stage of the analysis is designed to detect if the pneumonia is caused by COVID-19. Antagonistically, modern technology such as AI with imagery inputs like heatmaps and similar data can be used by physicians as decision support tools to minimize human errors and increase diagnosis efficiency [18], [19]. For several decades, AI has been used by academia and industry; it is inspired by human mental learning by mimicking the brain's cognitive features to learn and make decisions like a human artificially.\nTraditional AI models function as black-box models for most researchers and professionals using them for various tasks, including medical diagnostic purposes. Such traditional AI methods lack details and explanations to help physicians make better decisions and interpretations. Explainable AI (XAI) provides this opportunity, which",
        "summary": " Pneumonia is the most common radiological manifestation of COVID-19, which can be detected using CXR images. The second stage of the analysis is designed to detect if the pneumonia is caused by CO VID-19"
    },
    {
        "link": "https://www.sciencedirect.com/science/article/pii/S2405844023023447",
        "body_split": " transfers the AI-based black-box models to more explainable and transparent gray-box models. The major limitations of all the mentioned methods are that they cannot: 1) analyze the level of COVID-19 cases and 2) provide sufficient insights regarding model details. This study proposes a model for COVID-19 case detection and its interpretation using XAI, depicted in Fig. 1. The contributions of the proposed framework are: (i) Detection and classification of COVID-19 cases from affordable CXR images, (ii) Automatic interpretation of COVID-19 cases using a LIME-based heatmap implementation with XAI from X-ray images to assist clinicians and radiologists.\nThe proposed model utilized lung segmentation, transfer learning, and data augmentation technique for faster and adequate model training. A pre-trained network comparison was performed where the ResNet model achieved the highest classification performance (F1-Score: 98%).\nThe remaining sections of this paper are organized as follows. Related works are introduced in Section 2. Section 3 presents data collection and processing steps and the validation methods. Section 4 provides details about the methodology and the implementation of the proposed model. The results and related discussions are examined in Section 5. New trends and future work are provided in Section 6. Conclusions are drawn in Section 7.\n2. Related works\nRecently, there have been many studies that utilize ML techniques to combat COVID-19 pandemic. For instance, the authors in [20] proposed multi-level thresholding with a Support Vector Machine (SVM) classifier for the early detection of COVID-19 cases. Firstly, features were extracted using a multi-level thresholding technique. After that, an SVM classifier was applied to the extracted features of 40 contrast-enhanced CXR images, and classification accuracy was obtained at 97%. In another study [21], the authors applied an improved SVM classifier to detect COVID-19 cases. They collected an image dataset from 235 patients, of which 43% were confirmed COVID-19 cases. Five ML algorithms, i.e., logistic regression, random forests, gradient boosting trees, neural networks, and SVM, were trained with 70% of the dataset and evaluated their performances with 30%. The results showed that the SVM classifier performs the best in detecting COVID-19 cases compared with other conventional ML methods with an accuracy of 85%. In [22], Random Forest and XGBoost algorithms were applied to the X",
        "summary": " Study proposes a model for COVID-19 case detection and its interpretation using XAI, depicted in Fig. 1. The contributions of the proposed framework are: (i) Detection and classification of COVID.19 cases from affordable CXR images. The authors applied an improved SVM classifier to detect CO VID-19 cases"
    },
    {
        "link": "https://www.sciencedirect.com/science/article/pii/S2405844023023447",
        "body_split": "-ray images to detect COVID-19 cases. The results showed that XGBoost, with an accuracy of 97.7%, provides similar performance to the Random Forest method, with an accuracy of 97.3%. Advanced learning methods based on Convolutional Neural Networks (CNN) have also been proposed and employed to detect COVID-19 cases using X-ray images to overcome the limitation of the conventional ML approaches. Ozturk et al. [23] proposed a Deep Learning (DL) model for the early detection of COVID-19 cases using X-ray images. The proposed model consists of 17 convolutional layers and five pooling layers using Maxpool. Moreover, these layers have different filter numbers, sizes, and stride values. The model was employed on 1125 X-ray images, including 125 for the COVID-19 class, 500 for the pneumonia class, and 500 for the normal class. The model provided a classification accuracy of 98.08% for binary classes and 87.02% for multiclass classification. Toraman et al. [24] proposed Convolutional Capsule Network architecture (CapsNet) to detect COVID-19 cases using CXR images. The method was applied to a dataset containing X-ray images containing COVID-19 [25], No-Findings, and pneumonia [26]. The results showed that the CapsNet approach provides highly accurate diagnostics for COVID-19 with 97.24% and 84.22% for binary and multiclass classification, respectively. In [27], a CNN model has been designed and developed using EfficientNet architecture to automatically diagnose COVID-19 cases with X-ray images. The proposed model uses EfficientNet with 10-fold stratified cross-validation, which was applied to classify binary multiclass cases using X-ray images containing COVID-19, pneumonia, and normal patients. The proposed method achieved an average recall result of 99.63% and 96.69% for binary and multiclass classification, respectively. A DL-based ML method has been developed by Apostolopoulos et al. to detect COVID-19 cases [28]. The method was applied for both binary and multiclass analysis, and they used a dataset composed of 224 COVID-19 X-rays, 700 bacterial pneumonia, and 500 no-findings images. The proposed model finds high accuracy results, which are 98.78% and 93.48% for binary (COVID-19 vs. No-find",
        "summary": " XGBoost, with an accuracy of 97.7%, provides similar performance to the Random Forest method. Advanced learning methods based on Convolutional Neural Networks have also been proposed and employed to detect COVID-19 cases using X-ray images. The proposed model consists of 17 convolutional layers and five pooling layers"
    },
    {
        "link": "https://www.sciencedirect.com/science/article/pii/S2405844023023447",
        "body_split": "ings) and multiclass (COVID-19 vs. No-findings vs. pneumonia), respectively. Moreover, Hemdan et al. [29] designed a COVID-19 case detection method based on DL using X-ray images, and the proposed method was compared with seven other DL-based COVID-19 case detection methods. The method was performed for only binary class classification, and an accuracy rate of 74.29% was estimated. Three different automated COVID-19 case detection methods have been developed based on three different DL models, which are ResNet50, InceptionV3, and InceptionResNetv2 in [30]. The developed methods were applied for binary class classification only, and the highest accuracy rate was achieved by ResNet50 with an average of 98%. Islam et al. [31] proposed a combination of two different methods, which are CNN and long short-term memory (LSTM), for detecting COVID-19 cases using X-ray images. In the proposed approach, CNN was first applied to the X-ray images to extract the features. After the obtained features were used by LSTM to classify COVID-19 cases. The method was performed on a collection of 4,575 X-ray images, including 1525 images of COVID-19. The experimental results indicated that the CNN-LSTM performs better than the state-of-the-art methods with an accuracy of 99.4%. Loey et al. [32] used a Generative Adversarial Network (GAN) with deep transfer learning to diagnose COVID-19 from X-ray images. The proposed approach used three different transfer learning pre-trained models, i.e., AlexNet, GoogleNet, and RestNet18. The method was performed on a collection of datasets consisting of 69 COVID-19, 79 pneumonia bacterial, 79 pneumonia viruses, and 79 normal cases. The experimental results showed that using GAN with pre-trained GoogleNet provides the highest accuracy rate with 99.9% for binary class classification problems. Bandyopadhyay et al. [33] developed a hybrid model based on two different ML methods, LSTM and Gated Recurrent Unit (GRU), to detect COVID-19 cases automatically. The proposed method obtained 87% accuracy for the confirmed COVID-19 cases. A DL method was presented in [34] to automatically classify COVID-19 cases from CXR. The proposed model achieved an accuracy",
        "summary": " Hemdan et al. designed a COVID-19 case detection method based on DL using X-ray images. The proposed method was performed for only binary class classification, and an accuracy rate of 74.29% was estimated"
    },
    {
        "link": "https://www.sciencedirect.com/science/article/pii/S2405844023023447",
        "body_split": " of 89.5%, a precision of 97%, and a recall of 100% for COVID-19 cases. In [35], a multi-dilation DL approach (CovXNet) for automatic COVID-19 and other pneumonia case detection from CXR images was proposed. Experiments were performed on two different datasets to evaluate the performance of the CovXNet. The first dataset consisted of 5,856 X-ray images, and another dataset contained 305 X-ray images of different COVID-19 patients. The results showed that the CovXNet method achieved an accuracy of 97.4% for COVID/Normal detection and an accuracy of 96.9% for binary class, and 90.2% for multiclass classification. Other DL methods have been designed and developed based on different pre-trained models such as VGG16, VGG19, ResNet50, DenseNet121, Xception, and capsule networks [36], [37], [38], [39], [40], [41], [42]. Generally, existing approaches attempt to resolve binary and multiclass COVID-19 cases classification problems.\n3. Data collection, preprocessing, and validation\nThe collected images for COVID-19 cases were preprocessed using various methods. This section explains them, including the data collection, preprocessing, validation, and test/computational environment.\n3.1. Data collection\nThe publicly accessible GitHub dataset of CXR and CT images for lung disease patients suspected of having COVID-19 or other viral and bacterial conditions such as MERS, SARS, and ARDS is available [25]. This dataset is gathered from both public sources and indirectly from hospitals and physicians [43]. In this research, the X-ray scan images have been used to create an XAI-based COVID-19 detection model, while the CT images have been disregarded. Also, low-quality photos and pictures with foreign objects (metals, cables, etc.) were omitted. First, the selected X-ray scan images were rescaled to 512\u00d7512. Second, various image enhancement techniques were applied to produce enhanced input images, including flipping (right/left and up/down), rotation and translation with five random angles. Our previous work [44] had only 50 positive and 50 negative X-ray scan images for the training and 20 positive and 20 negative samples for testing. In this study, we have benefitted from the existing data repositories to extend and improve the",
        "summary": " A multi-dilation DL approach (CovXNet) for automatic COVID-19 and other pneumonia case detection from CXR images was proposed. Experiments were performed on two different datasets to evaluate the performance of the CovXNet. The results showed that the Cov XNet method achieved an accuracy of 97"
    },
    {
        "link": "https://www.sciencedirect.com/science/article/pii/S2405844023023447",
        "body_split": " classifier's performance and added an explainer. Another issue with the dataset is class distribution. This dataset contains X-ray scan images from those infected by COVID-19 and other diseases. There were only three records in the dataset with COVID-19 negative samples. Therefore, X-ray pictures with ARDS and Streptococcus results were labeled as COVID-19 negative samples. In this study, 6,000 images are collected from the GitHub repositories mentioned above to increase the number of training and testing samples in the dataset. Thus, an analysis using different neural network structures is conducted, similar to the previous study. This study created a binary classification model by marking labels other than COVID-19 as the \u201c0\u201d class. Of these 6,000 samples, 5,500 are COVID-19 negative, and the rest are COVID-19 positive X-ray images. Furthermore, 1,200 of them were used for testing, and the remaining 4,800 for training.\n\n3.2. Data preprocessing\nCollected raw data were processed with various techniques to increase the model's classification performance. Samples of the dataset are depicted in Fig. 2.\nTo improve the classification model's performance and increase the number of samples in the dataset, various image augmentation techniques were employed. The parameters used were a rotation range of 20 degrees, zoom range of 15, width shift range of 0.2, height shift range of 0.2, shear range of 0.15, and horizontal flipping. An example of the image augmentation techniques is illustrated in Fig. 3.\n\n3.3. Validation\nTo assess the performance of the COVID-19 detection part of the framework, we utilized the following performance metrics, F1-Score, recall, precision, and accuracy extracted from the confusion matrix shown in Table 1.\nPerformance measures are given in Eqs. (1)\u2013(4) below. (1) Precision= True Positive True Positive+False Positive (2) Recall= True Positive True Positive+False Negative (3) F1-Score=2\u22c5 Precision\u22c5Recall Precision+Recall (4) Accuracy= True Negative+True Positive True Positive+False Positive+True Negative+False Negative\nEvaluation of the explanation part of the framework was performed by an MD specialized in COVID-19 by reviewing the test images. Generated heatmaps were reviewed and evaluated one by one by the medical professional.\n\n",
        "summary": "6,000 images are collected from GitHub repositories to increase the number of training and testing samples in the dataset. This study created a binary classification model by marking labels other than COVID-19 as the \u201c0\u201d class. The parameters used were a rotation range of 20 degrees, zoom range of 15, width shift range of 0"
    },
    {
        "link": "https://www.sciencedirect.com/science/article/pii/S2405844023023447",
        "body_split": "3.4. Test computational environment\nThe proposed COVID-19 XAI framework in this study was implemented using the Keras DL framework built with Python 3.6. The workstation \u2013 an Intel\u00ae Core\u2122 i7-8700 processor @3.20 GHz \u2013 was used to run AI-based models, which has 32 GB memory and GTX 1080 GPU (NVIDIA GeForce). The classification part of the model is trained for 60 epochs, whereas the lung segmentation part is trained for 50 epochs since the accuracy and loss values do not improve notably after these epochs. The complete training of the framework took around 3 hours. We used a constant learning rate of 0.001 and the \u201cRMSprop\u201d optimizer to train classification parts. The segmentation is realized using an \u201cadam\u201d optimizer with a 0.005 learning rate. XAI module training takes around 1 minute on average.\n4. Methodology and implementation\nCOVID-19 detection has been explored in many studies [45]. In contrast to existing approaches, lung segmentation is utilized in the proposed pipeline to enhance the COVID-19 detection and explanation tasks. It forces the system to better learn the features inside the lung region with improved training. Further, it provides greater emphasis on the lungs during the explanation phase of the framework. After this step, transfer learning is adopted to speed up and ease the feature extraction since the number of X-ray images is limited. Following feature extraction, X-rays are classified, and the classification performance of the model is measured. The framework's third stage focuses on explaining the COVID-19 cases. The COVID-19 positive cases are then fed into the LIME-based heatmap explanation part of the pipeline to spotlight the areas with COVID-19 pneumonia to help physicians during the diagnosis in a non-invasive manner. The following sections discuss lung segmentation, transfer learning models for classification, and XAI tools used in this study.\n4.1. Lung segmentation\nAnomalies in the lung provide information about many diseases. Our study examines the CXR to determine whether the patient has COVID-19. An additional lung segmentation part is added to the proposed pipeline to increase the performance of the detection and explanation part of the proposed model. Manual segmentation is time-consuming and not available for many biomedical applications. Also, human annotations are prone to cause inconsistencies as well as to make mistakes. With lung segmentation, COVID-19 detection and",
        "summary": " The proposed COVID-19 XAI framework in this study was implemented using the Keras DL framework built with Python 3.6.6. The workstation \u2013 an Intel\u00ae Core\u2122 Core\u2122 i7-8700 processor @3.20 GHz \u2013 was used to run AI-based models"
    },
    {
        "link": "https://www.sciencedirect.com/science/article/pii/S2405844023023447",
        "body_split": " its explanation networks are fed with masked lung images, which force these parts to detect and explain only in the lung section of the CXR. The output of the explanation part is shown in the whole CXR for a better interpretation.\nA reference hybrid U-Net [46] architecture that uses a pre-trained VGG11 feature extractor is utilized in the encoder part of U-Net to obtain lung segmentation. Pre-trained networks on a large dataset, i.e., ImageNet, outperform the networks trained from scratch. The U-Net architecture consists of an encoder and decoder structure with skip connections to carry low-level feature maps to the decoder. Concatenating feature maps from encoder to decoder improves the performance and convergence of the network. To train this model, publicly available lung segmentation images are used [47], and various augmentation techniques such as horizontal and vertical shift, minor zoom, and padding are applied. The lung segmentation model has a Jaccard index of 92% and a dice score of 96%. Additionally, morphological operations, i.e., dilation, are implemented to ensure the correct segmentation of the lungs with a kernel size of 90\u00d730 and three iterations using the OpenCV function, cv2.dilate (bc, kernel, iterations = 3). Our aim is to segment and interpret all parts of the lung in X-ray images using the proposed approach. Including the perimeter outside the lung does not have an important effect, but the entirety of the lung has a great impact on classification and especially on explanation tasks. Therefore, we have applied dilation operations. Lung segmentation is used as a pre-processing method to increase the performance of COVID-19 classification and explanation tasks. The comparison of the model results with/without lung segmentation is summarized in Table 2.\n\n4.2. Transfer learning\nTransfer learning is a widely used technique in machine learning that simplifies the process of building models. It involves utilizing knowledge gained from a previously trained model on a related task and applying it to a new, but related problem. This approach is based on extracting features from the input data obtained from a related initial task and transferring these features to the new task to improve accuracy and reduce training time. Specifically, pre-trained DL network models have already been trained on large datasets and have achieved high accuracy, and these pre-trained models can be used as a starting point for the new task. The transfer learning process starts with",
        "summary": " explanation networks are fed with masked lung images, which force these parts to detect and explain only in the lung section of the CXR. Pre-trained networks on a large dataset, i.e., ImageNet, outperform the networks trained from scratch. The U-Net architecture consists of an encoder and decoder structure with skip connections"
    },
    {
        "link": "https://www.sciencedirect.com/science/article/pii/S2405844023023447",
        "body_split": " the previously learned patterns while solving a different problem. Further, it decreases the time-consuming training process and enables the creation of a model with high classification performance. These pre-trained models are based on Deep Evolution Neural Networks (DENN). In deep learning, this method involves initially training a CNN for a classification problem using large-scale training datasets. Since a CNN model can learn to extract the image's discriminative features, the availability of initial training data is an essential part of successful training. The model performance evaluation depends on the model's fitness for transfer learning, which relies on CNN's capacity to select the most important image features.\nThe VGG-Net model was used in both the segmentation and detection part of this framework, which was developed with a tiny convolution in the neural network by Simonyan et al. [46]. Compared to previous models, the most significant difference is the widespread use of CNN models due to their deeper structure, which typically includes multiple layers of convolution and association. This model consists of nearly 138 million parameters. VGG is one of the popular networks, which is trained with more than a million images from the ImageNet dataset with 1,000 different classes. Therefore, the model can be applied as a helpful tool for the feature extractor of new images.\nThe ResNet (residual networks) won the ImageNet challenge in 2015 and was proposed by He et al. [48] with a paper titled \u201cDeep Residual Learning for Image Recognition\u201d. The version that is used in this model has 50 neural network layers and was trained on the ImageNet dataset having 1,000 different classes. Increased layer quantity brings some challenges, such as model complexity and vanishing gradient. ResNet was inspired by the VGG networks, but it has fewer filters and less complexity. The vanishing gradient problem was mitigated by the skip connections, which allow gradients to flow through alternative paths. This method is the core concept in residual blocks of ResNet to alleviate the vanishing gradient problem. The ResNet50 model has more than 23 million parameters.\nThe Inception V3 model was developed by Szegedy et al. with a paper titled \u201cRethinking the Inception Architecture for Computer Vision\u201d published in 2015 [49]. This iteration of the inception architecture is more computationally efficient than the previous models. Larger convolutions are changed within parallel smaller convolutions. Additionally, factorized convolutions and an auxiliary classifier are utilized to improve the model's performance.",
        "summary": " These pre-trained models are based on Deep Evolution Neural Networks (DENN) In deep learning, this method involves initially training a CNN for a classification problem using large-scale training datasets. The model performance evaluation depends on the model's fitness for transfer learning, which relies on CNN's capacity to select the most important image features"
    },
    {
        "link": "https://www.sciencedirect.com/science/article/pii/S2405844023023447",
        "body_split": " A new grid size reduction was proposed to combat bottlenecks of expensive computation.\nOur study used VGG16, VGG19, ResNet, and Inception V3 neural network models to improve the performance of our X-ray image based on the COVID-19 detection model.\n\n4.3. Explainable artificial intelligence (XAI)\nAI has diverse applications and provides unprecedented advantages, such as higher efficiency and broader data analysis, for many daily tasks such as manufacturing, finance, and entertainment [50], [51]. However, the use of AI is lagging in high-risk systems, especially in healthcare [52]. The inner workings of AI systems comprise complicated mathematical and statistical processes, which are not interpretable. Fortunately, a black-box AI model can be converted to a glass-box model by applying explainable AI tools.\nAI models are converted to more understandable systems by making them interpretable or comprehensible. Shallow Learning (SL) methods such as decision trees and regression algorithms are more transparent as the mathematical backgrounds are well-defined and studied. These methods are interpreted by utilizing the underlying math. On the other hand, the inner workings of DL methods, such as CNNs and Recurrent Neural Networks (RNNs), are conceived by finding the relationship between the inputs and the outputs. DL methods consist of nodes and weights associated with the inputs and outputs. This relationship should be clarified to mitigate risks and build trust in AI models for enhanced adoption. A comprehensive program driven by the DARPA showed that XAI improves user trust significantly and increases user adoption through the provided explanation [53], [54].\nGrad-Cam, Tylor decomposition, and LIME are some of the XAI tools used to make the AI models more understandable. Our model provides a LIME-based heatmap explanation method to detect and find COVID-19 in CXR scans.\nIn this study, the LIME model-independent general XAI method is used, which finds the statistical connection between the inputs and the outputs of the models. Inputs are perturbed during the training of local surrogates to understand their effects on the output instead of globally training them. This process results in the instance of an interpretable representation and visualization. The mathematical definition of LIME is given in the equation as follows: (5) Explanation(x)=argmi n g\u2208G L(f,g, \u03c0 x )+\u03a9(g)\nWhere:\nx: An",
        "summary": "Study used VGG16, VGG19, ResNet, and Inception V3 neural network models to improve the performance of our X-ray image based on the COVID-19 detection model. The inner workings of AI systems comprise complicated mathematical and statistical processes, which are not interpretable"
    },
    {
        "link": "https://www.sciencedirect.com/science/article/pii/S2405844023023447",
        "body_split": " instance of out of the data space for which we desire an explanation for its predicted target value\nL(f,g, \u03c0 x ) : Fidelity function that measures how unfaithful the g is while approximating to f in the locality defined by the \u03c0 x. It is the locality-aware loss.\n\u03a9(g) : Measures the model complexity of the explainer (g)\nf: The black-box model to be explained\ng: The explainer\nG: The total set of interpretable models\n\u03c0 x : Proximity measure\nLocality aware loss ( L(f,g, \u03c0 x ) ) is minimized for local faithfulness with low complexity of the second term ( \u03a9(g) ) for interpretability.\nAfter obtaining the explanations, the areas determined by the LIME are fed into the heatmap creation part of the explainer. These areas are highlighted with heatmaps to spotlight the areas with COVID-19 pneumonia to provide additional information to the physicians during the diagnosis of COVID-19. In comparison to the regular LIME output [55], our XAI part takes the LIME output one step further to better localize the COVID-19 affected areas as shown in Fig. 4.\n5. Results and discussions\nUnder X-rays, dense structures such as bones and metal blocks are seen as white since they block the X-rays. Less dense areas appear in tones of gray, and the least dense regions, such as lungs, will be black. Healthy and COVID-19 CXR images are shown in Fig. 5.\nCXR images of healthy lungs are shown as black (see Fig. 5a), whereas the areas with COVID-19 are shown in white. The complications in the lungs can be detected by examining the CXR. A CXR with COVID-19 has more white areas spread over the lungs, as shown in Fig. 5b.\nClassification performances of seven DL models with/without transfer learning and with/without segmentation are given in Table 2. The models (VGG16, VGG19, ResNet, InceptionV3) are developed using transfer learning with getting lung-segmented CXR images as inputs. Performance results of another pre-trained ResNet model without lung segmentation and the model without transfer learning are also compared in Table 2. Weighted averages are used to take the sample size into consideration to provide a more accurate representation",
        "summary": "L(f,g, g, \u03c0 x) measures how unfaithful the g is while approximating to f in the locality defined by the \u03c0x. It is the locality-aware loss ( L(f),g) is minimized for local faithfulness with low complexity of the second term ( \u03a9(g) ) for interpretability"
    },
    {
        "link": "https://www.sciencedirect.com/science/article/pii/S2405844023023447",
        "body_split": " of the averages during the calculations.\nIn our previous studies [44], [58], we investigated whether the transfer learning models could be used in detecting COVID-19 positives in the X-ray data to increase the model's classification performance without XAI, which we now focus on in the present study. The results showed we could detect COVID-19 from X-ray images in a similar manner to the current techniques for other imaging approaches, although this was more limited than explainable. In this study, we used the cognitive learning approach to confirm and measure the quality of the predictions based on those of medical doctors, which provides a generalizable model to evaluate the validity of the projections in the X-ray data and the use of the XAI techniques. Table 2 indicates that the pre-trained ResNet transfer model is better suited for detecting COVID-19 with 98% accuracy. The VGG16 and VGG19 models indicate a similar classification performance with an accuracy of 96%. The InceptionV3 model had the lowest classification accuracy at 91%. The VGG16, VGG19, ResNet, and InceptionV3 models performed comparably in COVID-19 classification with F1 scores of 0.96, 0.92, 0.98, and 0.90, respectively. The ResNet model presents the highest performance in classification, i.e., 98% F1-score. It also suggests that the high F1 classification metric performance is more effective in predicting the best classification performance.\nAdditionally, Table 2 compares models with/without transfer learning and lung segmentation, i.e., the ResNet model without lung segmentation and other studies without transfer learning [36], [57]. This comparison indicates that transfer learning provides better resource management and improved efficiency during training. Models without transfer learning require far more complicated models and high-performance computational sources, which also consume more time. However, models with transfer learning do not rely on high computational power as much as those without transfer learning. The previously trained networks are taken as a tool to solve the problem at hand efficiently and faster without expensive and extensive resources. Our proposed model outperforms both of the models without transfer learning [36], [57]. In addition, the proposed model has better performance than ResNet without lung segmentation.\nFig. 6a shows a COVID-19-positive CXR image. The lung on the left has a pattern, with whiter areas showing more dense pneumonia regions. These areas are well detected and highlighted with a",
        "summary": " The ResNet model presents the highest performance in classification, i.e., 98% F1-score. Table 2 indicates that the pre-trained ResNet transfer model is better suited for detecting COVID-19 with 98% accuracy. The InceptionV3 model had the lowest classification accuracy at 91%"
    },
    {
        "link": "https://www.sciencedirect.com/science/article/pii/S2405844023023447",
        "body_split": " heatmap by the proposed method in Fig. 6b.\nThe COVID-19-positive patient CXR image is highlighted in Fig. 7. The model correctly identified the disease and stressed the infected regions by pneumonia for better understandability in a low-quality image.\nThe pneumonia infection areas are depicted and highlighted in Fig. 8a and Fig. 8b, respectively.\nAnother CXR image indicating and highlighting the areas with pneumonia is shown in Fig. 9 with cables and other medical devices. Our model is resilient to these external elements.\nThe proposed model successfully identifies the COVID-19 positive case; however, the explanation of the CXR image is slightly off the correct place in Fig. 10. The heatmap indicates the infected areas with the perimeter of the lung on the left. The whiter areas on the lung perimeter are considered pneumonia.\nThe deficiencies due to the reflections were eliminated by introducing lung segmentation to the pipeline. The heatmap provided a better explanation for pneumonia areas where it is impossible to notice the color difference with the naked eye. Also, for most X-rays, it is evident that there is still room for improvement with the use of extensive data collection and labeling. The limited color change is observed on one side of the lung, which could be improved as well. In addition, it can be studied for different color scales that will create more contrast of the increased different heat zones specific to the limited area with the lesion in the heatmap. Many countries and regions in the world cannot access tomography. For these, direct radiographs can be the only diagnostic tool. Furthermore, computed tomography cannot be repeated one by one due to high radiation exposure. That's why a well-working heatmap can be very helpful. With the help of this study, even those who are not very experienced can easily see the area where the lesions are. The following observations are deducted from the overall results:\nObservation 1 COVID-19 could be detected by AI using transfer learning with higher performance (F1-Score: 98%).\nObservation 2 Application of an XAI tool, i.e., LIME, makes the COVID-19 detection AI model more understandable.\nObservation 3 The XAI application on CXR images has a high potential for faster diagnosis and prognosis of COVID-19.\nObservation 4 The treatment of COVID-19 can be tracked more easily by applying the proposed model to the CXR images. We would like to emphasize",
        "summary": " The heatmap provided a better explanation for pneumonia areas where it is impossible to notice the color difference with the naked eye. For most X-rays, it is evident that there is still room for improvement with the use of extensive data collection and labeling"
    },
    {
        "link": "https://www.sciencedirect.com/science/article/pii/S2405844023023447",
        "body_split": " that many learning-based methods have been designed and proposed to classify COVID-19 cases, and these methods have been compared with those of radiologists. The results show that learning-based systems provide better results in terms of precision and time [59], [60], [61].\nObservation 5 The application of XAI methods enables the adoption of AI applications in high-risk industries such as healthcare.\nObservation 6 Segmenting lung images as a preprocessing step improves the COVID-19 detection performance and its explanation.\n6. New trends and future work\nUnder normal conditions, a well-trained physician can detect a pneumonia case by looking at the CXR and providing diagnosis results relatively quickly without investigating thousands or millions of X-ray images. This study is motivated by the power of mental modeling, which is performed by the human brain to understand the concept of pneumonia and its causes by comprehending the associated facts such as human anatomy, fundamentals of virology, how lungs and ribs function, and other information learned during their medical education at school or in a clinic. Researchers and engineers have developed various XAI tools to help professionals and academia improve their understanding and insights regarding AI-based implementations. The following XAI tools have significant potential to be implemented primarily in the field of medical sciences, where image processing and clustering play an important role: \u2022 Local Interpretable Model-Agnostic Explanations (LIME) \u2022 Class activation mapping (CAM) \u2022 Deep SHapley Additive exPlanations (Deep SHAP) \u2022 LRP (Layer-wise Relevance Propagation) \u2022 DeepLIFT (Deep Learning Important FeaTures)\n7. Conclusion\nThis paper presents an XAI approach for COVID-19 diagnosis using transfer learning with CXR images. The proposed model supports decision-making for COVID-19 cases, i.e., positive and negative. The framework accepts a CXR image as the input and predicts the COVID-19 classification and its explanation as the output. To improve the classification and explanation performances, a lung segmentation model is realized, and its output, segmented lung images, is fed to the framework. An XAI approach, i.e., LIME, is used to faithfully describe predictions of COVID-19 cases in an interpretable manner through heatmaps. The proposed model is also extended through the LIME and heatmap methods to offer better explainability. XAI tools help non-expert end-users understand the black",
        "summary": " The application of XAI methods enables the adoption of AI applications in high-risk industries such as healthcare. Researchers and engineers have developed various XAI tools to help professionals and academia improve their understanding and insights regarding AI-based implementations. The results show that learning-based systems provide better results in terms of precision and time"
    },
    {
        "link": "https://www.sciencedirect.com/science/article/pii/S2405844023023447",
        "body_split": " box AI model by providing explainability and transparency. It provides feedback to the end-user and explains, i.e., by providing more information and tracing the insight right back to the inner workings of the black box AI model.\nThe internal working of AI models, especially the deep learning models, are black box concepts that cannot be explained why the AI model outputs a specific result. Our model requires lung segmentation before classification and explanation, extending the overall processing time. It is also trained on a limited number of CXR images. Additional data, CXR images, will increase the robustness and performance of classification. In addition to this, our model's XAI part has limitations while interpreting the CXR images. Our model first needs to classify the COVID-19 CXR images from the healthy ones; then, it will provide heatmaps indicating the areas with COVID-19 pneumonia. When a healthy CXR image is given to the XAI part of the model pipeline, the model tries to find COVID-19-affected areas and provides the highlighted results, which are the closest COVID-19-affected areas. Therefore the classification part is also critical in this project.\nThis study demonstrated how XAI techniques could be helpful for COVID-19 diagnosis in the healthcare domain when assessing trust and gaining insights into predictions. The proposed hybrid model provides two outputs: (1) COVID-19 diagnosis and (2) Model-decision explanation. Interpretation of the results obtained from the proposed XAI module offers adequate information about COVID-19 diagnosis. This study is expected to benefit researchers and physicians working on COVID-19 diagnosis or related studies by providing insight into XAI's potential.\nCRediT authorship contribution statement\nSalih Sarp, Ferhat Ozgur Catak, Murat Kuzlu: Conceived and designed the experiments; Performed the experiments; Analyzed and interpreted the data; Contributed reagents, materials, analysis tools or data; Wrote the paper.\nUmit Cali, Huseyin Kusetoglu, Ozgur Guler, Yanxiao Zhao: Contributed reagents, materials, analysis tools or data; Wrote the paper.\nGungor Ates: Analyzed and interpreted the data; Contributed reagents, materials, analysis tools or data; Wrote the paper.\nFunding statement\nThis research did not receive any specific grant from funding agencies in the public, commercial, or not-for",
        "summary": "The internal working of AI models, especially the deep learning models, are black box concepts that cannot be explained why the AI model outputs a specific result. Our model requires lung segmentation before classification and explanation, extending the overall processing time"
    },
    {
        "link": "https://www.sciencedirect.com/science/article/pii/S2405844023023447",
        "body_split": "-profit sectors.\n",
        "summary": "profit sectors.profit sectors are key sectors in the U.S. and Europe"
    },
    {
        "link": "https://www.sciencedirect.com/science/article/pii/S2405844023025458",
        "body_split": "1. Introduction\nMining is a crucial part of national development, and its success is based on the development of national infrastructures such as power, transportation, telecommunications, and human capabilities. Even though previously found ores were of good quality and required little or no further processing, they sufficed to meet mankind's mineral needs for almost 3000 years [,]. Most of these ores had been consumed by the end of the industrial revolution, leaving only those that required enrichment, purification, or beneficiation []. Advanced technology is critical in beneficiating these natural raw solid minerals for the government to generate the necessary revenue and expand its economic potential. The existing process technique employed by these miners does not offer good, accurate, or quality results, especially in developing countries. In Nigeria, approximately 80% of the mining was done locally by using shovels, hoes and diggers [,, ]. This is especially true in the states of Nasarawa, Benue and Cross Rivers, where barite mining is prevalent. Thus, mineral extraction such as barite contributes to less than 1% of the country's GDP [,,, ]. The Nigerian government recently stated that the solid minerals sector should aim to contribute 5\u201310% of Nigeria's GDP [,]. Even the Asians who are involved in the production use local miners in their operations []. These solid minerals, in most cases, do not occur in naturally useable forms and must be concentrated, purified and enriched before being used for industrial purposes [,].\nPrevious studies have evaluated and developed new mineral enhancement methods [,,,,,,,, ]. Atalaly et al. [] appraised the valuable mineral elements of the Beylikahur complex ore deposits, which contained fluorspar, barites, and bastnaesite. The bastnaesite mineral was found either as a block of cement between fluorspar and barite particles or as a finely distributed mineral within these minerals. Reddy et al. [] examined 75.00% BaSO4, 16.62% SiO2, 3.46% Al2O3, 0.33% S(Py) and 1.52% LOI in low-grade barite samples from Mangampet (India) during beneficiation investigations. Since the sample containing fine-grained barite was intimately connected with silicates with iron mineral inclusions, characterization studies revealed that processing may need to be done at a fine MOG of \u2212150 mesh. Singh et al. [] presented the",
        "summary": " In Nigeria, 80% of the mining was done locally by using shovels, hoes and diggers. This is especially true in the states of Nasarawa, Benue and Cross Rivers, where barite mining is prevalent. The existing process technique employed by these miners does not offer good, accurate, or quality results"
    },
    {
        "link": "https://www.sciencedirect.com/science/article/pii/S2405844023025458",
        "body_split": " findings of characterization and beneficiation experiments performed on a low-grade barite sample to create techniques for upgrading it to a commercial commodity. Two beneficiation techniques, gravity separation, and froth flotation were tested based on the characteristics of the barite and related gangues. With a specific gravity of 4.25, the flotation technique resulted in an 88% recovery. According to the study, the barite deposits in Mangampet, India, have been improved into a product appropriate for usage in oil wells [,,] and the processed ore can be used for various industrial applications [,, ]. In Nigeria, Afolayan et al. [] characterized barite reserves and applied them as weighting agent in drilling fluid. Khan et al. [] used varied concentrations of hydrochloric acid (5%\u201330%) at ambient temperature and equal time interval to improve the quality of barite from Gunga (Pakistan) deposits to internationally required standard. Khan et al. [] studied the grade of barite available in Hazara (Pakistan) that may be used following a simple 13-min treatment with commercial hydrochloric acid. The result yielded an internationally standard barite for industrial purposes.\nArtificial neural network (ANN) has found wide applications in industry, science and engineering []. It has been used to model the structure and function of biological nature of human brain []. As a result, ANN is shown to be more adaptable and appropriate than other modelling techniques [,]. ANN has recently gained tremendous importance and appplications in mineral processing. It has been applied to predict the degree of particle misplacement in liquid solid fluidization and predict bond work index from rock mechanics properties []. In related study, adaptive fuzzy inference systems and artificial neural networks were applied to evaluate the performance of multi-gravity separator treating iron ore fines [] and grade and reserve of a copper deposit []. They were also applied to predict the reductive leaching of cobalt(III) from oxidised low-grade ores [] and breakage properties of Platinum group of elements bearing chromite ore []. Also, response surface methodology has been applied for the optimization of copper leaching from refractory flotation tailings [] and beneficiation of Sudanese chromite ore via pilot plant shaking table separator [].\nHowever, ANN has found little or no application in the optimization of low-grade barite beneficiation as far as authors know. The high density of barite as compared to other metal minerals, its white colour and X-rays ads",
        "summary": " Two beneficiation techniques, gravity separation and froth flotation were tested based on the characteristics of the barite and related gangues. With a specific gravity of 4.25, the flotation technique resulted in an 88% recovery. According to the study, the Barite deposits in Mangampet, India, have been improved into a product appropriate for usage in oil wells"
    },
    {
        "link": "https://www.sciencedirect.com/science/article/pii/S2405844023025458",
        "body_split": "orptive properties have given it an edge to be used respectively as drilling mud, filler in paints and to examine cancerous cells []. The need for the optimization of Azare low-grade barite beneficiation has greatly influenced the rationale behind this study in which a comparative investigation between using response surface methodology (RSM) and artificial neural network (ANN) for the optimization of barite beneficiation was conducted. Optimization requires the use of mathematical tools and techniques to determine the best solution out of the available several alternatives purposely to have maximum production rate and return on investment. The response surface methodology combines the regression analysis and statistic techniques to predict relationships between independent and dependent variables using already existing experimental data purposely to develop, improve and optimize processes.\nIn this study, Box-Behnken Design and Central Composite Design of response surface methodology were adopted. The effect of barite mass, reaction time and particle size at three levels on barite composition was studied for the batch barite beneficiation experiments. Analysis of variance was used for the statistical analysis. A 3-16-1 feed-forward ANN structure with sigmoid transfer function was considered. The mean square error (MSE) technique was used for network training. In this study, 53%, 23.5% and 23.5% of a total of 17 experimental datasets were used for training, validation and testing respectively to model BBD experimental results using ANN. For CCD results modelling, 60%, 20% and 20% of a total of 20 experimental datasets were used for training, validation and testing respectively. The ANN model performance was determined by measuring mean squared error at different Epoch. This study is limited to conducting a comparative study of CCD and BBD as response surface methodologies with ANN purposely for the optimization of Azare low-grade barite beneficiation.\n2. Materials and methods\nHydrochloric acid used in this work is of analytical grade. All stock solutions were prepared using distilled water.\n2.1. Geographical mapping of sampling locations\nIn this research work, geographical mapping of sampling locations was performed using Geographic Information Systems (GIS) in the mineral belts of Azare Local Government Area, Bauchi State (North-East). Global Positioning System (GPS) satellite network coordinates was used to generate the map location of the study area (Fig. 1). In this case, the input; manipulation; management; query and analysis; and visualization were all part of any of the Geographic Information System (GIS) process aggregates [].",
        "summary": " The need for the optimization of Azare low-grade barite beneficiation has greatly influenced the rationale behind this study. In this study, Box-Behnken Design and Central Composite Design of response surface methodology were adopted. The effect of barite mass, reaction time and particle size at three levels on barite composition was studied"
    },
    {
        "link": "https://www.sciencedirect.com/science/article/pii/S2405844023025458",
        "body_split": "\n\n2.2. Description of study areas\nAzare is located at 11\u00b040\u203227\u2033N\u200210\u00b011\u2032 28\u2033E, at an elevation of 436 m in Bauchi State, Nigeria. Fig. 1 describes the location of barite in the coordinate system of the site (Azare). It illustrates road networks and commercial areas. The brown shaded portions with asterisks on the map indicate barite locations and areas where barite was not available on the subject site were indicated with dotted points. The dark brown shaded coloured part of the map is the development area of the site, where the dwellers of Azare live. The red line shows the road network while the red dotted line is the boundary of the study site. The coordinate system of the map area or zone is located within 11\u00b042\u203200\u2033N, 10\u00b010\u203200\u2033E and 11\u00b040\u203230\u2033N, 10\u00b013\u2032300\u2033 E Northern part of the site while the Southern part of the site has 11\u00b039\u203200\u2033N, 10\u00b010\u203230\u2033E and 11\u00b039\u203200\u2033N 10\u00b013\u203230\u2033E with the perimeter of 21642 m and it occupies a land area of 27808036 m square.\n\n2.3. Sample collection\nLow-grade samples of 5 kg barite-bearing ores were collected at an interval of 0.6 m from the bottom to the top of the exposure in 15 different locations of the site. A sledgehammer was used to disaggregate the amalgamated barite ores. The raw barite-bearing aggregates were chosen using a stratified random sampling procedure across the study site, which ensured that all probable barite-containing ores were sampled.\n\n2.4. Preparation and chemical composition of samples\nSamples were washed with clean water to eliminate dirt and then dried in an electronic oven. Samples were crushed in a hydraulic press to reduce them to a representative size, ground to a fine size in a ball and hammer mill, and screened through 150, 300, and 450 (micron) mesh screens. In a 250 ml beaker, different masses of the sample were taken and 100 ml of 5.0 M hydrochloric acid was added. The acid was decanted when the mixture had settled after 15 min of mixing with a mechanical conical flask shaker. By decantation and filtration, the sample was rinsed with distilled",
        "summary": " Fig. 1 describes the location of barite in the coordinate system of the site (Azare) It illustrates road networks and commercial areas. The brown shaded portions with asterisks on the map indicate barite locations and areas where barite was not available on the subject site were indicated with dotted points"
    },
    {
        "link": "https://www.sciencedirect.com/science/article/pii/S2405844023025458",
        "body_split": " water until the filtrate was chloride and iron-free. Atomic absorption spectrometer (AAS Buck Scientific 210 VGP, USA) was used to determine the chloride and iron concentration. The sample was washed and then dried at 110 \u00b0C in an electric oven. X-ray fluorescence analyser (Model Xsupreme 8000 by Oxford instrument) was used to determine the quality and elemental chemical compositions of barite samples collected from Azare Local Government Area locations.\n\n2.5. The experimental design\nBatch beneficiation experiments and optimization of Azare low-grade Barite were carried out using Box\u2013Behnken Design and Central Composite Design of response surface methodology. To optimize the beneficiated working parameters for reaching maximum BaSO4 percent, the effect of three key components, namely barite mass (g), reaction time (minutes) and particle size (\u03bcm) were explored. Table 1 shows the range of experiment variables used at lowest level (\u22121), middle point (0) and highest level (+1). Table 2 represents the coded factors\u2019 levels for the beneficiation of barite ore sample using CCD.\n\n2.6. Statistical analysis and optimization\nThe Box-Behnken design (BBD) and Central Composite Design (CCD) are the two-best predictable RSM methods [,]. The Central Composite Design is a more robust optimization technique but the Box-Behnken design addresses the issue of where exactly the experimental boundaries should lie and thus, avoid extreme treatment combinations. However, there is still need to compare the efficacies of the two methods of Design Expert with artificial neural network and determine the most suitable for the prediction of barite composition for the beneficiation process. Previous studies rarely compare the prediction efficiencies of these methods. In this study, 17 experimental runs for 3 factors were required for BBD as compared to the CCD which requires 20 experimental runs for 3 factors. The experimental design was developed using Design Expert\u00ae version 7.0.0 (Stat-ease, Inc. Minneapolis, USA) using the data presented in Table 1. The experiments were randomized to minimize the effects of unexplained variability in the observed responses due to extraneous factors []. The independent variables investigated were the mass of barite sample (g), reaction time (minutes) and particle size (\u03bcm). The response was the percentage yield of BaSO4 after the beneficiation of the ore sample. Equation (1) represents a generalised second-order linear regression model used",
        "summary": " X-ray fluorescence analyser (Model Xsupreme 8000 by Oxford instrument) was used to determine the quality and elemental chemical compositions of barite samples collected from Azare Local Government Area locations. The experimental design was developed using Design Expert\u00ae version 7.0.0 (Stat-ease, Inc"
    },
    {
        "link": "https://www.sciencedirect.com/science/article/pii/S2405844023025458",
        "body_split": " for fitting the experimental data to estimate the response. (1) Y= b o + \u2211 N i=1 b i X i + \u2211 N i,j=1 b ij X i X j + \u2211 N i=1 b ii X 2 i + \u2211 N i=1 e i Where Y is the predicted response, Xi and Xj are the independent variables, bo is the offset term, bi (i = 1,2, \u2026 \u2026.N) is the first-order model coefficient, bij is the interaction effect of operational variables, bii represents the quadratic coefficients of Xi, and ei is the error term or random error.\n\n2.7. Neural network\nThe Artificial Intelligence technique was used in the ANN modelling and optimization of low-grade Azare barite beneficiation data. Multilayer Normal Feed-Forward neural networks were used was used with sigmoid transfer function (Equation (2)) to estimate the percentage composition of Barium Sulphate. This network was trained using the mean square error (MSE) technique and altering the number of neurons to find the optimal number of neurons that best predicts the model. This was based on the MSE (Equation (3)) and coefficient of determination (R2) obtained by training the network at various numbers of neurons. The model is better when the R2 value is large. Fig. 2 shows a 3-16-1 three-layer feed-forward network's ANN structure used in this study consisting of one input layer having three input node values, one hidden layer having sixteen hidden node neurons and one output layer having three output node values. The mass of barite (g), particle size (\u03bcm) and reaction time (min) are the input layers respectively and the output layer is the barite composition (percent). All the neurons having different weights are interconnected. In this study, a total of 17 experimental datasets were used for the ANN modelling of BBD experimental results. Out of these, 53%, 23.5% and 23.5% were used for training (9 runs), validation (4 runs) and testing (4 runs) respectively. To model the CCD experimental results using ANN, 20 datasets were considered out of which 60%, 20% and 20% were used for training (12 runs), validation (4 runs) and testing (4 runs) respectively. (2) f(k)= 1 (1+ e \u2212k ) where k = output of the summation",
        "summary": "Fig. 2 shows a 3-16-1 three-layer feed-forward network's ANN structure used in this study. The network was trained using the mean square error (MSE) technique to find the optimal number of neurons that best predicts the model"
    },
    {
        "link": "https://www.sciencedirect.com/science/article/pii/S2405844023025458",
        "body_split": " from previous step. (3) MSE= 1 N \u2211 N i=1 ( y network \u2212 y target ) 2 where N = number of dataset, ynetwork = network output and ytarget = target output.\n3. Results and discussion\nThis covers beneficiation and statistical modelling of the barite processing, and the application of Response Surface Methodology (RSM) in the optimization of the beneficiation process of the barite minerals using Box-Behnken Design (BBD) and Central Composite Design (CCD).\n3.1.1. Batch barite beneficiation experiments\nThe results of batch beneficiation experimental works are presented in Table 2, Table 3 showing the actual variables at different values, experimental and predicted BaSO4 composition (percent) for BBD and CCD, respectively where A is the mass of barite (g), B is the reaction time (min), C is the particle size (\u03bcm), Yexperimental is barite percentage (BaSO4) for the experimental study and Ypredicted is the predicted barite percentage (BaSO4). For the BBD (Table 3), maximum barite composition of 98.07% was observed at barite mass, reaction time and particle size of 100 g, 30 min and 150 \u03bcm respectively while minimum barite composition of 88.50% was observed at barite mass, reaction time and particle size of 80 g, 10 min and 450 \u03bcm respectively. For the CCD (Table 4), maximum barite composition of 95.43% was observed at barite mass, reaction time and particle size of 80 g, 30 min and 300 \u03bcm respectively while minimum barite composition of 60.51% was observed at barite mass, reaction time and particle size of 60 g, 45 min and 450 \u03bcm respectively. Generally, the results proved increased in barite composition with increase in barite mass and reaction time but decrease in particle size. An increase in the mass of barite increases the amount of barite available for reaction which conversely increases the composition of barite. Also, increase in time increases the period available for the barite components to react. The lower the particle size, the more the surface area available for reaction and the more the barite composition. In a similar study by Chaurasia et al. [] in which beneficiation of iron ore fines was considered, the results showed that simultaneous increase in Fe(T) grade from 50.74%% to 65.26%% with 71.",
        "summary": " The results of batch beneficiation experiments are presented in Table 2, Table 3 showing the actual variables at different values, experimental and predicted BaSO4 composition (percent) for BBD and CCD. For the BBD, maximum barite composition of 98"
    },
    {
        "link": "https://www.sciencedirect.com/science/article/pii/S2405844023025458",
        "body_split": "25% recovery is possible. Studies conducted by Bai et al. [] and Al-Tigani et al. [] also presented similar results.\n\n3.1.2. Mathematical models developed\nThe BaSO4 composition was chosen as the response or dependent variable. Equation (4) is the statistical linear regression model for BBD and Equation (5) is the quadratic statistical model for CCD relating the coded variables with the responses respectively. These model equations were developed using the experimental data for BBD and CCD respectively. (4) Y(%)=94.76\u22120.85A+1.93B\u22122.59C+1.34AB\u22120.64AC+1.74BC (5) Y(%)=95.19+1.47A\u22120.3425B\u22124.71C+5.47AB+3.56AC +2.53BC\u22126.06 A 2 \u22127.89 B 2 \u22121.78 C 2\nThese Equations were used to predict the barite compositions (responses) at different process conditions as stated in Table 3, Table 4 A correlation coefficient value of R2 close to unity strongly indicates a good agreement between experimental and predicted values of barite composition. Also, the analysis of variance results revealed the fitness of these developed models for the study.\n\n3.1.3. Analysis of variance (ANOVA)\nAnalysis of Variance (ANOVA) for BaSO4 for BBD and CCD are shown in Table 5, Table 6, respectively. The regression model's F-values of 21.54 and 4.91; and low p-values of <0.0001 and 0.0103 reported respectively for BBD and CCD indicated that they were very significant. The coefficient of determination (R2) was used to check the models' quality of fit, yielding 0.9420 (94.20%) and 0.9272 (92.72%) for BBD and CCD respectively. Fig. 3 presents the correlation coefficient plots of experimental and predicted values using BBD (Fig. 3a) and CCD (Fig. 3b). This suggests the models are fitted for response prediction. Also, it means the independent variables (mass, reaction time and particle size) are responsible for 94.20% and 92.72% of the variation in BaSO4 composition in the samples using BBD and CCD respectively. Thus, the BB",
        "summary": " The BaSO4 composition was chosen as the response or dependent variable. Equation is the statistical linear regression model for BBD and Equation (5) Equations were developed using the experimental data forBBD and CCD respectively. They were used to predict the barite compositions (responses) at different process conditions"
    },
    {
        "link": "https://www.sciencedirect.com/science/article/pii/S2405844023025458",
        "body_split": "D and CCD models could not account for 5.78% and 7.28% of the total variation respectively. As a result, the model could be used to calculate the percentage composition of BaSO4 in a theoretical model.\n\n3.1.4. Contour plots\nA contour plot gives information on where minimum and maximum response could be obtained within the data range. The contour plots for BBD and CCD are shown in Fig. 4a and b respectively. These plots resemble a topographical map that investigates the possible potential relationship between the three variables: x and y (mass and reaction time), with a particle size as the real factor and the response variable (BaSO4 composition). The x- and y-factors (predictors) are displayed on the x- and y-scales, and response values (BaSO4 composition) represented by contours. The contour plots depict the three-dimensional connection in two dimensions. As a result, the response variable (BaSO4 composition) can be anticipated from this plot. The contour plot of BBD (Fig. 4a) shows that even at high values of barite mass approximately 100 g, less than 90% of barite composition could be achieved at very low reaction time of approximately 15 min. However, 96\u201398% of barite composition is achievable for barite mass between 87 and 100 g and reaction time greater than 37 min. This is also possible at reaction time greater than 40 min but barite mass of less than 70 g. The contour plot of CCD (Fig. 4b) shows 65\u201370% of barite composition is possible at less than 10 min reaction time and barite mass ranging between approximately 75\u201385 g. However, the result revealed barite composition of 90\u201395% at the same barite mass range but at reaction time ranging between 27 and 32 min. This barite composition is also achievable at reaction time and barite mass ranging between approximately 40\u201350 min and 95\u2013108 g respectively. These plots revealed high significance of reaction time in this study. Increase in barite composition at high reaction time could be to increase in the exposure period between the barite and hydrochloric acid used for the beneficiation process.\n\n3.1.5. 3D surface plot showing parameters interaction effect on the response\nFig. 5a and b shows a three-dimensional surface plot for BBD and CCD, respectively, that explores the potential link between three",
        "summary": "The model could be used to calculate the percentage composition of BaSO4 in a theoretical model. The model could not account for 5.78% and 7.28% of the total variation respectively"
    },
    {
        "link": "https://www.sciencedirect.com/science/article/pii/S2405844023025458",
        "body_split": " variables: two independent variables (barite mass and reaction time), and a response variable (barite composition). The three-dimensional relationship is depicted in this graph, with mass and reaction time on the x- and y-axes, and BaSO4 on the z-axis. The batch beneficiation and statistical ANOVA results have reaveled high significance of barite mass and reaction time on the response (barite composition). The 3D surface plots revealed that the composition of barite increased from 90.0% to approximately 95%, and 60% to approximately 80% as the reaction time and barite mass increased from 20 to 40 min and 60\u2013105 g, and 20\u201360 min and 60\u2013120 g for BBD and CCD respectively. Under these conditions, the particle size was at a constant actual value of 150 \u03bcm. Increase in the mass of barite increases the amount of barite available for the beneficiation process which simultaneously increases the barite composition after the process. Also, an increase in the reaction time increases the barite exposure period for the beneficiation process which in return increases the barite composition.\n\n3.1.6. Process optimization\nThe optimum predicted points by the BBD and CCD are presented in Table 7. At optimum point, barite mass, reaction time and particle size of 60 g, 15 min and 150 \u03bcm respectively were recorded for both the BBD and CCD. However, 98.71% and 94.59% of barite composition were predicted respectively for BBD and CCD. Laboratory experiment was conducted separately at the optimum predicted point in order to affirm the efficacy of developed mathematical models. Experimental values of 96.98% and 91.05% were obtained for BBD and CCD respectively. A strong correlation exists between the experimental and predicted values because the error was minimal.\n\n3.2.1. ANN training, validation and testing\nThe regression plots between experimental and predicted values of barite composition for training, validation and testing of BBD (Fig. 6) and CCD (Fig. 7) datasets are presented. In both cases, the predicted and experimental barite composition were observed to be highly satisfying for the examined datasets. ANN method was adopted to predict the test results. The values of correlation of determination were found to be 0.9905, 0.9419, and 0.9997 (BBD); and 0.9851, 0.9381, and 0.9911 (CCD",
        "summary": " The 3D surface plots revealed that the composition of barite increased from 90.0% to approximately 95%. The reaction time and barite mass increased from 20 to 40 min and 60\u2013105 g, and 20\u201360 min. and 60-120 g for BBD and CCD respectively"
    },
    {
        "link": "https://www.sciencedirect.com/science/article/pii/S2405844023025458",
        "body_split": ") for the training, validation, and testing respectively. The overall correlation of determination by the ANN for BBD and CCD was recorded to be 0.9672 and 0.9711 respectively which suggests high significance of ANN for the process. Also, these are higher than the R2 values reported between experimental and predicted values by BBD (R2 = 0.9420, Fig. 3a) and CCD (R2 = 0.9272, Fig. 3b). Thus, ANN predicts better than BBD and CCD.\n\n3.2.2. ANN performance\nThe ANN model efficiency was measured from the training, validation and testing performance. A plot of mean squared error versus Epoch, indicating the error prediction of BBD (Fig. 8) and CCD (Fig. 9), is presented. The output data exhibited an acceptable error range with different optimum number of epochs. The network training terminated at 5th and 1st epoch for BBD and CCD respectively suggesting overfitting started from 6th and 2nd epoch for BBD and CCD. The best validation performance was 48.5437 and 5.1777 at epoch 5 and 1 respectively. Thus, the optimal number of epochs to train the datasets was 5th and 1st for BBD and CCD respectively.\n\n3.3. Comparison between RSM (BBD and CCD) and ANN\nFig. 10 presents a plot of residual values at different experimental runs for BBD, CCD and ANN while Table 8 presents MSE, R2 and ADD values to compare the models. Mean squared error of 14.972, 43.560 and 0.255; R2 value of 0.942, 0.9272 and 0.9711; and ADD of 3.610, 4.217 and 0.370 were recorded for BBD, CCD and ANN respectively. Tables S1 and S2 in the supplementary file represent detailed mean square error of ANN prediction using BBD and CCD experimental data respectively. Higher value of R2 close to 1 but lower value of MSE and ADD signifies a better model. Thus, ANN proved to be the best among the three models investigated. The order of model fitness is ANN > BBD > CCD. Tables S3 and S4 in the supplementary file represent R2 values for ANN prediction using BBD and CCD experimental data respectively. Previous studies have presented similar results in which the predicted results obtained from neural network modelling were close to",
        "summary": " The overall correlation of determination by the ANN for BBD and CCD was recorded to be 0.9672 and 0.9711 respectively which suggests high significance of ANN for the process. These are higher than the R2 values reported between experimental and predicted values by BBD (R2 = 0.9420, Fig"
    },
    {
        "link": "https://www.sciencedirect.com/science/article/pii/S2405844023025458",
        "body_split": " experimental results with correlation of determination close to unity [,]. Also, a study conducted by Deniz et al. [] compared the effectiveness of multivariable linear regression (MLR) and ANN for the estimation of grade and recovery in barite tailings concentration using flotation approach. The presented result revealed ANN to be the best model.\n4. Conclusion\nIn this study, Box-Behnken Design and Central Composite Design of response surface methodology were used for batch experimental design and optimization of low-grade barite beneficiation. Barite mass (60\u2013100 g), reaction time (15\u201345 min) and particle size (150\u2013450 \u03bcm) at three levels were considered as the process parameters for the process. A comparative study was conducted between using these methods and ANN to determine the best predictive optimization tool. A 3-16-1 feed-forward ANN structure with sigmoid transfer function was adopted and mean square error (MSE) technique was used for network training. Batch experimental result revealed maximum barite composition of 98.07% and 95.43% at barite mass, reaction time and particle size of 100 g, 30 min and 150 \u03bcm; and 80 g, 30 min and 300 \u03bcm for BBD and CCD respectively. The predicted and experimental outputs at optimum point (barite mass, reaction time and particle size of 60 g, 15 min and 150 \u03bcm) were recorded to be 98.71% and 96.98%; and 94.59% and 91.05% for BBD and CCD respectively. The correlation of determination recorded by ANN for training, validation and testing were 0.9905, 0.9419 and 0.9997; and 0.9851, 0.9381 and 0.9911 for BBD and CCD experimental results. The best validation performance was 48.5437 and 5.1777 at epoch 5 and 1 for BBD and CCD respectively. In conclusion, the overall mean squared error of 14.972, 43.560 and 0.255; R2 value of 0.942, 0.9272 and 0.9711; and absolute average deviation of 3.610, 4.217 and 0.370 recorded for BBD, CCD and ANN respectively proved ANN to be the best.\nDeclaration of competing interest\nAuthors declare no conflict of interest.\nAuthor contribution statement\nLekan Taofeek Popoola: Analyzed and interpreted the data; Wrote",
        "summary": " Study compared effectiveness of multivariable linear regression (MLR) and ANN for the estimation of grade and recovery in barite tailings concentration using flotation approach. The presented result revealed ANN to be the best model"
    },
    {
        "link": "https://www.sciencedirect.com/science/article/pii/S2405844023025458",
        "body_split": " the paper. Oluwafemi Fadayini: Conceived and design the experiements; Performed the experiments; Contributed reagents, materials, analysis tool.\nData availability statement\nData will be made available on request.\n",
        "summary": " Oluwafemi Fadayini: Conceived and design the experiements; Performed the experiments; Contributed reagents, materials, analysis tool. Data will be made available on request"
    },
    {
        "link": "https://www.sciencedirect.com/science/article/pii/S2405844023025896",
        "body_split": "1. Introduction\nDiscovering the class of student's performance in the first days of the course duration in the virtual learning environments (VLE) facilitates accomplishing Learning Analytics goals. Consequently, many researchers proposed predictive models to predict student performance earlier in Massive Open Online Courses (MOOC) courses in binary classes (pass or fail) or (dropout or not), but few studies have developed models that predict student performance in multi-classification form like [,,,,,, ]. Hence, the multi-class models need more studies to improve prediction performance []. As techniques used in the prediction of student performance in online higher education, traditional artificial intelligence techniques are commonly applied while more advanced techniques like deep learning are rarely applied []. Since the majority of studies have taken the approach of developing predictive models that target specific courses like [,,, ], but overfitting can take place if new courses are devised []. Thus, course-agnostic predictive models which train and evaluate with different courses are required to overcome this problem. As stated in Ref. [], significant contributions of clickstream data in the identification of student performance. So, institutions can use clickstream data to develop real-time analytic reports of online students. As a result, they can make decisions more timely and informed. Also, clickstream data can be combined with demographic and course data and get promising results in studying student performance []. The aforementioned gaps motivated us to propose a developed model that overcomes them. To facilitate higher education decision-making processes towards sustainable education in the MOOC environments as well as enable instructors to take suitable in-time actions, the main aim of this study is to build a multi-class course-agnostic day-wise predictive model to identify the class of students' performance in MOOC environments as early as possible with satisfied accuracy using demographic and activity clickstream data. To achieve this study aim, the following objectives rise. 1. To develop a new day-wise deep learning model to predict students' performance in multi-class form. 2. To determine which recurrent deep learning model among RNN, LSTM, and GRU can get better students' performance prediction accuracy. 3. To compare the proposed model with similar state-of-the-art models in terms of accuracy.\nThe rest of this study is organized as follows: Section 2 discusses the previous works in the scope of student performance predictive models implemented in MOOC environments. The research methodology and the description of the dataset are presented in Section 3. In addition, the settings of",
        "summary": " Many researchers proposed predictive models to predict student performance earlier in Massive Open Online Courses (MOOC) courses in binary classes (pass or fail) or (dropout or not), but few studies have developed models that predict students' performance in multi-classification form"
    },
    {
        "link": "https://www.sciencedirect.com/science/article/pii/S2405844023025896",
        "body_split": " experiments and the stages of the proposed model are described with the details of each stage. Section 4 highlights the results of the experiments. Moreover, the result of monthly prediction and comparison between the proposed model and the baseline models are shown in this Section. A discussion of the obtained results is presented in Section 5. Section 6 displays the conclusion and the future work.\n2. Related works\nMassive Open Online Course (MOOC) is an online course that is open to anyone with no restrictions, usually organized with a set of learning objectives in an area of study, often provided over a specific period in a virtual learning environment that allows interaction between peers or students and instructors. MOOCs facilitate creating a learning community []. Open University Learning Analytics Dataset (OULAD) dataset is a popular MOOC dataset used by several studies which applied performance predictive models on MOOC courses. Several predictive models were trained on the OULAD dataset and published in 2019, 2020, 2021, and 2022.\nIn 2019, Hassan et al. used clickstream data to predict withdrawn students every five weeks with an LSTM deep model []. The accuracy obtained in the 5th week was around 80% and in the 25th week was around 97%. Different machine and deep learning models (Random Forest (RF), Multiple Layered Perceptron with multiple activation functions, and Gaussian NB) were proposed in Ref. [] to predict student performance pass and fail using demographic information, clickstream data, generated features, and Total Features.\nEarlier in 2020, Yanbai et al. proposed Recurrent Neural Network (RNN)- Gated Recurrent Unit (GRU) joint neural network to predict whether students will fail or pass using demographic, clickstream, and assessment data []. The prediction was at the course level. So, the last courses were used as a test set. The average accuracy obtained in all courses was between 60 and 90% from the 5th week till the 39th week. The class of student performance was treated as a binary classification in Ref. [] The prediction was at the course level. So, the last courses were used as a test set. The average accuracy obtained in all courses was between 60 and 90% from the 5th week till the 39th week. The class of student performance was treated as a binary classification and four deep ANN models were deployed to predict failed, withdrawn, and distinct students using demographic and clickstream data. The sparse reduction was used to select 30 features over a total of 54 features and the \ufffd",
        "summary": " Open University Learning Analytics Dataset (OULAD) dataset is a popular MOOC dataset used by several studies. Several predictive models were trained on the OULAD dataset and published in 2019, 2020, 2021, and 2022"
    },
    {
        "link": "https://www.sciencedirect.com/science/article/pii/S2405844023025896",
        "body_split": "\ufffdMinMax\u201d scalar was used to transform the values of the selected features. The prediction was performed in four quarters of the semester. The accuracy obtained in the \u201cfail\u201d prediction model was around 77%, 81%, 86%, and 88% in quarter1, quarter2, quarter3, and quarter4, respectively. In the \u201cfail\u201d prediction model withdrawal student's data was ignored and the distinction merged with passed data. The accuracy obtained in the withdrawn prediction model was around 78%, 86%, 90%, and 93% in quarter 1, quarter 2, quarter 3, and quarter 4, respectively. In the withdrawn prediction model fail student's data was ignored and the distinction merged with passed data. The accuracy obtained in the two distinction prediction models was between 80 and 81% when withdrawn and fail data was ignored and between 80 and 85% when withdrawn and pass data was ignored. A multi-class predictive model was proposed in Ref. []. The target classes were \u201cpass\u201d, \u201cfail\u201d, and \u201cwithdrawn\u201d. Assessment and clickstream features were used as input for the predictive model. Also, it developed a regression model to predict the final assessment grades.\nIn 2021, Adnan et al. used a Deep Feed Forward Neural Network (DFFNN) to predict a student's final result (distinct, pass, withdrawn, and fail) with the best average accuracy obtained at the last time of courses was 43% when input data were demographic data, 63% when input data was demographic and clickstream, 71% when input data was demographic, clickstream and assessment and 72% when input data was all features available in OULAD dataset []. In Addition, multi-classification was converted to binary classification. After this conversion, the accuracy obtained was 90% at the end of the courses. Another research published is []. It used demographic and aggregated clickstream data of two social science courses (AAA_2013J and AAA_2014J) and two STEM courses (CCC_2014B and CCC_2014J) as input. It developed a supervised machine learning model with an expectation maximum algorithm to predict whether a student will stay or drop out in the previous week then used the resulting probability as input to improve prediction accuracy for the current week. A Synthetic Minority Over-Sampling (SMOTE) technique was used in Ref. []. The average accuracy obtained for selected courses in all weeks was approximately 88%. A semi-supervised learning ensemble model",
        "summary": "MinMax\u2019 scalar was used to transform the values of the selected features. The prediction was performed in four quarters of the semester. The accuracy obtained in the \u201cfail\u201d prediction model was around 77%, 81%, 86%, and 88% in quarter1, quarter2, quarter3, and quarter4, respectively"
    },
    {
        "link": "https://www.sciencedirect.com/science/article/pii/S2405844023025896",
        "body_split": " of Artificial neural network (ANN) was proposed in Ref. [] to predict student performance (pass or fail) before midterm and at the end of the course. Five specific courses of the OULAD data set were chosen as input to the proposed model. Each chosen course was available in two different semesters. The first course was used for training while the second was used for testing. The average accuracy obtained for selected courses was 87.47% in the middle of the semester. Distinction and withdrawal of students' data were ignored in Ref. [], which represents around 40% of the OULAD dataset. Prediction limited to the trained course. As [], Hlioui et al. [] predicted withdrawal students with different models (Decision tree(J48), Random Forest, Bayesian classifier (TAN), SVM classifier, and MLP) classifiers using clickstream, assessment, and demographic data. Student performance was divided into two values: withdrawal and completion (\u201cDistinction\u201d or \u201cPass\u201d or \u201cFail\u201d). The importance of assignment information for students' performance prediction was explored in Ref. []. It developed a Multiple Instance Learning predictive model to predict passed and failed students. Features used were assessment data. Predictive models with different classification algorithms (NB, RF, KNN SVM, and ANN) were proposed in Ref. [] and compared the performance in predicting final exam grades. Assessment, Demographic, and Clickstream data were used as input for the prediction models. Three predictive models were proposed and compared in Ref. []. The performance of ANN, SVM, and ANN in binary classification (pass or fail) was compared at the end of the course using demographic, assessment, and clickstream data. In Ref. [], predictive models were developed to classify students as withdrawal or non-withdrawal and at-risk or not at-risk using RFDT, FFNN, MLP, Gradient Boosting Machine, and LR. Two multi-class predictive models (RF and ANN) were constructed in Ref. [] to classify students to distinction, pass and fail. Withdrawn students' data were excluded. A three-layer LSTM model was proposed in Ref. [] to classify student performance with multi and binary classification using clickstream and demographic data. The attention technique was used in this model. The clickstream data was aggregated weekly. A binary classification target, pass or fail, was the output of the predictive models proposed in Ref. [].",
        "summary": " Artificial neural network (ANN) was proposed in Ref to predict student performance (pass or fail) before midterm and at the end of the course. Five specific courses of the OULAD data set were chosen as input to the proposed model. Distinction and withdrawal of students' data were ignored in Ref"
    },
    {
        "link": "https://www.sciencedirect.com/science/article/pii/S2405844023025896",
        "body_split": " KNN, ANN, SVM, RF, and Naive Bayes (NB) were compared. Total clickstreams in all activities were calculated and input to these models besides assessments and demographic data. Multi-classification predictive models proposed in Ref. []. ANN, SVM, RF, and Naive Bayes were compared. Feature selection algorithms were applied to demographic and clickstream data before inputting them into the selected models.\nIn 2022, a prediction framework based on six traditional machine learning algorithms (SVC (R), SVC (L), Na\u00efve Bayes, KNN (U), KNN (D), and Sofmax) was proposed in Ref. []. The expected output value of the predictor was\u201d qualified\u201d or\u201d unqualified\u201d. The proposed framework in Ref. [] used the \u201cDDD\u201d module (course) with clickstream data in 12 activities. In Ref. [] researchers developed a Bayesian Network (BN) base prediction model for the final performance and compared it with Gradient Boosting Decision Tree, Multi-Layer Perception (MLP), and Na\u00efve BN ensemble models. The value of calculated performance was divided, according to the student's scores in the assessments, into fail, pass, good, and distinction. As a multi-class classification model, Adnan et al. [] developed different ML models to predict student performance with the four classes Distinct, pass, fail, and withdrawn. The compared models were Random Forest with two different criteria \u2018gini\u2019 and \u2018entropy\u2019, AdaBoost, Extra Tree classifier, K-Nearest Neighbour (KNN), Decision Tree, Support Vector Machine, Gradient Boosting, Logistic Regression, Gaussian NB, Bernoulli NB classifier, and only one DL classifier (Feed Forward Neural Network (FFNN)).\nMulti-class course-agnostic student performance predictive models still need to increase the accuracy at the early time of MOOC courses in case of using demographic and clickstream data. Although the predictive models proposed in Refs. [,] are multi-class course-agnostic models, they are not applied in the early time during the course period. while a periodical predictive model proposed in Ref. [] which starts the prediction for the class of student performance from the fifth week, the accuracy obtained in the fifth week was 53%. Generally, the accuracy obtained at the end of the course was 66% and 63%, for models proposed in Ref",
        "summary": "In 2022, a prediction framework based on six traditional machine learning algorithms was proposed in Refs. Feature selection algorithms were applied to demographic and clickstream data before inputting them into the selected models. Total clickstreams in all activities were calculated and input to these models"
    },
    {
        "link": "https://www.sciencedirect.com/science/article/pii/S2405844023025896",
        "body_split": "s. [,], respectively which consider low accuracy.\nBased on the discussed related works the gap of this study raised. First, models in Refs. [,,, ] are not general. They are applied to specific courses. So, overfitting can take place if new courses are devised []. Second, most researchers proposed predictive models to predict student performance in binary classes (pass or fail) or (dropout or not), but few studies have developed models that predict student performance in multi-classification forms like [,,,,,, ]. The multi-class prediction models lead to the ability to use the model for several objectives. For example, identify students expected to drop out of the course (withdrawn students) or at risk of failing the course (fail students, and help instructors in providing personalized content for students based on their performance. Third, models in Refs. [,,,] discard some parts of the dataset. For example, discarding all withdrawal students' data can lead to overfitting. Finally, although instructors want to predict students' performance on any day through the course duration based on the current and previous clickstream behavior, most of the related works predict the class of student's performance on the last day of the course duration.\n3. Research methodology\nThis work is in the context of predicting student performance who studies in Massive Open Online Courses provided by virtual learning environments using demographic and clickstreams data which generate during the interaction between students and virtual learning environments. The proposed model will use recurrent neural network models because they are commonly applied to time series data. This section discusses the experimental research methodology. It demonstrates the applied method in this study, the description of the used dataset, the evaluation metrics, and the proposed model architecture.\n3.1. Method\nThis experimental study will be implemented in four phases. Each phase contains several stages. Firstly, in the dataset preparation phase, the OULAD dataset will be downloaded. Then the dataset will be prepared and aggregated in a suitable form. The data relating to specific course duration will be selected starting from the first day of the course until reaching the last day of the course. After that the dataset will be split. In the model construction phase, the proposed model will be built, trained, and evaluated. Then, in the baseline models construction phase, two baseline models will be built, trained, and evaluated with the same dataset. Finally, in the results comparison phase, the result obtained by the proposed model and the baseline models will be compared and the",
        "summary": " The proposed model will use recurrent neural network models because they are commonly applied to time series data. The data relating to specific course duration will be selected starting from the first day of the course until reaching the last day. After that the dataset will be split, the model will be built, trained, and evaluated with the same dataset"
    },
    {
        "link": "https://www.sciencedirect.com/science/article/pii/S2405844023025896",
        "body_split": " next experiment with data from the next day of the course will start till reaching the last day of the course. Fig. 1 illustrates the experiment's flowchart.\n\n3.2. Dataset description\nOpen University Learning Analytics Dataset (OULAD) is one of the public Learning Analytics datasets containing data for 32,593 students studied in open online courses []. This data was collected from the Open University, one of the largest distance-learning institutions in the United Kingdom []. It contains more than 10 million student interaction records The OULAD dataset contains demographic, assessment, course, and clickstream data. Seven online courses are offered in four different semesters for students from different education levels and ages. Unlike the Harvard University dataset which did not provide a timely record of student activity behavior [], in OULAD each interaction \u201cclick\u201d on any activity in a virtual learning environment was recorded in a separate table called \u201cStudentVle\u201d. The OULAD dataset can be downloaded from (https://analyse.kmi.open.ac.uk/open_dataset). Table 1 describes the OULAD dataset.\n\n3.3. The evaluation metrics\nThe evaluation methods used in most previous research are precision, recall, F1-score, and accuracy. This study will use the same evaluated methods. All equations of the evaluation metrics were taken from Ref. []. The Precision is calculated by dividing the percentage of True Positive elements by the total of positively predicted units. To calculate precision, Equation (1) is used. (1) Precision= TP TP+FP where TP is the number of true Positive elements and FP is the number of false positive elements. The model's capacity to find all Positive elements in the dataset is measured by the recall recall. Recall can be calculated by Equation 2 (2) Recall= TP TP+FN where FN is the number of those elements that the model has labeled as negative but are positive. Accuracy is a popular metric in multi-class classification that can be calculated by Equation 3 (3) Accuracy= TP+TN TP+TN+FP+FN where TP and TN are the elements that the model correctly classifies and FP and FN are the elements that the model incorrectly classifies. F1-score is interpreted as a weighted average of Precision and Recall. Equation (4) can be used to calculate it. (4) F1\u2212Score=2* (Precision*Recall) Precision+Recall",
        "summary": "Open University Learning Analytics Dataset (OULAD) is one of the public Learning Analytics datasets containing data for 32,593 students studied in open online courses"
    },
    {
        "link": "https://www.sciencedirect.com/science/article/pii/S2405844023025896",
        "body_split": "\nTo monitor the performance of the proposed model when treating all classes equally macro F1-score will be recorded. Macro F1-score is a harmonic mean of Macro Average Precision and Macro Average Recall [] computed with Equation (5): (5) MacroF1\u2212Score=2* (MacroAveragePrecision*MacroAverageRecall) MacroAveragePrecision \u22121 + MacroAverageRecall \u22121 where the Macro Average Precision is the arithmetic mean of all classes' precision and can be calculated with Equation (6): (6) MacroAveragePrecision= \u2211 k k=1 Precision k k\nThe Macro Average recall is the arithmetic mean of all classes recall and can be calculated with Equation (7): (7) MacroAverageRecall= \u2211 k k=1 Recall k k\nAlso, to monitor the performance of the proposed model when treating all instances (students) equally micro F1-score will be recorded. Micro F1-score is computed with Equation (8): (8) MicroAverageRecall= \u2211 k k=1 TruePositive k GrandTotal where True Positive is the number of true predicted instances and Grand Total is the total number of instances.\n\n3.4. ANN-LSTM model architecture\nThe features which will be used to predict the class of student performance will be demographic and clickstream features. These features are arranged in comma-separated values text files (CSV) which will be merged into one text file and fed to the ANN-LSTM model. The ANN-LSTM model will contain three layers, input, hidden, and output layers. As the input layer LSTM neural network will be used because LSTM is widely used in sequential data analysis [] while the rest layers will use ANN which is the simplest neural network. The output of the proposed model will be the four categories of students\u2019 performance distinction (D), pass (P), fail (F), and withdrawn (W). Fig. 2 illustrates the proposed model architecture.\n4. Results and implementation\nTo conduct the proposed model, environmental settings are needed. In addition, the dataset requires pre-processing which is depicted in subsection 2. The proposed model and its baseline models have been constructed as presented in 3 Research methodology, 4 Results and implementation. Finally, the results of the experiments are shown in later subsections.\n4.1. Environmental settings\nThe Jupyter Notebook IDE was used to implement the ANN-LST",
        "summary": "To monitor the performance of the proposed model when treating all classes equally macro F1-score will be recorded. The output of the model will be the four categories of students\u2019 performance distinction (D), pass (P), fail (F), and withdrawn (W) The results of the experiments are shown in later subsections"
    },
    {
        "link": "https://www.sciencedirect.com/science/article/pii/S2405844023025896",
        "body_split": "M model in Python. Numpy, Pandas, Scikit-learn, TensorFlow and Keras open-source libraries were imported to implement the ANN-LSTM model. The source code of implemented experiments is uploaded on https://github.com/FatimaAlazazi/ANN-LSTM-model. The experiments were run on a machine with Core i7 and 16 GB RAM. The operating system was Windows 10 64-bit. Due to deep learning models taking a long time in training, a group of experiments was run on Windows 10 64-bit virtual machine workstation on a server machine to accelerate obtaining the results. All baseline experiments were run on a desktop PC with Core i7 and 32 GB RAM.\n\n4.2. Dataset preparation\nA SQL server database was created then all data inside these files were imported to the created database. Because the data of students' interaction with the activities are recorded as rows, a query was built to extract the sum of students\u2019 clicks with all the activities in one row for each day with columns representing each activity Fig. 3(a) describes the organization of raw data related to student activity interaction while Fig. 3(b) describes the organization of resulted query.\nThe data set CSV files were loaded into Pandas Dataframes Then, a sequence of merge operations was done till all demographic and clickstreams data were collected in the final Dataframe. Dataframes were merged based on unique identifiers like id_student, and id_site and arranged in Pandas DataFrame. The final dataframe was stored in a Numpy array in preparation for being entered into the ANN-LSTM model. As stated in Ref. [], student interaction with the virtual learning environments (VLE) before the course stating does not importantly affect their performance, before the course started clickstreams were ignored.\nThe data set did not keep records for days with no interaction. So new records were added to represent which day each student did not interact. For example, if record r represented that students interacted with activity \u201ca\u201d in day 6 and did not interact with any activity in the previous days, then 5 records will be added related to this student with zero value interaction.\nTo improve the results of the ANN-LSTM model, feature value scaling was applied for numerical features with the \u201cMinMax\u201d scalar method.\nOne-hot encoder using the \u201cget_dummies\u201d method was applied to categorical",
        "summary": " Numpy, Pandas, Scikit-learn, TensorFlow and Keras open-source libraries were imported to implement the ANN-LSTM model. The experiments were run on a machine with Core i7 and 16 GB RAM. The operating system was Windows 10 64-bit"
    },
    {
        "link": "https://www.sciencedirect.com/science/article/pii/S2405844023025896",
        "body_split": " demographic features to transform them into a binary vector while the \u201cLabelEncoder\u201d method was applied to the final result column for the same purpose. The number of features after the encoder was 71. The final prepared dataset can be downloaded from https://drive.google.com/file/d/1S5_k1VPA_U5mX1B7ac_ZxSjw18N1QA-p/view?usp=sharing.\nFor each time step (a day of the course duration), the ANN-LSTM model was trained and evaluated using demographic and clickstream data in this day and all previous days. So, the records related to this time interval were selected from the entire dataset. The total implemented experiments were 270 experiments which represent an experiment for each day in the course period. For each experiment that was implemented on a specific day of the course duration, the clickstream data related to that day and all days before it was used as input features to the ANN-LSTM model. A numerical variable named \u201cTime Step \u201cwas declared to represent the current day in the course duration starting from one which represents the first day. When the time step equals one, the clickstream data of the first day is selected from the entire dataset and was fed to the ANN-LSTM model and baseline models as input features. After ANN-LSTM and baseline models have been constructed, trained, and evaluated using the selected clickstream data, the \u201cTime Step\u201d variable incremented by one to start the second-day experiment. The clickstream data of the first and second day of the course duration were selected and fed to the ANN-LSTM model and baseline models as input features and so on until reached the last day of the course duration which is day 270.\nHoldout validation was applied. Prepared data was split into approximately 30% for testing, and 70% for training, while 20% of training data was used as validation data.\n\n4.3. Model construction and configuration\nFrom Keras API a sequential model was constructed. The input layer was an LSTM layer with an input shape of a 3-dimensional Numpy array and 200 units as output. The next layer was the Dropout layer with an output of 200 units. This Dropout layer was added to reduce the overfitting of data. A hidden Dense layer with an output of 100 units and an activation function of \u201cRelu\u201d",
        "summary": "For each time step (a day of the course duration), the ANN-LSTM model was trained and evaluated using demographic and clickstream data in this day and all previous days. The total implemented experiments were 270 experiments which represent an experiment for each day in the course period"
    },
    {
        "link": "https://www.sciencedirect.com/science/article/pii/S2405844023025896",
        "body_split": " was added before the output layer. Finally, another Dense layer was added as an output layer with an activation function of \u201cSoftmax\u201d and with 4 output units representing the four classes of student performance categories Distinction, Pass, Fail, and Withdrawn. After adding layers, the constructed model was compiled. The optimizer hyperparameter was \u2018Adam \u2018. The loss function was set to \u2018Categorical_Crossentropy \u2018. As a metric of evaluation, \u2018Categorical_Accuracy\u2019 was used.\nAfter the ANN-LSTM model was constructed, the training stage started. In the training stage the training data was fed to the ANN-LSTM model with the number of epochs set to 100 as reference [] and batch size set to 100.\u201d Fit\u201d method provided by Keras API was used for training the ANN-LSTM model.\nTwo evaluation stages were applied, validation and testing. From the validation stage precision, recall, and F1-score measurements were recorded. Then, the ANN-LSTM model was evaluated by the \u201cevaluate\u201d method provided by Keras API with pre-determined testing data. The batch size during the evaluation stage was 100. In each time step experiment training and validation accuracy were compared. Finally, accuracy was recorded to be compared with other experiments\u2019 accuracy.\n\n4.4. Baseline models construction\nRNN and GRU models were constructed to check the validity of the ANN-LSTM model. The RNN model had the same sitting as the ANN-LSTM model except for the input layer. So, the input layer of the RNN model is an RNN network consisting of 200 \u201csimple RNN cells\u201d. Likewise, the GRU model had the same sitting as the ANN-LSTM model except for the input layer. So, the input layer of the GRU model is a GRU network.\nAlso, DFFN proposed in Ref. [], AML model proposed in Ref. [], and ML models in Ref. [] were used as baseline models because they implemented multi-class classification which are closed to this work.\nTwo stages of evaluation were applied, validation and testing. From the validation stage precision, recall, and F1-score measurements were recorded. Then, RNN and GRU models were evaluated by the \u201cevaluate\u201d method provided by Keras API with pre-determined",
        "summary": "After the ANN-LSTM model was constructed, the training stage started. In each time step experiment training and validation accuracy were compared to other experiments\u2019 accuracy. Baseline models construction were constructed to check the validity of theANN-L STM model"
    },
    {
        "link": "https://www.sciencedirect.com/science/article/pii/S2405844023025896",
        "body_split": " testing data. The batch size during the evaluation stage was 100. In each time step experiment training and validation accuracy were compared. Finally, accuracy was recorded to be compared with ANN-LSTM results.\n5. Results\nThis subsection shows the experimental results which are divided into: the accuracy of the ANN-LSTM, the comparison between the results of ANN-LSTM model with baseline models results and the comparison with the state of the art results. The chosen models in the state of the art are DFFNN, AML and RF.\n5.1. ANN-LSTM daily accuracy\nThe accuracy obtained on the first day of the course was 43% in the testing data. The accuracy of the ANN-LSTM model in the training data was higher than the accuracy in the validation data on the first day of the module. From day 15, the accuracy of the ANN-LSTM in the validation data was close to the accuracy of the ANN-LSTM in the training data. The accuracy was approximately 56%. After day 15 the accuracy of the ANN-LSTM in the validation data was higher than the accuracy of the ANN-LSTM in the training data. By day 41 the accuracy reached 65%. Per day 270 which represents the last day in the longest module (course), the accuracy reached 72%. Generally, the accuracy of the ANN-LSTM model in the testing data started from 43% on the first day of the course and reached 72% on the last day of the course duration. Fig. 4 shows the accuracy for all course days.\n5.1.1. Comparison of ANN-LSTM model with RNN and GRU baseline models\nTo check the validity of the ANN-LSTM model, the same prepared data of demographic and clickstream for the first three months of course duration was input to RNN and GRU models. GRU model got 43% accuracy on the first day and 54% in day 30. In the RNN model, the accuracy on the first day was about 45% and on day 30 the accuracy was about 52%. Although the RNN model got better accuracy than ANN-LSTM in the first eight days, the ANN-LSTM model outperformed RNN and GRU models in the rest course days. Generally, the ANN-LSTM model got better accuracy than RNN and GRU models. Fig. 5 shows the accuracy of the ANN-LSTM, RNN",
        "summary": " The accuracy of the ANN-LSTM model in the training data was higher than the accuracy in the validation data on the first day of the course. By day 41 the accuracy reached 65%. Per day 270 which represents the last day in the longest module (course) The accuracy reached 72%"
    },
    {
        "link": "https://www.sciencedirect.com/science/article/pii/S2405844023025896",
        "body_split": ", and GRU models in the first, second, and third months.\nThe arithmetic average of the accuracy enhancement rate between the ANN-LSTM and GRU model was about 15% while the arithmetic average of the accuracy enhancement rate between the ANN-LSTM and RNN model was about 21%. Fig. 6, Fig. 7 show the accuracy enhancement rates in the first three months of the ANN-LSTM model compared with GRU and RNN models, respectively.\nUsing macro F1-score measurement, which treats all classes equally regardless of the class size, the best result was obtained by the ANN-LSTM model. The value of the macro F1-score obtained by the RNN model in the first three months was between (20% and 30%). GRU model got a macro F1-score value between (23% and 40%), while the ANN-LSTM got a value between (24% and 61%). Fig. 8 shows the macro F1-score of the first three months.\nUsing micro F1-score measurement, which treats all instances equally so the weight of each class will differ based on the number of instances belonging to it [], the best result was obtained by the ANN-LSTM model. The value of the micro F1-score obtained by the RNN model in the first three months was between (28% and 41%). GRU model got a micro F1-score value between (29% and 47%), while the ANN-LSTM got a value between (31% and 63%). Fig. 9 shows the micro F1-score of the first three months.\nTo give the majority class higher weight, a weighted F1-score was calculated. The value of the weighted F1-score obtained by the RNN model in the first three months was between (26% and 38%). GRU model got a weighted F1-score value between (27% and 46%), while the ANN-LSTM got a value between (29% and 63%). Fig. 10 shows the weighted F1-score for the first three months.\nComparing macro, micro, and weighted F1-score for each model separately, the ANN-LSTM model got very close values of macro, micro, and weighted measurements. This phenomenon highlights that the ANN-LSTM model behaves equally when treating all classes equally or when giving a majority class higher weight. In RNN and GRU models micro F1-score",
        "summary": " The arithmetic average of the accuracy enhancement rate between the ANN-LSTM and GRU model was about 15% while the arithmetic average was about 21%"
    },
    {
        "link": "https://www.sciencedirect.com/science/article/pii/S2405844023025896",
        "body_split": " values were higher than macro and weighted F1-score values. This implies that RNN and GRU models do well in the case of treating all instances equally. Fig. 11, Fig. 12, and Fig. 13 show the macro, micro, and weighted F1-score of ANN-LSTM, GRU, and RNN Models respectively.\n\n5.2. Comparison of ANN-LSTM model with DFFNN, AML and RF models\nANN-LSTM model got better accuracy than the DFFNN model proposed in Ref. [] in the case of using demographic and clickstream data on the last day of the courses. DFFNN got an accuracy of 63% while ANN-LSTM got 72%. This rate of improvement is due to the ability of LSTM to remember long dependencies between parameters. Table 2 shows a comparison between ANN-LSTM and DFFNN models when using demographic and clickstream data.\nOther similar models to compare are RF models proposed in Ref. [] which got the best results among GB, DFFNN, Gaussian NB, Bernoulli NB, KNN, DT, LR, ada boost, SVM, and Extra tree classifier. As it is clear in Table 3 ANN-LSTM model got the best score in most cases and it got the best accuracy, macro, and weighted F1-score. This may be justified by ANN-LSTM being a time series model and keeping latent dependencies between futures while RF is not a time series. The accuracy obtained was 72%, 66%, and 66% for ANN-LSTM, RF \u201cgini\u201d and RF \u201centropy\u201d models respectively. In the F1-score metric, the ANN-LSTM got a better rate in the Distinction, Fail, and Withdrawn classes while RF got a better F1-score rate in the Pass class. It's possible that having a memory cell in the LSTM that can track the learner's weekly behavior [] is one of the underlying factors causing the ANN-LSTM model getting best results than the DFFNN and RF models.\nWhen comparing the ANN-LSTM model with the AML model proposed in Ref. [], as is seen in Table 4, ANN-LSTM outperforms the AML model by about 10% in the 5th week and all other weeks. This may be because ANN-LSTM was trained with",
        "summary": " RNN and GRU models do well in the case of treating all instances equally. DFFNN got an accuracy of 63% while ANN-LSTM got 72%. This rate of improvement is due to the ability of LSTM to remember long dependencies between parameters"
    },
    {
        "link": "https://www.sciencedirect.com/science/article/pii/S2405844023025896",
        "body_split": " day-wise data while AML was trained with aggregated weekly data. Another possible reason is that in the AML model, all activity interaction data was added to one variable which represents the sum of all clicks in all activities while ANN-LSTM treats each activity separately.\nFinally, Fig. 14 summarizes the comparison between baseline models proposed in similar studies in terms of accuracy, macro F1-score, and weighted F1-score. ANN-LSTM got the best result with 72% accuracy, 66% macro F1-score, and 68% weighted F1-score. Fig. 15 shows the enhancement rate of ANN-LSTM Compared with DFFNN, RF, and AML in terms of accuracy.\n6. Discussion\nThe ANN-LSTM model was developed to predict the class of students' performance in the multi-class form to achieve the first objective of this study which was to develop a day-wise deep learning model to predict students\u2019 performance in multi-class form. Generally, the ANN-LSTM got low accuracy for the first 15th days with an accuracy of 43% on the first day of the course. This may be due to a variety of students' behaviors and a lack of click stream data in the early days of the course. From day 15 data seems to be more representative. Hence, the accuracy of the ANN-LSTM in the validation data was close to the accuracy of the ANN-LSTM in the training data and the accuracy reached 56%.\nThe results show that the ANN-LSTM model obtained the best students' performance prediction accuracy. The average enhancements' rates of accuracy obtained in the first three months between the ANN-LSTM model and GRU and RNN were 15% and 21%, respectively. Although the RNN model got better accuracy than ANN-LSTM in the first eight days, the ANN-LSTM model outperformed RNN and GRU models in the rest course days. This phenomenon may be justified by the ability of LSTM to remember the previous student's behavior based on the architecture of the LSTM network. The answer to the second research objective, which was to determine which deep learning model among RNN, LSTM, and GRU can get better students' performance prediction accuracy, is that the LSTM model obtained the best students' performance prediction accuracy.\nThe result of comparing the ANN-LSTM model with similar state-of-the-",
        "summary": "Fig. 14 summarizes the comparison between baseline models proposed in similar studies in terms of accuracy, macro F1-score, and weighted F 1-score. In the AML model, all activity interaction data was added to one variable which represents the sum of all clicks in all activities while ANN-LSTM treats each activity separately"
    },
    {
        "link": "https://www.sciencedirect.com/science/article/pii/S2405844023025896",
        "body_split": "art models, which was the third objective of this research, is that ANN-LSTM obtained the best accuracy. ANN-LSTM model got better accuracy than the DFFNN model proposed in Ref. [] in the case of using demographic and clickstream data on the last day of the courses. DFFNN got an accuracy of 63% while ANN-LSTM got 72%. This rate of improvement is due to the ability of LSTM to remember long dependencies between parameters. Compared with RF models proposed in Ref. [], the ANN-LSTM model got the best score in most cases and it got the best accuracy, macro, and weighted F1-score. This may be justified by ANN-LSTM being a time series model and keeping latent dependencies between futures while RF is not a time series. Compared with the AML model proposed in Ref. [], as is seen in Table 4, ANN-LSTM outperforms the AML model by about 10% in week 5 and all other weeks. This may be because ANN-LSTM was trained with day-wise data while AML was trained with aggregated weekly data. Another possible reason is that in the AML model, all activity interaction data was added to one variable which represents the sum of all clicks in all activities while ANN-LSTM treats each activity separately. So, treating behavioral features in day-wise form increases prediction accuracy.\n7. Conclusion and future work\nA multi-class course-agnostic day-wise deep learning predictive model named Artificial Neural Network-Long Short-Term Memory (ANN-LSTM) was constructed to periodically predict the class of student performance in MOOC Massive Open Online Courses (MOOC). The ANN-LSTM model was developed to predict the class of students' performance in multi-class form. In addition, RNN and GRU models were developed, trained, and evaluated to determine which deep learning model among Recurrent Neural Network (RNN), Long-Short Term Memory (LSTM), and Gated Recurrent Unit (GRU) can get better students' performance prediction accuracy. Then, the results of RNN, GRU, and ANN-LSTM models were compared in terms of accuracy, precision, recall, and F1-score. The results show that the ANN-LSTM model obtained the best students' performance prediction accuracy. The average enhancements' rates of accuracy obtained in the first three months between the ANN-LST",
        "summary": " ANN-LSTM got better accuracy than the DFFNN model proposed in Ref. [], in the case of using demographic and clickstream data on the last day of the courses. This rate of improvement is due to the ability of LSTM to remember long dependencies between parameters"
    },
    {
        "link": "https://www.sciencedirect.com/science/article/pii/S2405844023025896",
        "body_split": "M model and GRU and RNN were 15% and 21%, respectively. Furthermore, for more validation of ANN-LSTM efficiency, the ANN-LSTM has been compared with the state-of-the-art models in terms of accuracy, recall, precision, and F1-score regardless of the implementation environments. ANN-LSTM model obtained the best accuracy results with enhancement rates of about 6% when compared with the AML model, 9% when compared with the RF model, and 14% when compared with the DFFNN model at the end of the course duration. In addition, the conclusions are as follows: Students' data (demographic and behavioral data) in the first days of course duration (especially the first three months) shows good results in the case of MOOC students\u2019 performance prediction. Hence, there are opportunities to predict the class of students in MOOC courses during the first months.\nAs a limitation of this study, researchers may need to consider that MOOC students generate massive clickstream interaction records and deep Learning techniques consume a long time in training computations. This may cause delay data processing, and consequently delay evaluating research results.\nIn future works, more studies and experiments are needed to increase the accuracy of prediction in the first days of course duration with additional preprocessing methods like trying to make the OULAD dataset balanced using under-sampling or oversampling techniques. Furthermore, study the effect of the functional model instead of the sequential model design, using assessment data on predicting student performance, assigning weights for each class based on the class importance, and aggregating clickstream data periodically on the accuracy of predicting student's performance.\nAuthor contribution statement\nFatima Ahmed Al-azazi: Conceived and designed the experiments; Performed the experiments; Analyzed and interpreted the data; Contributed reagents, materials, analysis tools or data; Wrote the paper.\nMossa Ghurab: Conceived and designed the experiments; Analyzed and interpreted the data.\nData availability statement\nData associated with this study has been deposited at original dataset - https://analyse.kmi.open.ac.uk/open_dataset prepared dataset - https://drive.google.com/file/d/1S5_k1VPA_U5mX1B7ac_ZxSjw18N1QA-p/view?usp=sharing ANN-LSTM model - https://github.com/",
        "summary": "Ann-LSTM model obtained the best accuracy results with enhancement rates of about 6% when compared with the AML model. GRU and RNN were 15% and 21% respectively. For more validation of ANN-L STM efficiency, the ANN- LSTM has been compared with state-of-the-art models"
    },
    {
        "link": "https://www.sciencedirect.com/science/article/pii/S2405844023025896",
        "body_split": "FatimaAlazazi/ANN-LSTM-model.\n",
        "summary": "imaAlazazi/ANN-LSTM-model is a model of the world's most popular model. The model is based on the concept of the European Union"
    },
    {
        "link": "https://www.sciencedirect.com/science/article/pii/S2590116823000206",
        "body_split": "1.1. Background\nThe electrification of transportation such as electric vehicles, electric ships, and electric aircraft has developed rapidly in recent years, where lithium-ion batteries serve as the main energy storage devices that benefit from their high energy and power density, low self-discharge rate, and long lifespan []. However, due to the side reactions that occurred along with the charging and discharging cycles, the batteries degrade with usage [], which influences serviceability, reliability, and safety. The health prognostics of the battery are essential for the battery management system (BMS) to guide predictive maintenance that avoids abusive usage, ensures reliable and safe operation, and extends the lifetime []. Besides health estimation and prediction, thermal estimation is important to reflect the safety status of the batteries, especially in sensor-free conditions, as a tool for developing thermal runaway warning strategies []. Therefore, the temperature variation and current capacity are key information for the prognostics of battery health. Unfortunately, the capacity cannot be measured directly and temperature sensors are not implemented on every battery in real applications. Therefore, advanced methods for indirect assessment of battery health are required, which becomes a hot topic in battery management recently for researchers []. Besides, artificial intelligence develops rapidly in recent years, which provides more opportunities for smarter battery management []. Inspired by the main interest of both academia and industry, this paper focuses on battery health prognostics with differential temperature (DT or dT) elementary reconstruction and state of health (SoH) estimation for energy storage systems using unlabeled aging data and without temperature measurement based on novel transfer learning strategies.\n\n1.2. Literature review\nModel-based and data-driven are the two main categories in battery state estimation and health prognostics. Model-based method generally first builds electrochemical models to capture the electrical/thermal characteristics of batteries [,]. Then, the parameter estimation method based on various optimization algorithms is conducted for state estimations and health prognostics [,]. Nevertheless, the high complexity and computation burden make it challenging for the practical implementation of model-based methods. On the other hand, data-driven methods are more flexible, easier to implement, can avoid those complex modeling processes, etc. [], which attracts more and more concerns in recent years.\nSoH, which is defined as the ratio of the current capacity to the nominal capacity (or the capacity of the fresh cell), is one key index to evaluate the health status and aging condition of the battery []. Data-driven battery SoH",
        "summary": " The health prognostics of the battery are essential for the battery management system (BMS) to guide predictive maintenance that avoids abusive usage, ensures reliable and safe operation, and extends the lifetime. Thermal estimation is important to reflect the safety status of the batteries, especially in sensor-free conditions"
    },
    {
        "link": "https://www.sciencedirect.com/science/article/pii/S2590116823000206",
        "body_split": " estimation can be divided into feature based-methods and feature-free methods. In the feature-based method, manual health indicator (HI) extraction and selection are required []. Methods for HI extraction can be divided into measured data-based extraction and calculated data-based extraction. In the measured data-based method, as the voltage, current, and time are recorded online by the BMS, the HIs could be extracted from these data []. For example, the widely used time interval during certain voltage ranges, voltage slope, voltage skewness, etc. [], based HIs have shown satisfactory correlations with battery SoH and helped estimate SoH accurately with machine learning algorithms, such as linear/multi-linear regression, support vector regression, and gaussian process regression, etc. [,, ]. In addition to the direct HIs extraction from the measured data, another way to extract the HIs from parameters such as the incremental capacity (IC) and differential voltage (DV) []. The peak values, valley values, and peak areas of IC and DV curves are widely used HIs. Besides, the DT curve has also been proven to reflect battery aging and could help improve the accuracy of SoH estimation along with the IC curve []. After the HIs have been extracted, the feature selection process is required to select the most relevant HIs as the final input of the data-driven model. The most popular way is to calculate the correlation coefficients between the HIs and the capacity to help find HIs which are highly correlated with battery capacity []. Published works have proven that the wrapper and fusion-based feature selection methods could also help reduce the feature redundancy to help improve the SoH estimation []. In the feature-based method, the HI extraction and selection processes are the key steps affecting the final prognostic performance. On the other hand, in feature-free based methods, the raw measurement data is used as input for machine learning or deep learning models directly []. A popular feature-free method is to use an auto-encoder and -decoder for automatic feature extraction, with two of the more widely used types of encoder and decoder being recurrent neural networks and convolutional neural networks due to their ability to account for time and spatial dimensions [,, ]. Based on the deep learning method, the battery SoH can be estimated without manual feature engineering []. For example, the hybrid network combining convolutional module, ultra-lightweight subspace attention mechanism module, and recurrent unit module was used for decoding",
        "summary": " Methods for HI extraction can be divided into measured data-based extraction and calculated data- based extraction. In the measured Data-based method, as the voltage, current, and time are recorded online by the BMS, the HIs could be extracted from these data"
    },
    {
        "link": "https://www.sciencedirect.com/science/article/pii/S2590116823000206",
        "body_split": " with partial charging curve as input while the simple back propagation neural network was used to decode the hidden states for battery SoH estimations [].\nThe trained data-driven model mapping the relationship between the input HIs or measured data and battery SoH is then used for estimating the SoH of test batteries. However, the specific mapping might not be suitable for different operation scenarios, where the domains have clear discrepancies. Therefore, researchers have proposed transfer learning to improve the model performance on the SoH estimation for test batteries. Two kinds of transfer learning strategies are widely used (1) to retrain the model using only a few labeled data collected from the test batteries [,], and (2) to integrate the reduction strategy of domain discrepancy between the training battery and testing battery into the model []. In practical applications, it is very difficult to obtain labeled data from the test battery for model improvement. Therefore, methods that only take advantage of the unlabeled data of the test battery to improve the estimation accuracy are significantly more important, which is the main focus of this paper. In addition, batteries undergo variable temperature conditions in practical applications due to environmental variations. SoH estimation verifications of data-driven methods under variable temperatures have not been seen in the existing literature.\nIn addition to the direct estimation of battery SoH, more aging-related prognostics have been conducted. Tian et al. [], proposed a novel deep-learning method for the charging curve prediction with only 30 points collected in 10 min under different aging conditions. The effects have been validated with different batteries aging with different current rates and temperatures. They further implemented the deep neural network for the open circuit voltage reconstruction using partial charging data, where the electrode aging parameters can also be obtained directly []. Tang et al. [], proposed an IC curve reconstruction method that ensures effective feature extraction under noisy conditions for battery SoH estimation. Moreover, temperature variation estimation is also important in battery health prognostics. The temperature variation changes with the battery aging [], seen as e.g., the incremental value of temperature increases with aging. It is vital to estimate the temperature variation accurately, as it helps provide information for thermal management to avoid thermal runaway. Furthermore, the DT curve also provides additional information which can be used to improve the accuracy of SoH estimation as illustrated in Ref. []. For the data-driven temperature estimation, the long-short-term memory (LSTM) neural network is used to predict the future surface temperature using historical information including temperature, voltage, current, and state",
        "summary": " The trained data-driven model mapping the relationship between the input HIs or measured data and battery SoH is then used for estimating the SoH of test batteries. But the specific mapping might not be suitable for different operation scenarios, where the domains have clear discrepancies"
    },
    {
        "link": "https://www.sciencedirect.com/science/article/pii/S2590116823000206",
        "body_split": " of charge []. The temperature variation with battery aging was decomposed as reversible heat and irreversible heat, and the LSTM was used to predict the temperature variations []. However, most data-driven temperature estimation or prediction method needs the measured temperature for the input, while not all the cells have a related temperature sensor in the real world. Therefore, the temperature variation needs to be estimated in the context of a temperature sensor-free environment to provide the thermal behavior of a battery. The investigation of the effectiveness of the SoH estimation with additional estimated temperature characteristics is also valuable while lacking in the existing literature.\n\n1.3. Contribution of this work\nThis paper proposes a proper method for battery health prognostics with dT curve reconstruction and SoH estimation to handle the gaps mentioned above. The following main contributions distinguish this paper from existing works. 1) The dT curve is reconstructed by using the Q-V curve without the requirement of measured temperature to add information in battery health prognostic. The sequence-to-sequence (STS) method is proposed for the dT curve reconstruction by using dQ curve. 2) An end-to-end framework is proposed for the SoH estimation with measured dQ-V and reconstructed dT-V information without the requirement of manual feature extraction and selection. 3) Multiple domain adaptation with maximum mean discrepancy (MMD) is proposed to reduce the domain discrepancy between the training battery and test battery to improve the accuracy of the health prognostics. Both the dT curve reconstruction and SoH estimation will benefit from this transfer learning framework without labeled data from the test battery. 4) Both experimental data and publicly available data are used for the evaluation of the proposed method, where different battery chemistry and formats, temperature variations, dT curve shapes, and capacity degradation patterns are used for the evaluations of the proposed framework.\nThe health prognostic framework for the STS dT curve reconstruction and sequence-to-point (STP) SoH estimation based on the domain adaptative end-to-end framework is shown in Fig. 1. The Q-V curve is transformed into dQ-V curve (i.e., building the dQ sequence) for input into the neural network. The dT curve reconstruction network takes the dQ sequence as the input, using the LSTM layer for hidden information extraction since the high coupling relationship on the time series data, and finally, a fully connected layer is added to output the predicted dT sequence. The MMD loss is",
        "summary": "This paper proposes a proper method for battery health prognostics with dT curve reconstruction and SoH estimation to handle the gaps mentioned above. The dT Curve is reconstructed by using the Q-V curve without the requirement of measured temperature"
    },
    {
        "link": "https://www.sciencedirect.com/science/article/pii/S2590116823000206",
        "body_split": " added to the output of the second LSTM to reduce the domain discrepancy of the feature distributions between the source battery and the target battery. The estimated dT sequence and the dQ sequence are then used for the SoH estimation of batteries, where the MMD loss is also integrated after the second LSTM layer to reduce the domain discrepancy before the final output. The proposed health prognostic strategy is finally verified using different batteries working under different scenarios.\n\n1.4. Article organization\nThe remainder of this paper is arranged as follows: Both the experimental and public datasets for the verification are introduced in section 2. Then, the main framework of the proposed method will be described in section 3. The prognostic results will be presented and evaluated in section 4, and lastly, the main conclusions of the paper are summarized in section 5.\n2. Data description\nTo better demonstrate the effectiveness of the proposed framework, two private experimental datasets and two public datasets are used. Detailed information for all the datasets used in this paper is listed in Table 1.\nThe private lab data (denoted as Lab dataset #1 and Lab dataset #2 in Table 1) consists of two pouch cells (with a nominal capacity of 8 Ah). In Lab dataset #1, the batteries are aged using a multi-stage fast constant current (MCC) and constant voltage (CV) charging, and dynamic discharging profile. Specifically, batteries were charged with a series MCC (i.e., 10 C-5 C-3 C\u20131 C in order). The batteries were charged using 10C first until the voltage reaches the upper limit, i.e., 4.2 V. Then the current was reduced to 5C, 3C, and 1C in sequence to charge until 4.2 V. Then CV charging is followed until the current drops below 0.1C. During discharging, 100 A pulses lasting 10 s are used first to discharge the battery, thereby preventing its voltage from varying beyond the upper voltage limit. The battery was then discharged with two dynamic discharging profiles, one is the urban dynamometer driving schedule (UDDS) and another is HWFET tests for the purpose to simulate battery aging under urban driving conditions and highway driving conditions, respectively. Finally, a 50 A current is loaded to discharge the battery until its voltage drops below the lower voltage limit, i.e., 2.75 V. Furthermore, a short rest lasting 120 s was added between the charging and discharging profiles. The load current as",
        "summary": " The proposed health prognostic strategy is finally verified using different batteries working under different scenarios. The main framework of the proposed method will be described in section 3. The prognostic results will be presented and evaluated in section 4, and lastly, the main conclusions are summarized in section 52"
    },
    {
        "link": "https://www.sciencedirect.com/science/article/pii/S2590116823000206",
        "body_split": " well as the corresponding voltage and temperature responses for the cycling profile with UDDS and HWFET are shown in Fig. 2 (a) and Fig. 2 (b), respectively. A fast temperature increase was found in MCC charging, especially in the first stage of the charging process (at 10C). Thus, accurately estimating the dT curve during fast MCC is important to avoid thermal runaways, which could have catastrophic consequences in the real world.\nAs for the second-life batteries (Lab dataset #2), a typical loading profile of CC-CV/CC is used to continue aging the batteries at 2.5C. The load current, corresponding voltage, and measured temperature are plotted in Fig. 1 (c). It can be seen that the temperature mainly increases during the discharging process, which is different from the primary battery aging process. It shows that the temperature variation of the new batteries (i.e., in Lab dataset#1) is larger than 15 \u00b0C while that of the second-life batteries (i.e., in Lab dataset#2) is larger than 10 \u00b0C. The temperature variation of the new batteries is larger than the second-life batteries because of the larger current rates. This further solidifies the importance of dT curve estimation to provide key information for predictive maintenance of the battery cell. In addition, the environmental temperature of the Lab dataset#2 is changeable between 25 and 35-25-15 \u00b0C, which will cause discontinuous degradation curves. It is more related to real scenarios where batteries are aged under variable temperature conditions due to climate change, which makes the verification more conformed to practical applications.\nFor the public dataset, the data sets from Oxford university [] (Ox dataset) and MIT-Stanford group [] (M \u2212 S dataset) were used. Detailed information is listed in Table 1. The Ox dataset also uses the pouch cell but with a much smaller current rate and different nominal capacity from the Lab datasets. The M \u2212 S dataset uses a large current rate but has different battery chemistry and format from the Lab datasets. It is worth mentioning that the used four datasets cover different scenarios including battery chemistries (i.e., LFP and NCA), battery formats (i.e., pouch and cylindric), and working currents (i.e., high and low current). The large range of operation scenarios, conditions, and chemistries causes large differences in temperature variation and degradation patterns. Therefore, these datasets create a good basis for verifying the robustness",
        "summary": " A fast temperature increase was found in MCC charging, especially in the first stage of the charging process (at 10C) The temperature variation of the new batteries is larger than the second-life batteries because of the larger current rates. This further solidifies the importance of dT curve estimation to provide key information for predictive maintenance of the battery cell"
    },
    {
        "link": "https://www.sciencedirect.com/science/article/pii/S2590116823000206",
        "body_split": " and generalization of the proposed framework.\n3. Methodology\nThe detailed methods for the dT curve reconstruction and SoH estimation are described in this section. Firstly, the STS framework for dT curve reconstruction is introduced in section 3.1. Then, by integrating the dQ sequence and the reconstructed dT sequence, the STP-based battery SoH estimation is described in section 3.2. Finally, the domain adaptation for performance improvement via unlabeled data is proposed in section 3.3.\n3.1. DT curve reconstruction\nThe dT curve reconstruction is achieved by an STS prediction framework. The input and output of the deep learning model are the dQ sequence and dT sequence, respectively. The following steps are used to form the dQ sequence and dT sequence. When the dQ sequence is constructed, the voltage is required to pass through a certain voltage range to ensure homogeneity. Thus, Q-V curve is chosen to be in the voltage range [3.2 V, 3.6 V] for LFP battery and [3.6 V, 4.2 V] for NCA battery during primary life and [3.0V, 4.0 V] for the second life battery (i.e. Lab dataset#2). For the Oxford dataset, the whole voltage range is used to demonstrate the whole dT curve variation under a small current rate. The different voltage ranges can also be used to demonstrate that the proposed method could work with both different partial and full Q-V data with these different settings. Then, a predefined voltage interval (0.025V in this paper) is used to split the V sequence and a new Q sequence is obtained by interpolating the Q-V curve based on the split V sequence. Finally, the dQ sequence could be calculated by the difference between each element and the first value in the Q sequence. The same process is used for pre-processing the T-V curve to obtain the dT sequence.\nThe neural network used for the end-to-end STS dT curve reconstruction is shown in Fig. 3, where the LSTM layer is used for hidden information extraction. LSTM has been widely used in battery SOH estimation and prediction and has proven to be superior in health prognostic []. The basic equations governing an LSTM cell are denoted as follows []. (1) f(t)=\u03c3( w f1 x(t)+ w f2 h(t",
        "summary": " The STS framework for dT curve reconstruction is introduced in section 3.1.1. By integrating the dQ sequence and the reconstructed dT sequence, the STP-based battery SoH estimation is described. Finally, the domain adaptation for performance improvement via unlabeled data is proposed in section3.3"
    },
    {
        "link": "https://www.sciencedirect.com/science/article/pii/S2590116823000206",
        "body_split": "\u22121)+ b f ) (2) i(t)=\u03c3( w i1 x(t)+ w i2 h(t\u22121)+ b i ) (3) S \u02dc (t)=tanh( w c1 x(t)+ w c2 h(t\u22121)+ b c ) (4) S(t)=f(t)\u2299S(t\u22121)+i(t)\u2299 S \u02dc (t) (5) o(t)=\u03c3( w o1 x(t)+ w o2 h(t\u22121)+ b o ) (6) h(t)=o(t)\u2299tanh(S(t)) where x(t) and h(t) are the input and output, S(t) is the state information, f(t), i(t), and o(t) are the information updated by the forget gate, input gate, and output gate respectively, w and b are the weights and biases, \u03c3 and tanh are the activation functions.\nAfter the second LSTM, a fully connected layer is added to output the estimated dT sequence. The numbers of neurons used in the two LSTM layers are set to 50 and 30, respectively. Since the dT curve is estimated by inputting the dQ sequence, and the sequences are obtained by splitting the corresponding curves with the same voltage sequence, the output length is the same as the input.\n\n3.2. End-to-end SoH estimation\nAfter reconstructing the dT curve, the temperature variation information is added together with the dQ curve for the end-to-end STP SoH estimation. Specifically, the dQ sequence is selected as one input dimension. In addition, the information from the temperature variation also contains important aging information and helps improve the SoH estimation accuracy, which is proved in the previous study []. Therefore, the reconstructed dT curve is added as a second input dimension to the deep learning model. The definition of SoH in this paper is the ratio of the current test capacity (Ci) and the capacity of the first cycle (C0) [,], which is given below. (7) SOH= C i / C 0\nThe structure of the neural network for the STP SoH estimation is shown in Fig. 3. It follows the same general structure as the deep neural network used for the dT curve reconstruction, two L",
        "summary": "The structure of the neural network for the STP SoH estimation is shown in Fig. 3. It follows the same general structure as the deep neural network used for the dT curve reconstruction, two LSTM layers. The numbers of neurons used in the two L STM layers are set to 50 and 30, respectively"
    },
    {
        "link": "https://www.sciencedirect.com/science/article/pii/S2590116823000206",
        "body_split": "STM layers are first used to extract the hidden aging information. Then, a fully connected layer with one neuron is added to the output for the final SoH estimation. The hyperparameters are set as the same as the model in dT curve reconstruction instead of the output neuron, which is set as 1 for the SoH estimation.\n\n3.3. Transfer learning with domain adaptation\nDeep learning-based battery health prognostics for dT curve reconstruction and SoH estimation have been introduced above. However, conventional data-driven methods suffer from poor generalization, as the model trained on the source battery may have poor performance on a target battery, especially when the application scenarios show obvious dissimilarities. These dissimilarities will cause large domain discrepancies between the source battery and the target battery. In most existing transfer learning-based battery health prognostic methods, a few labeled data from the target domain are used to fine-tune the model. However, the labeled data are generally unavailable in real-world applications. Therefore, it is more valuable to use unlabeled to improve the prognostic accuracy. To this end, this paper adopts the MMD to reduce the domain discrepancy of the hidden features outputted by the second LSTM. The implementation of the MMD in the prognostic model is shown in Fig. 3, where the domain discrepancy of the outputs at the last time step of the second LSTM in the source and target domains is reduced, which helps improve the estimation accuracy of dT curve and SoH.\nThe MMD is a measure of the difference between two probability distributions in the mean embedding of the features []. Given two samples in two datasets X= { x i } n 1 i=1 and Y= { y i } n 2 i=1, the MMD between the X and Y could be expressed as [], (8) MMD H (X,Y)= sup \u03a6\u2208H ( E p [\u03a6(x)]\u2212 E q [\u03a6(y)]), where H represents a reproducing kernel Hilbert space (RKHS), \u03a6(\u2219) is a nonlinear mapping function from raw data space to the RKHS space, and p and q are the probability distributions of generating the two data sets. The empirical approximation to the MMD can be denoted as follows [,], (9) MMD 2 H (X,Y)= \u2225 1 n 1 \u2211 n 1 i=1 \u03a6(",
        "summary": " The MMD is a measure of the difference between two probability distributions in the mean embedding of the features. This paper adopts the MMD to reduce the domain discrepancy of the hidden features outputted by the second LSTM. The implementation of the M MD in the prognostic model is shown in Fig. 3"
    },
    {
        "link": "https://www.sciencedirect.com/science/article/pii/S2590116823000206",
        "body_split": " x i )\u2212 1 n 2 \u2211 n 2 j=1 \u03a6( y j ) \u2225 2 H. The kernel trick is then used to get the expression [], (10) MMD 2 H (X,Y)= 1 n 2 1 \u2211 n 1 i=1 \u2211 n 2 j=1 k( x i, x j )\u2212 2 n 1 n 2 \u2211 n 1 i=1 \u2211 n 2 j=1 k( x i, y j ) + 1 n 2 2 \u2211 n 1 i=1 \u2211 n 2 j=1 k( y i, y j ), where k(\u2219,\u2219) is the kernel function of the RKHS, where the Gaussian radial basis function (RBF) is used [], (11) k( x i, y j )=(\u2212 \u2225 x i \u2212 y j \u2225 2 )/2 \u03b3 2.\nIn the training process for both the dT curve reconstruction and SoH estimation, the mean square error is used to evaluate the fitting performance of the output, while the MMD is used as an additional loss to evaluate the domain discrepancy of the hidden features outputted by the second LSTM. Therefore, the final loss function is the combination of the regression loss and the transfer loss, which is denoted as, (12) L= L MSE + \u03bbL MMD, where \u03bb is the transfer loss weight, which represents the penalty coefficients to denote how much the domain adaptation needs to be considered. The transfer loss weight is set as 0.01 in this paper. In this way, the trained model could estimate the SoH or dT curve while considering the domain discrepancy between the source battery and the testing battery. The model is built by PyTorch, where the Adam optimizer is used to train the parameters of the neural networks.\n4. Health prognostic results and discussion\nThe health prognostic results including the dT curve reconstruction and SoH estimation are presented and evaluated in this section according to the framework described above, where different testing scenarios are considered. To evaluate the accuracy of the prognostic results, the root mean square error (RMSE) and mean absolute error (MAE) are used, which are denoted as follows, (13) RMSE= 1 N \u2211 N i=1 ( z \u02c6 i \u2212 z i ) 2 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u221a",
        "summary": " The model is built by PyTorch, where the Adam optimizer is used to train the parameters of the neural networks. In this way, the trained model could estimate the SoH or dT curve while considering the domain discrepancy between the source battery and the testing battery"
    },
    {
        "link": "https://www.sciencedirect.com/science/article/pii/S2590116823000206",
        "body_split": ", (14) MAE= 1 N \u2211 N i=1 | z \u02c6 i \u2212 z i |, where z \u02c6 i and z i are the estimated value and real value for the i-th observation. The maximum absolute error (MaxAE) is also included in the performance evaluation. In the following subsections, the results for the dT curve reconstruction and SoH estimation are first presented and evaluated using the Lab datasets. Second, the generalization and robustness are evaluated by verifying the results on the two public datasets. Finally, further comparative evaluation and limitations are discussed.\n4.1. DT curve reconstruction\nThe results for the dT curve reconstruction of the primary life battery aging under MCC fast charging and dynamic discharging and the second life battery aging under CC-CV charging and CC discharging are presented and evaluated. The results for the primarily used batteries under MCC fast charging and dynamic discharging are shown in Fig. 4. Fig. 4 (a)\u2013(c) show the results of using cell 1 (UDDS) as the source battery and cell 2 (HWFET) as a target battery (denoted as \u201cL#1_C1_to_C2\u201d), while Fig. 4 (d)\u2013(f) are the results considering the opposite scenario (denoted as \u201cL#1_C1_to_C2\u201d). Fig. 4(a) and (d) are the results obtained by the basic LSTM model while Fig. 4(b) and (e) are the results obtained by the DA-based LSTM, where the transfer loss is added in the total loss function for the parameter training, i.e. the proposed method. Fig. 4(c) and (f) show the peak values of the reconstructed dT curves and the real dT curve. Note that \u201cReal\u201d, \u201cBase\u201d, and \u201cDA\u201d in the figures represent the real curve, results obtained by the basic LSTM model, and the results obtained by the proposed domain adaptative method, respectively. It illustrates that the STS model by the LSTM can reconstruct the dT curve with the measured dQ curve in a scenario without the requirement of the temperature sensor. However, the trend of the reconstructed curve by the basic LSTM model still shows some obvious outliers, which is caused by the domain discrepancy between the training battery and the test battery",
        "summary": " The results for the dT curve reconstruction of the primary life battery aging under MCC fast charging and dynamic discharging are presented and evaluated. The results are shown in Fig. 4 (a)\u2013(c) and (f) show the peak values of the reconstructed dT curves and the real dT Curve"
    },
    {
        "link": "https://www.sciencedirect.com/science/article/pii/S2590116823000206",
        "body_split": ". While the DA model shows better performances, the numerical comparison is given later below. The peak value of the dT curve is one significant feature for battery health prognostic and has a high correlation with the battery capacity [,]. It also shows that the proposed method could improve the accuracy of the estimations from the peak values of the dT curve.\nThe performance of the proposed method for the dT curve reconstruction under variable temperatures for the second-life batteries is also evaluated. The estimation results are shown in Fig. 5, where the interpretation of each subfigure is identical to that of Fig. 4. Under variable temperatures, the dT curves do not have monotonous trends for cycling from start to end because the correlation between the capacity and the environmental temperature also influences the shape of the degradation curves. Under this scenario, the reconstructed curves based on the proposed method have significant improvement compared to the basic LSTM method. Although the trends of the dT curve could be estimated by the basic LSTM, the curves show obvious differences from the real curves and the estimated peak values have large differences from the real values, which will negatively affect the SoH estimation. However, the reconstructed dT curves and the corresponding peak values found using the DA-based LSTM are much closer to the real values, which means that the information from the temperature variation could be extracted to help the health prognostic.\nThe numerical comparisons between the basic LSTM and the proposed DA-based LSTM are listed in Table 2 and Table 3, showing the errors of the reconstructed dT curve and the estimated peak values respectively. Note that the nomenclature for the test scenarios is defined as \u201cDataset_source battery_to_target battery\u201d. Significant improvements can be seen in the proposed method compared to the basic LSTM. The mean RMSE and MAE for the four testing scenarios of the basic LSTM for the dT curve reconstruction were 0.0854 \u00b0C/V and 0.0634 \u00b0C/V, respectively. While those for the proposed method are reduced to 0.0661 \u00b0C/V and 0.0491 \u00b0C/V, i.e. a reduction of 22.6%. Furthermore, the reduction in errors of the estimated peak values, when using the proposed method compared to the basic LSTM was 51.5% and 54.4% respectively. In addition, three generally used machine learning algorithms are also included for comparison of the STS",
        "summary": " The peak value of the dT curve is one significant feature for battery health prognostic and has a high correlation with the battery capacity. It also shows that the proposed method could improve the accuracy of the estimations from the peak values of the DT curve"
    },
    {
        "link": "https://www.sciencedirect.com/science/article/pii/S2590116823000206",
        "body_split": " dT estimation: artificial neural network (ANN), random forest (RF), and Gaussian process regression (GPR). It shows that the basic LSTM, RF, and GPR have similar accuracy and are better than ANN. Although one of them may perform better than the others on some batteries, it also performs worse than others on other batteries, which means the robustness of the conventional data-driven methods is not satisfactory. The mean error of the LSTM, RF, and GPR shows that LSTM is better for most cases but not significantly. All three methods could be used for the STS dT curve reconstruction conventionally, but the proposed DA-based LSTM method can improve the accuracy of the LSTM by accounting for the domain discrepancies and maintaining high accuracy in the different testing scenarios. The MaxAE for the reconstructed dT curves are listed in Table 4, where the LSTM and proposed domain adaptative LSTM are compared. It shows that the mean MaxAE of the reconstructed dT curves for the four testing scenarios is 0.156 \u00b0C/V with basic LSTM while is 0.100 \u00b0C/V with the proposed method. The MaxAE for the peak value between the reconstructed dT curve and the real curve is 0.218 \u00b0C/V and 0.117 \u00b0C/V based on the basic LSTM and the domain adaptative LSTM, respectively. Both of the results indicate that the proposed method improves the accuracy of the dT curve reconstruction. Therefore, it is illustrated from the comparative results that the proposed method has better accuracy and robustness than the conventional data-driven methods.\n\n4.2. SoH estimation\nThe SoH estimation results of the two Lab datasets are presented and evaluated in this section. Due to the different working conditions, the degradation of the two batteries in the two datasets shows significant differences with different degradation rates and lifespans.\nThe estimations for the primary batteries aging under HWFET and UDDS discharging with MCC fast charging are shown in Fig. 6(a)-(b) and Fig. 6(c)\u2013(d), respectively. The SoH estimation results obtained with the basic LSTM reconstructed dT curve, and DA-based LSTM reconstructed dT curve but without DA for the SoH model are also presented for the comparisons, referred to as \u201cBenchmark 1\u201d and \u201cBenchmark 2\u201d, respectively. The proposed method",
        "summary": " The mean error of the LSTM, RF, and GPR is better for most cases but not significantly. All three methods could be used for the STS dT curve reconstruction conventionally, but the proposed DA-based L STM method can improve the accuracy of the lSTM by accounting for the domain discrepancies and maintaining high accuracy in the different testing scenarios"
    },
    {
        "link": "https://www.sciencedirect.com/science/article/pii/S2590116823000206",
        "body_split": " in the figures is a two-stage DA process, which is denoted as the \u201cMulti DA\u201d. The SoH is normalized by dividing the first value because of the manufactural inconsistency. It shows above that when the DA-based model is used for dT curve reconstruction, more accurate temperature variation information could be obtained. Therefore, an obvious improvement from the results obtained from \u201cBenchmark 1\u201d to \u201cBenchmark 2\u201d can be seen in Fig. 6, where a more linear correlation is shown for the figure of estimated SoH to the real SoH. The numerical results of the estimation errors for the SoH are shown in Table 4. It illustrates that the RMSE and MAE are reduced from 3.317% to 3.123%\u20131.687% and 1.524% for cell 2, and 2.241% and 1.828%\u20131.756% and 1.573% for cell 1 respectively from the \u201dBenchmark 1\u201d to \u201cBenchmark 2\u201d. When the DA is also used in the SoH estimation model, the estimated results are improved, as the RMSE and MAE are reduced to 1.364% and 1.257% for cell 2, and 1.671% and 1.519% for cell 1, respectively. Therefore, it indicates from the evaluation of the results above that the multi-DA processes using only unlabeled data help improve the health prognostic for batteries compared to the basic LSTM without domain adaptation.\nThe SoH estimations for the second-life batteries aging under cyclic environmental temperatures are also presented and evaluated. The estimation results for cell 2 and cell 1 are shown in Fig. 7(a)-(b) and Fig. 7(c)\u2013(d), respectively. Note that the jumps shown in the SoH curves are caused by the temperature changes since the charged/discharged capacity under different temperatures is different. Also, two benchmarks are used for the comparisons, which should be interpreted the same way above. The results in Fig. 7(b) and (d) clearly demonstrate the convergence of the real values from \u201cBenchmark 1\u201d to \u201cBenchmark 2\u201d to the \u201cMulti-DA\u201d, improving the accuracy with each step. The results in Fig. 7(a) and (c) show that although linear correlation exists between the estimated SoH and the real SoH for \u201cBenchmark 1\ufffd",
        "summary": " The SoH is normalized by dividing the first value because of the manufactural inconsistency. It shows above that when the DA-based model is used for dT curve reconstruction, more accurate temperature variation information could be obtained"
    },
    {
        "link": "https://www.sciencedirect.com/science/article/pii/S2590116823000206",
        "body_split": "\ufffd and \u201cBenchmark 2\u201d, they diverge from the one-to-one line, implying a divergence between the estimated and real values. While the estimated SoH of the proposed method shows very little divergence indicating a satisfactory linear correlation. The colors of the dots in Fig. 7(a) and (c) do not show the monotonic variations and the dots have discontinuous blanks because the capacity also changes with environmental temperatures. The numerical results shown in Table 4 clearly show the accuracy improvement of the proposed method. It illustrates that \u201cBenchmark 1\u201d has the largest estimation errors compared to \u201cBenchmark 2\u201d and the multi-DA method. The RMSE and MAE of \u201cBenchmark 1\u201d in the two are even larger than 4%, which are significantly reduced to less than 2.1% by the proposed multi-DA method. The errors of \u201cBenchmark 2\u201d are also less than those of \u201cBenchmark 1\u201d, but still larger than that of the proposed multi-DA method. This indicates that the DA-based dT curve reconstruction could provide more accurate temperature variation information to improve the SoH estimation accuracy, and the second DA in the SoH estimation model reduces domain discrepancy of the hidden features for the SoH estimation between the source and target domain, which therefore further improves the SoH estimation.\nThe mean RMSE, MAE, and MaxAE for the four testing scenarios are also shown in Table 5. It shows that \u201cBenchmark 1\u201d has the largest errors, whose RMSE and MAE are 3.344% and 3.038% respectively. The errors are reduced to 2.750% and 2.482% i.e., an error reduction of 17.763% and 18.302%, with the improved dT curve reconstruction to provide better temperature information. When the DA is also used for the SoH estimation, the RMSE and MAE are reduced to 1.772% and 1.532%, respectively. Therefore, the RMSE and MAE have been reduced by 47.010% and 49.572% respectively from \u201cBenchmark 1\u201d to the proposed method for battery SoH estimation under variable temperatures. The reduction trend also shows in the results of MaxAE, indicating the accuracy improvement of the proposed method compared to benchmarks, which reduced from 7.340% to 4.689%. The results indicate the effectiveness of the proposed end-to-end multi-",
        "summary": " The colors of the dots in Fig. 7(a) and (c) do not show monotonic variations and the dots have discontinuous blanks because the capacity also changes with environmental temperatures. \u201cBenchmark 1\u201d has the largest estimation errors, whose RMSE and MAE are 3.344% and 3"
    },
    {
        "link": "https://www.sciencedirect.com/science/article/pii/S2590116823000206",
        "body_split": "DA method for battery health prognostic with dT curve reconstruction and SoH estimation.\nAlthough it has been verified in published works that the information of dT curve could help improve the accuracy of battery SoH estimation, this paper also proves that under variable temperature conditions. To demonstrate the effectiveness of the dT information on the SoH estimation, the comparisons between the proposed method and the ones without dT information are presented in Table 6. \u201cBenchmark without dT\u201d refers to the model trained by only using dQ information and is directly used for the SoH estimation of the test batteries. \u201cDA without dT\u201d means the above model with DA for the SoH estimation. It shows that although the DA could improve the accuracy of the model compared to \u201cBenchmark without dT\u201d, the additional dT information provides more accurate estimations, especially with two stages of DA. The results illustrate that the temperature variation information provided by the dT curve helps improve the SoH estimation, which is because the capacity also varies with temperatures. In real applications, the environmental temperatures also change with the seasons. Therefore, the additional temperature information provided by the reconstructed dT curve based on the proposed prognostic method could not only provide thermal behavior monitoring but also help improve the estimation of SoH.\n\n4.3. Robustness evaluation with different manufactory\nThe results above have verified the prognostics under more practical scenarios, which are fast charging with dynamic discharging and variable temperature conditions. In this section, the public datasets are also used for verification to evaluate the robustness of the proposed method. The first two batteries from the Ox data set, whose current rate is much smaller than the Lab datasets with smaller temperature variations, will be used for demonstration. The estimation results are shown in Fig. 8, where Fig. 8(a)\u20138(c) are the results for cell 2 while Fig. 8(d)\u20138(f) are the results for cell 1 with the other battery serving as the source battery. The estimation results for the dT curve based on the proposed method, the error for the peak value, and the SoH estimation results based on the proposed method and two benchmarks are given. The peak value here refers to the valley, which is still presented as the peak value to make the presentation consistent across the entire paper. The numerical results of the dT curve reconstruction are listed in Table 7. It shows that the variation of the dT curve could be estimated satisfactor",
        "summary": " The results illustrate that the temperature variation information provided by the dT curve helps improve the SoH estimation, which is because the capacity also varies with temperatures. In real applications, the environmental temperatures also change with the seasons. The results are shown in Fig.8, where Fig"
    },
    {
        "link": "https://www.sciencedirect.com/science/article/pii/S2590116823000206",
        "body_split": "ily with RMSE and MAE less than 0.018 \u00b0C/V and 0.013 \u00b0C/V, respectively. The MaxAE is less than 0.026 \u00b0C/V for all the testing scenarios while the mean MaxAE for those results based on the basic LSTM is 0.045 \u00b0C/V. The proposed multi-DA SoH estimation method is also more accurate than the two benchmarks. The results obtained by the DA estimated dT curve are more accurate than the most conventional method, i.e., \u201cBenchmark 1\u201d. The RMSE, MAE, and MaxAE of the SoH estimations for the proposed method are less than 1.46%, 1.04%, and 4.07% respectively as listed in Table 8. It shows that the MaxAE is not reduced significantly, because the largest errors come from the estimations of some deviation points as shown in Fig. 8. But the overall estimations based on the proposed method are better converged to the real values.\nFinally, two batteries from MIT-Stanford data sets are used for the verification. The \u201cchannel id\u201d of the two batteries are 25 and 29 respectively, whose lifetimes are 1638 and 1115 cycles, respectively. The estimation results are also shown in Table 7 for the dT curve reconstruction and in Table 8 for the SoH estimation, respectively. The results show the accuracy improvement from \u201cBenchmark 1\u201d to \u201cBenchmark 2\u201d and to the proposed method. The RMSE and MAE for the dT curve reconstructions are less than 0.013 \u00b0C/V and 0.0085 \u00b0C/V respectively while those for SoH estimation are less than 0.92% and 0.72% of the proposed multi-DA method. The mean values of the four testing scenarios using the public data sets are also listed in Table 7, Table 8. The results show that the RMSE and MAE for the dT curve reconstruction have been reduced from 0.0187 \u00b0C/V and 0.0127 \u00b0C/V to 0.0132 \u00b0C/V and 0.00954 \u00b0C/V, with the reduction percentages of 29.412% and 24.882% respectively. The results show that the RMSE and MAE for the SoH estimation have been reduced from 1.502% and 1.084% to 1.045% and 0.815% respectively, with reduction percentages",
        "summary": " The RMSE, MAE, and MaxAE of the SoH estimations for the proposed method are less than 1.46%, 1.04%, and 4.07% respectively as listed in Table 8. The MaxAE is less than 0"
    },
    {
        "link": "https://www.sciencedirect.com/science/article/pii/S2590116823000206",
        "body_split": " of 30.519% and 24.815% respectively. Therefore, the verification by the two public datasets also demonstrates the effectiveness of the proposed method for battery health prognostics and the performance improvement compared to the basic LSTM-based method.\n\n4.4. Discussion\nThis paper proposes a novel and more comprehensive battery health prognostic method to provide the estimation of both dT curve variation and battery SoH. The experimental data sets are collected from the aging test that is closer to the real world and the full life span test is conducted. From the results presented and discussed above, it can be summarized that DA is a good way to make better use of the unlabeled data from the target domain to improve the prognostic accuracy. Two public data sets are used to verify the effectiveness of the proposed method under different application scenarios. In order to further demonstrate the improvement of the dT curve reconstruction and SoH estimation with the multi-DA processes, the comparative results of the RMSE and MAE for the dT curve and dT curve peak estimations based on the base model and the DA model are shown in Fig. 9(a). The comparisons between the Multi DA-based SoH estimation and two benchmarks are shown in Fig. 9(b). The box plots clearly illustrate the error reduction for the dT curve reconstruction by the proposed DA method compared to the basic LSTM model. In addition, the error distributions of the proposed method are also much narrower for the multi-DA method, which means the generalization and robustness of the proposed method are also better than the conventional data-driven method.\nHowever, there are still some limitations. For example, the dT curve and dQ curve vary with the current rate, which makes it difficult to reconstruct the dT curve when the batteries work under different charging loads. While the method proposed in this paper could cover the prognostic under the same charging or discharging profile but with different aging degradation patterns, further research is needed to get accurate prognostics under different dT shapes. Although different voltage ranges have been selected for the demonstration of the effectiveness of the proposed strategy and satisfactory results have been obtained under the ultra-fast charging stage with 10C (Lab dataset #1), the continuously sampled data are used. In practical applications, the sampling frequency is much smaller with some missing data. Therefore, the dT curve reconstruction and SoH estimation with discontinuously sampled data are valuable for studying in the future. In addition, a battery pack will have an inconsistent",
        "summary": "Paper proposes novel battery health prognostic method to provide estimation of both dT curve variation and battery SoH. The experimental data sets are collected from the aging test that is closer to the real world and the full life span test is conducted. Two public data sets used to verify the effectiveness of the proposed method under different application scenarios"
    },
    {
        "link": "https://www.sciencedirect.com/science/article/pii/S2590116823000206",
        "body_split": " temperature distribution, i.e. connected battery cells will have different temperature variations. Therefore, the prognostic for the battery pack and its connected battery cells also needs further research. The experimental result in this paper has been a good example of more comprehensive health prognostics considering both temperature and capacity variation estimations. For example, in EVs or smart grids, the charging schedule keeps the same despite the different dynamic discharging profiles or variable temperatures, the results using the Lab datasets have proved the effectiveness of the proposed method in those application scenarios.\n5. Conclusion\nBattery health prognostics are a key component of battery management systems for predictive maintenance of electrified transportation. Both temperature variation and capacity fade are significant for monitoring safe operation. This paper proposed one novel end-to-end method with multi-DA for sensor-free dT curve reconstruction and SoH estimation using Q-V data. To improve the accuracy and generalization, DA is integrated into the hidden layers to reduce the domain discrepancy. In SoH estimation, the reconstructed dT curve is added to provide information on temperature variation as an input, and another DA is adopted to further reduce the domain discrepancy of the hidden features for the SoH estimation.\nBattery aging test for the whole life span is conducted, which includes both primary life and second life testing. The estimation results for the dT curve have proved that the proposed method brings more than 20% error reduction compared to the conventional methods without domain adaptation. The estimation accuracy for SoH estimation could be improved by the reconstrued dT curve with error reductions of 17.763% and 18.302% for the RMSE and MAE, respectively compared to the basic model only using IC curve. When the DA is also adapted to the SoH estimation, the RMSE and MAE can be reduced by 47.010% and 49.572%, respectively. Furthermore, two public datasets were also used in this paper for generalization verification.\nOnly unlabeled data of the testing batteries are used, which broadens the transfer learning strategies for prognostic performance improvement. The proposed strategy is promising to be implemented considering various applications according to the satisfactory testing results under different scenarios. Moreover, different prognostic tasks can also be integrated into this framework by considering multi-task learning for more practical requirements, which will be studied in future work.\nCRediT authorship contribution statement\nYunhong Che: Conceptualization, Methodology, Software, Investigation, Visualization, Writing \u2013 original draft. S\u00f8ren By",
        "summary": " The experimental result in this paper has been a good example of more comprehensive health prognostics considering both temperature and capacity variation estimations. For example, in EVs or smart grids, the charging schedule keeps the same despite the different dynamic discharging profiles or variable temperatures"
    },
    {
        "link": "https://www.sciencedirect.com/science/article/pii/S2590116823000206",
        "body_split": "g Vilsen: Methodology, Writing \u2013 original draft, Writing \u2013 review & editing. Jinhao Meng: Validation, Writing \u2013 original draft, Writing \u2013 review & editing. Xin Sui: Writing \u2013 review & editing. Remus Teodorescu: Supervision, Project administration, Funding acquisition.\n",
        "summary": "Vilsen: Methodology, Writing \u2013 original draft, writing \u2013 review & editing. Remus Teodorescu: Supervision, Project administration, Funding acquisition. Xin Sui: Writing \u2013 review and editing"
    },
    {
        "link": "https://www.sciencedirect.com/science/article/pii/S277250302300018X",
        "body_split": "Introduction\nCommon forms of intelligence include imagery intelligence (IMINT), signals intelligence (SIGINT), and human intelligence (HUMINT). These forms of intelligence rely on collection of imagery (e.g. satellite images), interception of signals, or interpersonal contact. There are also measurement and signature intelligence (MASINT) and financial intelligence gathered from the analysis of monetary transactions (FININT), among others. In modern days, the intelligence community has started to value social media as an important and reliable source of intelligence because of so much human activity on these platforms that generates abundant information in every second and so much interrelation between people's everyday life and their online presence. Therefore, social media intelligence (SOCMINT) has been gradually accepted as a form of intelligence during the past decade.\nSOCMINT typically involves two types of sources, open source (i.e., public information) and private data that requires privileged access [,]. Open source intelligence (OSINT) itself has been gaining more attention [,] as people realize now how easy it could be to use open source information online to deduce private information, such as figuring out where a celebrity lives by analyzing the home photos they share online. OSINT is different from other intelligence sources in that open source information is available to the public without the need to infringe on copyrights, patents, or privacy laws [,]. As so many people are openly sharing personal information on social media, SOCMINT naturally has become a dominant variant of OSINT, although SOCMINT can certainly involve more than open source information.\nIn this study, the focus was solely on open source information found on social media. To test whether it is reliable to profile a person based on open source SOCMINT, political orientation was chosen to be the subject of profiling for two reasons. First, political orientation is a construct easy to understand and accept for most people in terms of liberals vs. conservatives. It is not usually associated with social stigma and therefore it is less controversial, unlike criminal activity. This is important to extract honest opinions in qualitative research. Second, literature has suggested the feasibility of predicting political orientation using social media data and some previous research has established reliable measures of political orientation. Such empirical foundation helps ensure construct validity. In other words, what is considered political orientation in this study is less likely to be fundamentally different how political orientation is defined by others.\nGiven today's political climate, profiling a person's political orientation seems not only relevant but also important in cases of mass shootings, terrorism",
        "summary": "Common forms of intelligence include imagery intelligence (IMINT), signals intelligence (SIGINT), and human intelligence (HUMINT) In modern days, the intelligence community has started to value social media as an important and reliable source of intelligence. Social media intelligence has been gradually accepted as a form of intelligence during the past decade"
    },
    {
        "link": "https://www.sciencedirect.com/science/article/pii/S277250302300018X",
        "body_split": ", and national security. A better understanding on a person's political persuasion can help more accurately analyze digital footprints associated with the person, which makes computer forensics more reliable. For instance, just because someone posts a lot of photos of a certain politician on social media, it does not necessarily mean he or she is a fan. Political orientation is believed to be influenced by many factors, such as personality, environment, moral reasoning, and even genetics [,,]. Many researchers have tried to predict a person's political orientation by analyzing social media data. Some studies focused on Twitter and claimed inference accuracy reaching 90% [,]. Despite the different terminology used in these studies, essentially they were all doing cyber profiling with Twitter data to infer political orientation. Their success provides evidence that supports using SOCMINT to perform cyber profiling.\nNonetheless, the existing literature exhibits a tendency to rely on artificial intelligence (AI) and big data as the methods of data collection and analysis. All discussions regarding its reliability and application are centered around the premise that SOCMINT is collected through Web mining in the form of big data that need to be converted into numbers for data analytics, such as computerized sentiment analysis or other statistical analysis [,,,]. Although it is fair to analyze SOCMINT with AI algorithms, it also needs to be acknowledged that in some fields SOCMINT can be very limited in terms of data sources, which renders AI-based analysis unsuitable. For example, criminal investigation typically is more concerned about specific individuals rather than aggregate patterns. Accordingly, this study intends to contribute to the literature by suggesting SOCMINT can still be helpful in the absence of big data and AI.\nAccordingly, cyber profiling in this study aimed at using limited open source SOCMINT to predict individual political orientation so as to examine whether SOCMINT profiling can still be reliable when there is limited information to work on. Previous research has indicated it is possible to profile a person based on open source SOCMINT in terms of personality traits, such as self-control, and other implicit characteristics [,]. In its application to criminal investigation, cyber profiling is normally meant for performing online behavioral analysis in order to understand a person better and to identify potential leads to forensic evidence. Online behavior is usually determined by analyzing digital footprints. Digital footprints are the data trail left behind in online activities by a user, either intentionally or unknowingly. These may include various digital artifacts (e.g. photos, comments, videos), and systematic information (e.g. metadata, login records, search history).",
        "summary": " A better understanding on a person's political persuasion can help more accurately analyze digital footprints associated with the person, which makes computer forensics more reliable. Political orientation is believed to be influenced by many factors, such as personality, environment, moral reasoning, and even genetics"
    },
    {
        "link": "https://www.sciencedirect.com/science/article/pii/S277250302300018X",
        "body_split": " Obviously, not all digital footprints are open source information, such as search history and photos set as private. Cyber profiling collects whatever is attainable online and then after analysis converts it into online behavioral evidence. Behavioral evidence is normally the key ingredient in criminal profiling. In cyber profiling, the subject's real-world identity may or may not be known beforehand. Either way, studying online behavioral evidence helps understand this subject better. It helps connect online identities to real-world identities, and it helps perceive the subject as a person rather than just an online ID or avatar.\nIn this study, participants\u2019 online behavior was analyzed to predict their political orientation. Participants were recruited from two social media platforms and thus two samples were formed respectively. Cyber profiling was then conducted on each one of them based on their digital footprints on the platform in a qualitative manner. Prediction about their political orientation was then made. Participants were asked to take a quantitative measure to confirm their political orientation, in addition to a qualitative interview. The results of this study contribute to the literature which seems to have overlooked the application of profiling using limited data. In criminal investigation, oftentimes it is impossible to collect more data. We can only work on what we have. Therefore, it is important to test whether limited data may still be reliable. The results from this study were promising in this regard.\nProcedures\nStep 1: Two samples were drawn from two social media platforms respectively. Step 2: Participants were asked to take a survey to quantitatively measure political orientation. Step 3: Participants agreed to be observed by researchers on the social media platform. Only open source data was subject to observation. Step 4: Cyber profiling was conducted based on the participants\u2019 online activities on social media as the qualitative measure of political orientation. Step 5: Results from cyber profiling in terms of political typology were compared to the survey results. Step 6: Participants were interviewed to discuss the survey results and the results from cyber profiling. Participants also talked about their political orientation in their own words.\n\nSamples\nThis study opted to recruit participants from a social media platform that claims it does not impose any censorship. This platform is called \u201cMinds\u201d. It introduces itself as \u201can open source social network dedicated to Internet freedom\u201d. Users on Minds were contacted when they commented on postings that were trending at the time of recruitment as the purpose was to recruit as many users to participate as possible. As a result, 104 participants were recruited from Minds and they formed the first sample,",
        "summary": " Cyber profiling collects whatever is attainable online and then after analysis converts it into online behavioral evidence. In cyber profiling, the subject's real-world identity may or may not be known beforehand. Cyber profiling was conducted on each one of them based on their digital footprints on the platform in a qualitative manner. Prediction about their political orientation was then made"
    },
    {
        "link": "https://www.sciencedirect.com/science/article/pii/S277250302300018X",
        "body_split": " \u201cMinds sample\u201d. They were all at least 20 years old and used English as the primary language.\nA second sample consisted of users on Twitter. Twitter is a bigger and well-known social media platform, but it is known for censorship on information deemed false or controversial. Participants were recruited when they commented on trending tweets at the time of recruitment. The purpose was to recruit more participants. The only selection criteria were the use of English as the primary language and the age of 20 or older. As a result, 135 participants formed this \u201cTwitter sample\u201d.\n\nQuantitative: political orientation survey\nTo objectively measure political orientation, an online survey from Pew Research Center was adopted (https://www.pewresearch.org/politics/quiz/political-typology/). Pew Research Center has developed and conducted surveys for many years to measure political orientation and it came to a typology that consists of the following groups: From left to right in the political spectrum, \u201cprogressive left\u201d, \u201cestablishment liberals\u201d, \u201cdemocratic mainstays\u201d, \u201coutsider left\u201d, \u201cstressed sideliners\u201d, \u201cambivalent right\u201d, \u201cpopulist right\u201d, \u201ccommitted conservatives\u201d, \u201cfaith and flag conservatives\u201d. The characteristics and definitions of each group can be found in Figs. 1 and 2. The survey questions are in the appendix. Additionally, participants were asked to self-report their political orientation before they took this survey. After taking the survey, they were asked again if they had any thoughts.\n\nQualitative: cyber profiling\nDifferent platforms allow for different information to be viewed and users may set privacy settings differently. Some users have a detailed self-introduction on the platform that pretty much reveals their political views already while others may share nothing of the sort. Therefore, it is hard to standardize what information to be used for profiling, which prevents meaningful quantification of SOCMINT in this study. A qualitative review thus was deemed more appropriate based on what was attainable. Since this study was intended to focus on open source intelligence, participants did not need to offer privileged access to their social media data. Only information that was already openly shared was collected.\nIn cyber profiling, three elements were particularly subject to analysis, including online artifacts, online context, and online behavior. Artifacts, like photos, textual comments, and videos are most commonly found on social media",
        "summary": " The only selection criteria were the use of English as the primary language and the age of 20 or older. Participants were recruited when they commented on trending tweets at the time of recruitment. The characteristics and definitions of each group can be found in Figs. 1 and 2"
    },
    {
        "link": "https://www.sciencedirect.com/science/article/pii/S277250302300018X",
        "body_split": ". Content analysis on these artifacts was conducted to assess what message or ideology was intended to be expressed and whether it was political in nature. For example, the following comment found on Minds would indicate a participant's leaning toward the conservative side.\n\nParticipant M011: \u201cYou can tell the world is edging closer and closer to communism\u2026even the republicans are socialist's anymore.\u201d\nAs for how far right this participant is, it would depend on their other comments and the context.\nThe context matters as the same message could bear very different meanings or serve different purposes in different contexts. For example, one participant made the following comment on Twitter in response to someone else.\n\u201cif Trump still the president they [Russia] would never dare.\u201d (not a participant in this study)\n\nParticipant T029: \u201csure, trump is the answer to all problems.\u201d\nGiven this participant's other activities on Twitter, it was believed she was being sarcastic. This context in turn would indicate she was probably more on the liberal side.\nMoreover, social media platforms have their own restrictions and functions on what users can do, which could affect how a context is developed. Finally, online behavior should be analyzed as to why it took place, especially when it was a matter of choice rather than necessity. Such behavior could include sharing personal photos openly online, retweeting some memes, openly expressing political opinions, and hitting the like button on some comments, to name a few. All in all, the purpose of cyber profiling in this study was to infer each participant's political orientation, along with gender, age, education, and race based on open source SOCMINT. As such, the Minds sample was profiled based on their digital footprints on Minds, and likewise the Twitter sample was profiled based on Twitter. However, if the participants revealed their other social media accounts in their digital footprints, their online activities on other social media platforms would be included as the digital footprints subjected to profiling as well, as long as they were open source information.\nThe prediction was very subjective without standardized criteria because it was deemed impossible to have meaningful criteria set for this qualitative review. The rationale behind prediction was mainly based on the level of political extremism being perceived. The Pew categories were seen as a spectrum, with \u201cProgressive Left\u201d being the extreme left and \u201cFaith and Flag Conservatives\u201d being the extreme right. Extremists were presumed to be unable to reasonably discuss facts and prefer",
        "summary": " Content analysis was conducted to assess what message or ideology was intended to be expressed and whether it was political in nature. Social media platforms have their own restrictions and functions on what users can do, which could affect how a context is developed. Online behavior should be analyzed as to why it took place, especially when it was a matter of choice rather than necessity"
    },
    {
        "link": "https://www.sciencedirect.com/science/article/pii/S277250302300018X",
        "body_split": " to empathize ideologies only. Cyber profiling in this study was thus based on this assumption and aimed to decide which category a participant should be assigned to. For example, some participants discussed where illegal immigrants should live as a practical issue, while some others focused on whether immigrants belong here. As such, the former would be considered less extreme than the latter. For another example, one would be categorized as \u201cOutsider Left\u201d if one appeared to be opposed to the right-wing rhetoric but also seemed reserved on echoing the left-wing rhetoric without a logical reason. It is important to note that decisions were not made based on just one activity. Participants in this study tended to express diverse opinions and engage in various activities on social media to different degrees. Hence, it was not practicable to outline specific guidelines for cyber profiling in this study.\nOther social media accounts\nMany participants in this study listed other social media accounts as part of their self-introduction. Facebook, YouTube, Instagram, and TikTok were among the most popular mentioned by the participants. A few participants also listed LinkedIn accounts. Notably, participants probably did not reveal all social media accounts they owned, and they were not asked to do so since in this study only open source information was looked for. However, owning an account does not necessarily mean there was much information to be collected from those accounts. Some accounts had not been updated for months or even years, and some accounts were set as private so no content could be viewed. Nonetheless, additional information is always welcome for cyber profiling even if there is not much of it. In fact, in some cases, the abandonment of some accounts itself could indicate a political attitude. For example, 24 participants from the Minds sample said they stopped using Twitter and switched to Minds because they were not happy about how Twitter handled issues related to \u201cpolitical correctness\u201d, although they did not delete their Twitter accounts.\n\nAge, gender, race, and education\nTo profile personal characteristics, photos were very informative for obvious reasons. Even without photos, some users would write a self-introduction that contains such information, or they might have revealed such information in some comments they left. Without such self-revelation, cyber profiling based on open source SOCMINT could become challenging. However, there were other clues that could be helpful, such as their names, photos of their friends, the celebrities they were following, the foods they mentioned, the videos they watched, and writing styles/skills. All these artifacts, if",
        "summary": " Cyber profiling in this study was based on this assumption and aimed to decide which category a participant should be assigned to. Facebook, YouTube, Instagram, and TikTok were among the most popular mentioned by the participants"
    },
    {
        "link": "https://www.sciencedirect.com/science/article/pii/S277250302300018X",
        "body_split": " available, could potentially help deduce age, gender, race, education, and more. Table 1 shows the characteristics of the two samples.\nIn the Minds sample, the average age was 35.71, ranging from 20 to 64. The prediction was based on five categories, including 20\u201330, 31\u201340, 41\u201350, 50\u201360, and 60 or older. The accuracy rate was 52%. In the Twitter sample, the average age was 32.56, ranging from 20 to 66. The accuracy rate was 69%, which was higher than the Minds sample because more Twitter users put their own photos on that platform. It could be hard to distinguish age in the absence of direct evidence indicative of age, even with photos. However, circumstantial evidence may help, such as the year of work experience or the reference of childhood events. The prediction of gender was 100% accurate for the Twitter sample, and close to 100% (102 out of 104) for the Minds sample. Race was easy to predict if there was a photo, but other clues could help as well. For instance, one participant mentioned her favorite celebrity was Beyonce and she also mentioned quite a few musicians who were all African Americans. In this case, even without a photo it was still relatively easy to presume she is African American. The accuracy rate for the Minds sample was 89% in terms of race, and it was 88% for the Twitter sample. As far as education is concerned, it was easier to predict someone with less education because of poor writing and misspellings, but better-educated people did not necessarily manifest better writing on social media. Hence, wrong predictions were most likely to be associated with those who had a college degree or better but did not write very properly perhaps because people tend to be more casual on social media in terms of writing. However, better-educated participants were more likely to write longer comments as opposed to those without a college degree who were more likely to write only one or two sentences in one comment. The accuracy rate in this regard was 77% for both samples.\n\nPolitical orientation\nAs aforementioned, a survey designed by Pew Research Center was used to quantitatively classify political orientation into 9 categories. Table 2 shows how participants fell into these categories.\nCompared to the Twitter sample, the Minds sample was leaning more toward the conservative side, which is not surprising since many people used Minds in pursuit of the kind the freedom of speech that was often deemed politically incorrect by liberals, according to the",
        "summary": " The prediction was based on five categories, including 20\u201330, 31\u201340, 41\u201350, 50\u201360, and 60 or older. The prediction of gender was 100% accurate for the Twitter sample, and close to 100% for the Minds sample"
    },
    {
        "link": "https://www.sciencedirect.com/science/article/pii/S277250302300018X",
        "body_split": " participants\u2019 accounts. Cyber profiling based on SOCMINT in this study was fairly accurate in predicting political orientation, although it was more prone to error when predicting \u201cStressed Sideliners\u201d. Table 3 shows the accuracy rates for each category. The accurate rates were calculated by comparing the cyber profiling prediction to the survey results. For example, Participant M004 took the survey and was categorized into \u201cAmbivalent Right\u201d, while cyber profiling suggested he belonged to \u201cStress Sideliners\u201d. In this case, cyber profiling was considered inaccurate. For the Minds sample, there were 4 people in the \u201cAmbivalent Right\u201d category (according to the survey), but only 3 were predicted as \u201cAmbivalent Right\u201d by cyber profiling. Therefore, the cyber profiling accurate rate for \u201cAmbivalent Right\u201d was 75%.\nBesides the quantitative measure, participants were also qualitatively interviewed to discuss their political view and how they thought about the survey results. Most participants in the Twitter sample accepted the survey results and found the classification reasonable. Only 2 participants from this sample disagreed strongly with the result. Both of them were classified into \u201cEstablishment Liberals\u201d but one of them believed he should have been in \u201cOutside Left\u201d while the other one believed she was supposed to be in \u201cAmbivalent Right\u201d. In the Minds sample 5 participants believed they should have been in \u201cCommitted Conservatives\u201d when 4 of them were classified into \u201cStressed Sideliners\u201d and 1 was in \u201cAmbivalent Right\u201d. They seemed offended by the implication that they were not conservative enough. Only one participant classified as conservative (\u201cAmbivalent Right\u201d) believed he was more liberal than conservative, which was actually consistent with the cyber profiling prediction.\nAlthough the Pew survey can classify participants into categories that seemed to make sense to most participants, it does not necessarily mean people's political opinions are always consistent with the definition of each category. From the qualitative interviews, it became clear that many, if not most, people's political view was not exactly unwavering. It means as more follow-up questions were asked the participants often started changing their stance on various topics to some extent and became more willing to consider exceptions and unusual circumstances. For example, when one participant was asked to elaborate on a comment she made on social media about how abortion should not be legal whatsoever, she at first adamantly defended her position but then gradually became more open to",
        "summary": " Cyber profiling based on SOCMINT in this study was fairly accurate in predicting political orientation. Cyber profiling was more prone to error when predicting \u201cStressed Sideliners\u2019\u2019s\u2019 \u2018Ambivalent Right\u2019 was considered inaccurate. The accurate rates were calculated by comparing the cyber profiling prediction to the survey results"
    },
    {
        "link": "https://www.sciencedirect.com/science/article/pii/S277250302300018X",
        "body_split": " allowing rape victims to seek abortion. For another example, one participant on Minds shared quite a few memes that were apparently conveying anti-immigration messages, but during the interview he claimed he was an illegal immigrant himself many years ago and explained how he did not mind immigration as long as immigrants live somewhere other than his hometown. In contrast, another participant who seemingly showed avid support for accepting all immigrants on social media started to show hesitation during the interview when he heard about the state of Texas transporting some immigrants to New York.\nIn addition, some participants were somewhat self-contradictory. For instances, some participants believed in small governments, but they called for a bigger police force; some believed racism is a problem in our society today but refuted the idea that some racial groups are being treated unfairly; some said they had no problem embracing diversity but admitted they were bothered when people do not speak English in USA; some claimed no politicians can be trusted despite showing strong affection toward a particular politician on social media. Such subtle wavering of political persuasion might not be easy to detect in quantitative profiling because in SOCMINT political views were often expressed in a relatively shallow manner, such as through a meme probably created by other people or through a comment that consisted of only one or two sentences that were not very well constructed. As such, although quantitative methods are convenient for processing data systematically, cyber profiling might need a qualitative review as well to capture the fluidity of political opinions.\nDiscussion\nIn this study cyber profiling as a qualitative method proved to be accurate in predicting political orientation. There is no denying that the accuracy of qualitative profiling is largely predicated on the profiler's ability and the availability of data, so the findings from this study can hardly be generalized. However, the purpose of this study was not to present a methodology for imitation, nor is it meant to draw conclusions about political orientation. Instead, the purpose was to confirm that even when data is limited to open source SOCMINT, cyber profiling may still be useful. This study utilized two independent samples drawn from two different platforms to avoid sampling error. The results from both samples were similar, which indicates the success of cyber profiling is unlikely due to platform-specific factors. In fact, Minds and Twitter are quite different in that Minds is more likened to Facebook. Moreover, the present study is different from other studies in the literature in that this study did not rely on artificial intelligence and big data. This micro level profiling method is more relevant to criminal justice in which investigation is often more concerned",
        "summary": " Cyber profiling as a qualitative method proved to be accurate in predicting political orientation. Study used two independent samples drawn from two different platforms to avoid sampling error. The results from both samples were similar, which indicates the success of cyber profiling is unlikely due to platform-specific factors"
    },
    {
        "link": "https://www.sciencedirect.com/science/article/pii/S277250302300018X",
        "body_split": " about a specific person rather than the overall trend among a certain population. In light of this difference, this study contributes to the literature on SOCMINT by offering the following research implications.\nFirst, although SOCMINT is mainly collecting data from social media, the information people share on social media could easily lead to other social media platforms and sources other than social media. For example, a search on a person's name, id, photos, or email address could easily reveal their other online presence and online activity. Such information could contain abundant open source digital footprints as well. Simply put, SOCMINT can be a good starting point to collect OSINT of all forms, without invading privacy. In this study digital footprints were restricted to only social media out of respect for our participants, but if necessary, such as in a criminal investigation, the expansion of SOCMINT into OSINT should help cyber profiling become even more accurate.\nSecond, with more OSINT sources, cyber profiling should be more accurate in predicting an unknown subject's personal characteristics. In this study, age was the most difficult characteristic to predict in the absence of corroborating evidence. This reflects the reality that in today's Internet culture, even if people's online behaviors appear different, such differences are less likely to be caused by age. This could be explained in part by the computer-mediated nature of online behavior. Simply put, regardless of age people all need to operate computers in the same way. The ones who are intimidated by computer technology probably would not engage in much online behavior to begin with. Hence, cyber profiling should always be mindful of not only what can be observed but also what is not being seen. There might be obvious differences between teenagers and adults, which remains to be confirmed by future research since this study did not include minors as participants. Nevertheless, seeking information from multiple OSINT sources should be a crucial step of cyber profiling.\nThird, seeking more OSINT sources also serve as a validity check, even if there is abundant information on social media already. This is because what is being presented on social media might not be what is happening in reality as some people like to fake their identity or lifestyle on social media. They might be doing so to impress others, or the disguises could be part of a malicious scheme. For whatever reasons, the interpretation of SOCMINT should always be aware of the potential audience effect. It means an online presence and activity on social media could be created for an audience's benefits instead of a way of self-expression. In this study",
        "summary": " SOCMINT is mainly collecting data from social media, but the information people share on social media could easily lead to other social media platforms and sources other than social media. A search on a person's name, id, photos, or email address could easily reveal their other online presence and online activity"
    },
    {
        "link": "https://www.sciencedirect.com/science/article/pii/S277250302300018X",
        "body_split": ", there was no sufficient evidence suggesting any participants were faking but during the interviews it was observed some discrepancy between what they presented social media and what they claimed during the interview. This is the benefit of using mixed methods to measure political orientation. The researcher could compare the quantitative measure with the qualitative account to interpret data. While qualitative accounts could be meant to impress the interviewer, it is equally possible that the participant did not answer the survey with all honesty. To be certain, drawing conclusions from more sources and different methods might be helpful. Accordingly, SOCMINT if possible should be collected from more than one platform, and cyber profiling should incorporate more than just quantitative methods despite the popularity because the quantitative method is particularly vulnerable to the audience effect when data is merely quantified at face value.\nFourth, although some participants\u2019 political opinions were quite expressive in their digital footprints, it is important to note that not all participants engaged in online activities directly related to politics. In fact, in some cases the explicit political view manifested on social media could be misleading if digital footprints were analyzed out of context. For example, one could share a very offensive tweet and voice support for it with the purpose to be sarcastic. Moreover, one might appear to show strong support for a politician on some postings but have strong criticism also on some other postings, so the support should not be mistaken as unconditional. Hence, it should be stressed how important it is for the context of digital footprints to be taken into account when conducting cyber profiling. Social media is known for interactivity, and therefore cyber profiling should distinguish the digital footprints that are a result of interaction from the digital footprints that are at one's own discretion. In this study, some participants said they \u201cliked\u201d a message posted by a friend sometimes only because the friend had \u201cliked\u201d a message of their own previously. This begs the question about how much they really agree with the message they \u201cliked\u201d in this type of reciprocal support.\nFifth, as mentioned above, people's political opinions are not always consistent, and their political orientation might not be unchangeable. When analyzing SOCMINT, it requires some attention to note any inconsistency and shift in attitude, which is impossible to capture when SOCMINT is analyzed as aggregate data. Political orientation is just an example. The same can be said about people's opinions about many subject matters. If the purpose is to better understand a person, then these changes in opinion and attitude can be quite significant. AI analysis does not usually",
        "summary": "During the interviews it was observed some discrepancy between what they presented social media and what they claimed during the interview. This is the benefit of using mixed methods to measure political orientation. In some cases the explicit political view manifested on social media could be misleading if digital footprints were analyzed out of context"
    },
    {
        "link": "https://www.sciencedirect.com/science/article/pii/S277250302300018X",
        "body_split": " take these within-subject changes into consideration. For example, Amazon's AI can easily detect your interest in a certain product but their algorithms are not smart enough to understand why you change your mind and choose not to place an order. This is not to discredit AI-based cyber profiling. Rather, this is to suggest the development of cyber profiling requires a good qualitative understanding as foundation, without which no machine learning algorithms can really provide reliable prediction. When cyber profiling is applied for commercial purposes, wrong predictions could be harmless other than being annoying, whereas when it is applied to criminal justice, wrong predictions could result in serious consequences.\nFinally, this study's mixed methods show strength. The qualitative component allows us to capture any contradiction and subtle wavering in opinions that cannot be reflected in a strictly quantitative study. While the quantitative survey can put people in the same category, it cannot tell whether there are individual differences within the category. It also cannot reveal the rationale or mindset underlying a certain group of political orientation. As out findings suggested, sometimes participants changed their opinion a bit after thinking about the issue more thoroughly. This is something a quantitative survey cannot detect. On the other hand, the quantitative component helps researchers refrain from subjective biases. It also helps participants who are less verbal to express their thoughts. Some people cannot explain their thoughts very well but at least they can confirm whether they agree with the survey results.\nConclusion\nThis paper discussed a study that used SOCMINT to conduct cyber profiling. After reviewing the literature on SOCMINT, this study appeared to be the first one applying qualitative analysis on SOCMINT with an emphasis on within-subject profiling rather than between-subject patterns. It is not intended to claim which is better. Rather, it is merely intended to contribute to the field by addressing an aspect that has been overlooked. This study utilized only open source SOCMINT to avoid privacy issues, and the results provided decent support for the effectiveness of using SOCMINT to predict political orientation. Political orientation as a well-established construct allows for unambiguous conclusions to be drawn regarding whether cyber profiling was accurate. Mixed methods were used to strengthen confidence in construct validity. Future research may apply it to other types of predictions. Cyber profiling was conducted in a qualitative manner and it was not able to propose a methodology that can be replicated to generate the same results. However, this qualitative nature revealed several important implications as discussed above. To sum up, although AI-based cyber profiling has merits, a qualitative review of digital footprints is imperative to improve the accuracy of prediction",
        "summary": " A paper discussed a study that used SOCMINT to conduct cyber profiling. The study appeared to be the first one applying qualitative analysis on SOCMint with an emphasis on within-subject profiling rather than between-subject patterns. Mixed methods were used to strengthen confidence in construct validity. Future research may apply it to other types of predictions"
    },
    {
        "link": "https://www.sciencedirect.com/science/article/pii/S277250302300018X",
        "body_split": ". This is especially needed when cyber profiling is applied to the field of criminal justice, which typically holds a higher standard for the concept of \u201cevidence\u201d. Undoubtedly machine learning will remain the main method for processing a large dataset, but future research should aim at developing methods that can incorporate the strength from both quantitative and qualitative approaches into the analysis of smaller datasets. Albeit this study focused on social media, the data used for cyber profiling ideally should be collected from diverse sources if possible. Contrary to popular belief, intelligence does not necessarily have to stem from covert operations and private sources. Open source information can be converted into OSINT and generates ample knowledge, too. However, information does not automatically become intelligence. In this aspect, cyber profiling is one promising method to process OSINT collected from electronic sources.\n",
        "summary": " Cyber profiling is one promising method to process OSINT collected from electronic sources. This is especially needed when cyber profiling is applied to the field of criminal justice"
    },
    {
        "link": "https://www.sciencedirect.com/science/article/pii/S2949863523000109",
        "body_split": "1. Introduction\nIn industrial machinery management,,,,,, the occurrence of unanticipated faults during equipment useful life period may affect the machinery availability, industrial productivity and economic profits of the underlying industrial process. Hence, fault diagnosis is a key factor in industrial engineering and management as occurrence of faults can lead to heavy investment and productivity losses. Therefore, fault diagnosis is very important for guaranteeing a high performance of machinery and industrial processes; specifically, for maintaining reliability and durability. Indeed, electric machines and mechanical drive systems are standard in various industrial sectors including electric transportation systems, handling systems, extruders, conveyors, and home appliances, to name few. This is because of their consistency of speed control, and cost effectiveness. However, because of several stresses, the electric powertrain can be affected by failures that may arise in actuator, power converter, or sensors. Such component faults cause a diminution in electric system availability and reliability.\nDue to these reasons mentioned above, a considerable attention has been given to early diagnosis of electric drive systems for better management of industrial and production plants,,,,,. Certainly, early fault diagnosis in sensor electric drive systems should be accurate at early stage in order to stop or to weaken their effects so as to reduce production costs. Specifically, condition monitoring and predictive maintenance of industrial interconnected systems are necessary to detect their malfunction due to various technical problems such as noise, direct current offset, drift, and disconnections. Unfortunately, such malfunction can inherently produce critical consequences to performance, ef\ufb01ciency and optimal control of production plant.\nIn this regard, recently, various popular statistical machine learning models for condition monitoring have been proposed to detect presence of errors in machines using; for instance; artificial neural networks,, support vector machines,, K-nearest neighbors, and decision trees classifier.\nAlthough numerous models have been used in the literature on electric drive fault diagnosis and management, there are two major limits that should be addressed. First, a comparative study is needed to compare performances of popular models. Second, there is a need to apply such models on a very large database to efficiently assess their respective performances. Motivated by the aforementioned issues, the purpose of the current work is to compare various statistical machine learning models in the problem of detecting presence of errors in electric machine. Indeed, such comparative study can help choosing the best predictive maintenance model used to accurately diagnose fault and subsequently reduce maintenance cost and improve production efficiency. Specifically, six predictive models are compared; including linear discriminant analysis (LDA)",
        "summary": " The occurrence of unanticipated faults during equipment useful life period may affect the machinery availability, industrial productivity and economic profits of the underlying industrial process. Fault diagnosis is a key factor in industrial engineering and management as occurrence of faults can lead to heavy investment and productivity losses"
    },
    {
        "link": "https://www.sciencedirect.com/science/article/pii/S2949863523000109",
        "body_split": ",, k-NN algorithm,, na\u00efve Bayes (NB) and kernel na\u00efve Bayes (KNB) [11], decision trees (DT), and support vector machine (SVM). In addition, all models are validated on a big data set to assess their respective performances and computational complexity.\nTo sum up, in the current work we focus on statistical machine learning because they allow understanding relationships between variables and performing statistical inference whenever they are needed so as to understand and explain the results. Furthermore, we conduct a comparative study of predictive models since it is a common way to find the most accurate predictive model in various machine learning applications,,,,,,,,,,,,,,,.\nIn short, we implement LDA, k-NN, NB, KNB, DT, and SVM to classify electric drive fault data. Then, we compare their performances based on calculated confusion matrices.\nThe contributions of our study are presented as follow:\n1. We compare advanced statistical machine learning models in the context of electric drive fault classification. This would help finding the best statistical machine learning model for industrial process monitoring and electric drive fault diagnosis and management.\n2. All models are applied to a large data set. Accordingly, the results are expected to be robust from a statistical point of view and one can evaluate and compare all models when in terms of processing time.\n3. Contrary to previous works,,,,, where binary classification problem was considered; in our study, all models are performed in the context of a multi-classification problem. This is important to shed light on the performance of various classifiers in detection of different electric drive fault scenarios.\n4. Except LDA model, all remaining five statistical machine learning models are nonlinear. Accordingly, we focus on nonlinear models as they are not prone to assumptions regarding linearity specification.\n5. This is the first paper to conduct a comparative study of the performance of statistical machine learning models in analysis and classification of large dataset from electric drive fault diagnosis. In this regard, our results are expected to help finding the fast and effective predictive model useful for electric drive fault classification and management.\nThe rest of the paper is organized as follows: Section 2 introduces the predictive models and Section 3 presents data and provide results. Section 4 discusses the study and Section 5 concludes the work.\n2.1. Linear Discriminant Analysis\nThe LDA, has become a standard baseline method in classification, due to its simplicity and interpretability.",
        "summary": "In the current work we focus on statistical machine learning because they allow understanding relationships between variables and performing statistical inference whenever they are needed so as to understand and explain the results. All models are validated on a big data set to assess their respective performances and computational complexity. The results are expected to be robust from a statistical point of view and one can evaluate and compare all models"
    },
    {
        "link": "https://www.sciencedirect.com/science/article/pii/S2949863523000109",
        "body_split": " Based on Fisher\u2019s discrimination criterion, it generates a linear projection matrix used to improve classification accuracy. In particular, it employs linear decision boundaries to maximize the proportion of between-class and within-class variability. These linear decision boundaries are obtained by applying eigenvalue decomposition to the scatter matrices and assuming that the scatter matrix is nonsingular. More precisely, LDA generates a discriminant function that separates samples into two or more groups by minimizing the expected misclassification cost and maximizing the ratio of between groups (Sb) and within groups (Sw) variances. LDA assumes that all predictors are normally distributed and that the covariance matrices are identical. The within group (Sw) and between groups (Sb) variances are given by: (1) S w = \u2211 C i=1 \u2211 x\u2208 C i ( x i \u2212 \u03bc i ) ( x i \u2212 \u03bc i ) T (2) S b = \u2211 C i=1 L i ( \u03bc i \u2212\u03bc) ( \u03bc i \u2212\u03bc) T where xi, Li, and \u03bci are respectively dysphonia pattern, the number and the mean of the labeled set in the ith class; and, \u03bc is the mean of all classes and T is the transpose operator. Then, the maximization of the ratio between Sb and Sw is obtained by finding W*\u2009that satisfies: (3) W \u2217 = argmax w \u2223 \u2223 W T S b W \u2223 \u2223 \u2223 \u2223 W T S w W \u2223 \u2223 where W is eigenvector of S \u22121 w S b.\n\n2.2. The k-NN algorithm\nThe k-NN algorithm, is a nonparametric supervised classifier in which retrieving nearest neighbors is based on the concept of similarity. It clusters a set of data points into groups and classifies new data based on a measure of similarity (such as the Euclidean distance). Its main advantage is that it does not assume the form of a fitted model. In particular, it is entirely based on data-driven learning. Technically, given a value k and a feature vector to classify I, it locates the k nearest neighbors of I in the sample set and uses the categories of neighbors to determine the class of I. This structure imposes a lower computational burden. Therefore, the k-NN algorithm is fast. The common algorithm of k-NN is as follows: 1. Compute the Eucl",
        "summary": " The k-NN algorithm is a nonparametric supervised classifier in which retrieving nearest neighbors is based on the concept of similarity. It clusters a set of data points into groups and classifies new data based on a measure of similarity. Its main advantage is that it does not assume the form of a fitted model"
    }
]