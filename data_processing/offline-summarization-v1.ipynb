{"cells":[{"cell_type":"markdown","metadata":{"id":"DxGk5T1ntmeD"},"source":["## Get all the articles to summarize"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-07-14T08:39:16.733306Z","iopub.status.busy":"2023-07-14T08:39:16.732873Z","iopub.status.idle":"2023-07-14T08:39:28.188390Z","shell.execute_reply":"2023-07-14T08:39:28.186353Z","shell.execute_reply.started":"2023-07-14T08:39:16.733273Z"},"id":"U3QaK6LBtmeE","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["1000\n"]}],"source":["import requests\n","\n","host = \"http://144.24.201.133:5000\"\n","articles_json = requests.get(f\"{host}/allPapers\").json()\n","print(len(articles_json))"]},{"cell_type":"markdown","metadata":{"id":"sRL-nBuotmeE"},"source":["## Instanciate BART"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-07-14T08:39:28.190830Z","iopub.status.busy":"2023-07-14T08:39:28.190379Z","iopub.status.idle":"2023-07-14T08:39:33.387367Z","shell.execute_reply":"2023-07-14T08:39:33.386358Z","shell.execute_reply.started":"2023-07-14T08:39:28.190796Z"},"id":"XaiIbmpItmeE","trusted":true},"outputs":[],"source":["from transformers import BartForConditionalGeneration, AutoTokenizer\n","\n","model_ckpt = \"sshleifer/distilbart-cnn-6-6\"\n","tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n","model = BartForConditionalGeneration.from_pretrained(model_ckpt).cuda()"]},{"cell_type":"markdown","metadata":{"id":"epITcYjKtmeF"},"source":["## Check Cuda comparibility"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2023-07-14T08:39:33.389309Z","iopub.status.busy":"2023-07-14T08:39:33.388958Z","iopub.status.idle":"2023-07-14T08:39:33.395768Z","shell.execute_reply":"2023-07-14T08:39:33.394878Z","shell.execute_reply.started":"2023-07-14T08:39:33.389277Z"},"id":"kJVuXRpTtmeF","outputId":"6ee83d7a-99a4-4e52-992d-0b1b437f3c50","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda\n"]}],"source":["import torch\n","from tqdm import tqdm\n","import math\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"]},{"cell_type":"markdown","metadata":{},"source":["# Tokenize and filter by number of pages\n","If a paper has more than 10 pages, it is filtered out as a single summary will be too long for chat gpt input\n"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2023-07-14T08:39:33.399199Z","iopub.status.busy":"2023-07-14T08:39:33.398480Z","iopub.status.idle":"2023-07-14T08:39:59.970343Z","shell.execute_reply":"2023-07-14T08:39:59.969416Z","shell.execute_reply.started":"2023-07-14T08:39:33.399118Z"},"id":"Uc8eLs1ptmeF","outputId":"04c74cd2-3fc6-4043-ad1d-ec5bf2fb1776","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["  0%|          | 0/1000 [00:00<?, ?it/s]"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 1000/1000 [00:40<00:00, 24.93it/s]\n"]}],"source":["page_size = 512\n","\n","paper_rows = []\n","skipped_papers = []\n","for paper in tqdm(articles_json):\n","    tokens = tokenizer(paper['body'], padding='max_length', return_tensors='pt').to(device)\n","    n_pages = math.ceil(len(tokens[\"input_ids\"][0])/page_size)\n","    paper_rows.append(\n","            {\n","            \"link\": paper[\"link\"],\n","            \"body\":  paper[\"body\"],\n","            \"summary\":  \"\",\n","            \"input_ids\":  tokens\n","            }\n","        )"]},{"cell_type":"markdown","metadata":{"id":"xXgv0sIltmeH"},"source":["## Summarize Docs\n","Split tokens in pages of 5 tokens\n","\n","A single paper (document) is divided in pages (token_splits): each page is an array of tokens.\n","\n","Documents_tokenized contains the list of papers that must be summarized, and it is an array of documents."]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["file = open(\"paper_splits.json\", \"w\")\n","paper_splits = []"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":890},"execution":{"iopub.execute_input":"2023-07-14T08:39:59.972642Z","iopub.status.busy":"2023-07-14T08:39:59.971991Z"},"id":"m5YHYROUtmeI","outputId":"bad5a935-56d5-4b17-e4c7-869c2acc44b7","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["0it [00:00, ?it/s]"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\fisch\\AppData\\Local\\Temp\\ipykernel_14816\\2832457497.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  token_splits = { \"input_ids\": torch.tensor(row['input_ids']['input_ids'][0][index*max_size:(index+1)*max_size]).unsqueeze(0).to(device),\n","C:\\Users\\fisch\\AppData\\Local\\Temp\\ipykernel_14816\\2832457497.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  \"attention_mask\": torch.tensor(row['input_ids']['attention_mask'][0][index*max_size:(index+1)*max_size]).unsqueeze(0).to(device)}\n","C:\\Users\\fisch\\AppData\\Local\\Temp\\ipykernel_14816\\2832457497.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  token_splits = { \"input_ids\": torch.tensor(row['input_ids']['input_ids'][0][index*max_size:len(row['input_ids']['input_ids'][0])%max_size + index*max_size]).unsqueeze(0).to(device),\n","C:\\Users\\fisch\\AppData\\Local\\Temp\\ipykernel_14816\\2832457497.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  \"attention_mask\": torch.tensor(row['input_ids']['attention_mask'][0][index*max_size:len(row['input_ids']['input_ids'][0])%max_size + index*max_size]).unsqueeze(0).to(device)}\n","1000it [04:37,  3.60it/s]\n"]}],"source":["max_size = 512\n","\n","counter = 0\n","\n","for idx, row in tqdm(enumerate(paper_rows)):\n","    \n","    n_splits = math.ceil(len(row[\"input_ids\"][0])/max_size)\n","    document_text_summary = \"\"\n","    token_count = 0\n","    for index in list(range(n_splits)):\n","        if(counter==100):\n","            break\n","        if(index != n_splits-1):\n","            token_splits = { \"input_ids\": torch.tensor(row['input_ids']['input_ids'][0][index*max_size:(index+1)*max_size]).unsqueeze(0).to(device),\n","                                \"attention_mask\": torch.tensor(row['input_ids']['attention_mask'][0][index*max_size:(index+1)*max_size]).unsqueeze(0).to(device)}\n","\n","        else:\n","            token_splits = { \"input_ids\": torch.tensor(row['input_ids']['input_ids'][0][index*max_size:len(row['input_ids']['input_ids'][0])%max_size + index*max_size]).unsqueeze(0).to(device),\n","                            \"attention_mask\": torch.tensor(row['input_ids']['attention_mask'][0][index*max_size:len(row['input_ids']['input_ids'][0])%max_size + index*max_size]).unsqueeze(0).to(device)}\n","\n","        if(token_splits[\"input_ids\"].nelement() == 0):\n","          continue\n","        \n","       \n","   \n","    \n","        doc_token_summary = model.generate(input_ids=token_splits['input_ids'],\n","                            attention_mask=token_splits['attention_mask'],\n","                            min_length=16,\n","                            max_length=128)\n","        token_count += len(doc_token_summary[0])\n","        extracted_summary = tokenizer.decode(doc_token_summary[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n","        \n","        paper_splits.append({\n","            \"link\": row[\"link\"],\n","            \"body_split\": tokenizer.decode(token_splits[\"input_ids\"][0], skip_special_tokens=True, clean_up_tokenization_spaces=True),\n","            \"summary\": \".\".join(extracted_summary.split(\".\")[0:-1])})\n","        \n","        if (\".\" in extracted_summary):\n","             document_text_summary += (\".\".join(extracted_summary.split(\".\")[0:-1])) + \".\\n\"\n","        else:\n","            document_text_summary += extracted_summary + \".\\n\"\n","\n","        counter += 1\n","    \n","    # requests.post(f\"{host}/updateArticle\", json={\n","    #   \"link\": row[\"link\"],\n","    #   \"token_count\": token_count,\n","    #   \"summary\": document_text_summary.replace(\"..\", \".\")\n","    # }) \n"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["import json\n","file.write(json.dumps(paper_splits))\n","file.close()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"}},"nbformat":4,"nbformat_minor":4}
